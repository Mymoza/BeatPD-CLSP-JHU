{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEAT-PD Challenge\n",
    "\n",
    "Challenge website : https://www.synapse.org/#!Synapse:syn20825169/wiki/596118\n",
    "\n",
    "Data information : https://www.synapse.org/#!Synapse:syn20825169/wiki/600405\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas/Doubts [Laureano]\n",
    "\n",
    "VAD like thing to remove unwanted data?\n",
    "modified MFCC?\n",
    "X,Y,Z = relative positions or acceleration?\n",
    "\n",
    "Imp: Predict per person. Maybe UBM like thing and adapt it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Imports for the high pass signal\n",
    "from scipy.signal import butter, freqz, lfilter\n",
    "\n",
    "# KFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Import required modules\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os.path\n",
    "\n",
    "# To write WAV File\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "# To make derivative work on multiple CPUs\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "\n",
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "# FIXME : Move this to data?\n",
    "path_save_accelerometer_plots = (\"/home/sjoshi/codes/python/BeatPD/code/accelerometer_plots/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_data_type(data_type, training_or_ancillary='training_data', data_real_subtype=None):\n",
    "    \"\"\"\n",
    "    Setup file names\n",
    "    \n",
    "    Keyword arguments:\n",
    "    - data_type = {cis , real}\n",
    "    - training_or_ancillary: {'training_data', 'ancillary_data'} to switch between both\n",
    "    - data_real_subtype: only provided for REAL-PD \n",
    "\n",
    "    If data_type is real, data_real_subtype will have to be declared as well \n",
    "    data_real_subtype={smartphone_accelerometer, smartwatch_accelerometer, smartwatch_gyroscope}\n",
    "    \"\"\"\n",
    "    if data_type == \"cis\":\n",
    "        # CIS-PD_Training_Data_IDs_Labels.csv\n",
    "        # CIS-PD_Ancillary_Data_IDs_Labels.csv\n",
    "        path_train_labels = (\n",
    "            data_dir\n",
    "            + data_type\n",
    "            + \"-pd.data_labels/\"\n",
    "            + data_type.upper()\n",
    "            + \"-PD_\"\n",
    "            + (\"Training_Data\" if training_or_ancillary == 'training_data' else \"Ancillary_Data\")\n",
    "            +\"_IDs_Labels.csv\"\n",
    "        )\n",
    "        path_train_data = data_dir + data_type + \"-pd.\"+training_or_ancillary+\"/\"\n",
    "\n",
    "    if data_type == \"real\":\n",
    "        path_train_labels = (\n",
    "            data_dir\n",
    "            + data_type\n",
    "            + \"-pd.data_labels/\"\n",
    "            + data_type.upper()\n",
    "            + \"-PD_\"\n",
    "            + (\"Training_Data\" if training_or_ancillary == 'training_data' else \"Ancillary_Data\")\n",
    "            +\"_IDs_Labels.csv\"\n",
    "        )\n",
    "        \n",
    "        path_train_data = (\n",
    "            data_dir + data_type + \"-pd.\"+training_or_ancillary+\"/\" + data_real_subtype + \"/\"\n",
    "        )\n",
    "\n",
    "    # Display labels\n",
    "    df_train_label = pd.read_csv(path_train_labels)\n",
    "    return path_train_data, df_train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_title(idx, df_train_label):\n",
    "    \"\"\"\n",
    "    Create a title that identifies the plotted graph with the measurement_id, \n",
    "    subject_id, on_off label, dyskinesia and tremor labels.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    - idx: \n",
    "    - df_train_label: DataFrame containing the following columns \n",
    "            [measurement_id, subject_id, on_off, tremor, dyskenisia]\n",
    "    \n",
    "    Returns: A string concatenating all the values mentioned \n",
    "    \"\"\"\n",
    "    # Following val_* variables are only used to format a cute title for the charts\n",
    "    val_subject_id = df_train_label.loc[[idx]][\"subject_id\"].values[0]\n",
    "    val_on_off = df_train_label.loc[[idx]][\"on_off\"].values[0]\n",
    "    val_dyskinesia = df_train_label.loc[[idx]][\"dyskinesia\"].values[0]\n",
    "    val_tremor = df_train_label.loc[[idx]][\"tremor\"].values[0]\n",
    "    return \"{0} = on_off: {1}, dyskinesia: {2}, tremor: {3}\".format(\n",
    "        val_subject_id, val_on_off, val_dyskinesia, val_tremor\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accelerometer(data_type, path_accelerometer_plots, path_inactivity=None):\n",
    "    \"\"\"\n",
    "    Plots the accelerometer data. There will be 3 subplots for each axis (X, Y, Z)\n",
    "    \n",
    "    Keyword arguments: \n",
    "    - data_type={cis , real} : It depends on which database is used \n",
    "    - path_accelerometer_plots: Path where the accelerometer plots are going to be saved \n",
    "    - path_inactivity: Path where the dataframe with inactivity  removed are \n",
    "    \"\"\"\n",
    "    # Iterating through all the indexes contained in df_train_label\n",
    "    for idx in df_train_label.index:\n",
    "        if path_inactivity is None:\n",
    "            df_train_data = pd.read_csv(\n",
    "                path_train_data + df_train_label[\"measurement_id\"][idx] + \".csv\"\n",
    "            )\n",
    "        else:\n",
    "            df_train_data = pd.read_csv(\n",
    "                path_inactivity + df_train_label[\"measurement_id\"][idx] + \".csv\"\n",
    "            )\n",
    "\n",
    "        # FIXME: BUG ?  why the following goes to 1000xxx sometimes? It should be max 59xxx\n",
    "        print(\"measurement_id : \", df_train_label[\"measurement_id\"][idx])\n",
    "        # Following val_* variables are only used to format a cute title for the charts\n",
    "        great_title = get_plot_title(idx, df_train_label)\n",
    "\n",
    "        # The time doesn't have the same name depending on the data_type\n",
    "        x_axis_data_type = \"t\" if data_type == \"real\" else \"Timestamp\"\n",
    "\n",
    "        # Normalize the data\n",
    "        cols_to_norm = [\"x\", \"y\", \"z\"] if data_type == \"real\" else [\"X\", \"Y\", \"Z\"]\n",
    "        df_train_data[cols_to_norm] = df_train_data[cols_to_norm].apply(\n",
    "            lambda x: (x - x.min()) / (x.max() - x.min())\n",
    "        )\n",
    "\n",
    "        df_train_data.plot(\n",
    "            x=x_axis_data_type, legend=True, subplots=True, title=great_title\n",
    "        )\n",
    "\n",
    "        # Save plotted graph with the measurement_id as name of the file\n",
    "        plt.savefig(\n",
    "            path_accelerometer_plots + df_train_label[\"measurement_id\"][idx] + \".png\"\n",
    "        )\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_missing_values(df_train_label):\n",
    "    \"\"\"\n",
    "    Filling NaN values with -1. \n",
    "    \n",
    "    Keyword arguments:\n",
    "    - df_train_label: DataFrame containing the following columns \n",
    "            [measurement_id, subject_id, on_off, tremor, dyskenisia]\n",
    "    \"\"\"\n",
    "    # Replace NaN values with -1.0 because otherwise plotting triggers an error\n",
    "    df_train_label = df_train_label.fillna(value=-1.0)\n",
    "    return df_train_label\n",
    "\n",
    "\n",
    "def compute_symptoms_occurences_dataframe(df_train_label):\n",
    "    \"\"\"\n",
    "    Computes how many times the symptoms are occuring for a single subject_id \n",
    "    \n",
    "    Keyword arguments:\n",
    "    - df_train_label: DataFrame containing the following columns \n",
    "            [measurement_id, subject_id, on_off, tremor, dyskenisia]\n",
    "    \"\"\"\n",
    "    df_train_label = prepro_missing_values(df_train_label=df_train_label)\n",
    "\n",
    "    # Group data by subject_id\n",
    "    df_train_label_subject_id = df_train_label.groupby(\"subject_id\")\n",
    "\n",
    "    df_occurences = []\n",
    "    symptoms = [\"on_off\", \"dyskinesia\", \"tremor\"]\n",
    "\n",
    "    for key, value in df_train_label_subject_id:\n",
    "        for symptom in symptoms:\n",
    "            # Pour un patient, prendre les 3 dernieres colonnes, et pour 1 symptome, calculer le nb d'occurences\n",
    "            counter = (\n",
    "                df_train_label_subject_id.get_group(key)\n",
    "                .iloc[:, -3:][symptom]\n",
    "                .value_counts()\n",
    "            )\n",
    "\n",
    "            for symptom_value, symptom_occurence in counter.items():\n",
    "                df_occurences.append(\n",
    "                    (\n",
    "                        {\n",
    "                            \"subject_id\": key,\n",
    "                            \"symptom\": symptom,\n",
    "                            \"symptom_value\": symptom_value,\n",
    "                            \"occurence\": symptom_occurence,\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    df_occurences = pd.DataFrame(\n",
    "        df_occurences, columns=(\"subject_id\", \"symptom\", \"symptom_value\", \"occurence\")\n",
    "    )\n",
    "\n",
    "    return df_occurences, df_train_label_subject_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_symptoms_occurences(df_occurences, df_train_label_subject_id):\n",
    "    \"\"\"\n",
    "    This function plots the occurences of symptoms according to subject_id \n",
    "\n",
    "    Keyword Arguments: \n",
    "    - df_occurences: contains the df with occurences computed in compute_symptoms_occurences_dataframe\n",
    "    - df_train_label_subject_id: contains df_train_label grouped by subject_id \n",
    "    \"\"\"\n",
    "    # There will be one graph plotted for each patient, for each of the 3 symptoms\n",
    "    nb_subjects_id = (\n",
    "        df_occurences.subject_id.nunique()\n",
    "    )  # nb of unique patients in the label file\n",
    "    print(\"Nb subject_id : \", nb_subjects_id)\n",
    "    height = 30 if nb_subjects_id > 10 else 10\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=nb_subjects_id, ncols=3, figsize=(10, height), sharey=True\n",
    "    )  # 3 cols for the 3 symptoms\n",
    "\n",
    "    # Quick fix to plot the graphs at the right place. Starts at -1 because in the first for loop\n",
    "    # it is incremented\n",
    "    patient = -1\n",
    "\n",
    "    # Plot for all subject_id 3 bar plots for all the symptoms and their occurences\n",
    "    # Reminder that NaN values (missing values) were replaced with -1 and are shown as such in the plots\n",
    "    symptoms = [\"on_off\", \"dyskinesia\", \"tremor\"]\n",
    "    for key, value in df_train_label_subject_id:\n",
    "        patient = patient + 1  # value used to position the plots (row)\n",
    "        symptom_no = 0  # value only used to position the plots (col)\n",
    "        for symptom in symptoms:\n",
    "            subject_symptom = \" \".join(\n",
    "                [str(key), symptom]\n",
    "            )  # variable used to create a title for each plot\n",
    "            df_train_label_subject_id.get_group(key)[symptom].value_counts().plot(\n",
    "                kind=\"bar\",\n",
    "                x=symptom,\n",
    "                title=subject_symptom,\n",
    "                ax=axes[patient, symptom_no],\n",
    "                sharey=True,\n",
    "            )\n",
    "\n",
    "            fig.tight_layout()\n",
    "            plt.tight_layout()\n",
    "            symptom_no = symptom_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interesting_patients(df_train_label, list_measurement_id):\n",
    "    \"\"\"\n",
    "    Filters df_train_label according to a list of measurement_id we are interested in analyzing\n",
    "\n",
    "    Keyword Arguments:\n",
    "    - df_train_label: Labels DataFrame containing the following columns \n",
    "            [measurement_id, subject_id, on_off, tremor, dyskenisia]\n",
    "    - list_measurement_id: list of measurement_id \n",
    "\n",
    "    Returns:\n",
    "    - df_train_label: filtered df_train_label containing only the measurements_id we are interested in \n",
    "    \"\"\"\n",
    "    filter_measurement_id = df_train_label.measurement_id.isin(list_measurement_id)\n",
    "\n",
    "    df_train_label = df_train_label[filter_measurement_id]\n",
    "    # display(df_train_label)\n",
    "    return df_train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "possible to have participant characteristics from additional db data? ex https://ieeexplore.ieee.org/abstract/document/7911257"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIP : Function to remove silence (inactivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_threshold(threshold_energy, max_values):\n",
    "    \"\"\"\n",
    "    This function returns a dataframe of shape (3,1) containing what is the treshold for each X,Y,Z axis \n",
    "    depending on threshold_energy. \n",
    "\n",
    "    For example if threshold_energy=10, the function is going to return what is 10% of the max_values\n",
    "\n",
    "    Arguments:\n",
    "    - threshold_energy: Percentage of the max values we want to use as treshold \n",
    "    - max_values: Dataframe of the max values \n",
    "    \"\"\"\n",
    "    return (max_values * threshold_energy) / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean offset removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_inactivity_mean_offset(df_train_label):\n",
    "    \"\"\"\n",
    "    Removal of inactivity segments detected with a threshold after the energy is centered over the mean\n",
    "    This function is expected to be less efficient than removal of inactivity with the highpass threshold \n",
    "    \n",
    "    Keyword arguments: \n",
    "    - df_train_label: DataFrame containing the following columns \n",
    "            [measurement_id, subject_id, on_off, tremor, dyskenisia]\n",
    "    \"\"\"\n",
    "    for idx in df_train_label.index:\n",
    "        df_train_data = pd.read_csv(\n",
    "            path_train_data + df_train_label[\"measurement_id\"][idx] + \".csv\"\n",
    "        )\n",
    "\n",
    "        ### Following section works on removing offset with mean\n",
    "        inputData_DCRemoved = df_train_data.iloc[:, -3:] - df_train_data.iloc[\n",
    "            :, -3:\n",
    "        ].mean(axis=0)\n",
    "\n",
    "        # inputData with DC Removed only has NaN for the Timestamp values, so we are adding them\n",
    "        inputData_DCRemoved.insert(0, \"Timestamp\", df_train_data[\"Timestamp\"])\n",
    "\n",
    "        # Plot the graph\n",
    "        great_title = get_plot_title(idx, df_train_label)\n",
    "        print(df_train_label[\"measurement_id\"][idx])\n",
    "        inputData_DCRemoved.plot(\n",
    "            x=\"Timestamp\", legend=True, subplots=True, title=great_title\n",
    "        )\n",
    "\n",
    "        ### Following section works on removing inactivity following a treshold\n",
    "        # Get the absolute max values for X, Y, Z\n",
    "        max_values = inputData_DCRemoved.iloc[:, -3:].abs().max()\n",
    "\n",
    "        # Compute what is X% of that max\n",
    "        df_treshold = get_df_threshold(10, max_values)\n",
    "\n",
    "        df_candidates = inputData_DCRemoved[\n",
    "            (inputData_DCRemoved.X.abs() <= df_treshold[\"X\"])\n",
    "            & (inputData_DCRemoved.Y.abs() <= df_treshold[\"Y\"])\n",
    "            & (inputData_DCRemoved.Z.abs() <= df_treshold[\"Z\"])\n",
    "        ]\n",
    "\n",
    "        print(\"Candidates to be removed:\")\n",
    "        display(df_candidates)\n",
    "\n",
    "        filter_df = inputData_DCRemoved[\n",
    "            ~inputData_DCRemoved.isin(df_candidates)\n",
    "        ].dropna(how=\"all\")\n",
    "        great_title = \"filter_df for : \" + great_title\n",
    "        filter_df.plot(x=\"Timestamp\", legend=True, subplots=True, title=great_title)\n",
    "\n",
    "        # FIXME: This function is not done. As it's not the priority, i'm switching to work on highpass filter\n",
    "        # It doesn't remove the identified candidates or save the 0/1 vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High pass filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source of the code about the highpass: https://gist.github.com/junzis/e06eca03747fc194e322\n",
    "# A bit more explanation here: https://medium.com/analytics-vidhya/how-to-filter-noise-with-a-low-pass-filter-python-885223e5e9b7\n",
    "\n",
    "\n",
    "def butter_highpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype=\"high\", analog=False)\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def butter_highpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_highpass(cutoff, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "def remove_inactivity_highpass(\n",
    "    df_train_label,\n",
    "    energy_threshold,\n",
    "    duration_threshold,\n",
    "    mask_path,\n",
    "    plot_frequency_response=False,\n",
    "    plot_accelerometer_after_removal=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Removes inactivity according to a high pass filter. It will only be applied to the measurement_id provided\n",
    "    in the df_train_label variable. A first condition for a measurement to be removed at a certain timestamp\n",
    "    is that first, the energy is less than the energy_treshold. After that, we identify candidates with a vector\n",
    "    where 0 represents a timestamp we want to keep, and 1 represents timestamps we detected as below the minimum \n",
    "    energy threshold. \n",
    "    \n",
    "    The second threshold, called duration_threshold, represents the condition that there must be a minimum \n",
    "    number of consecutives candidates to be removed befoer the candidates will be indeed removed and confirmed\n",
    "    as inactivity. For example, we could decide to only remove sections that are at least 1 minute long of \n",
    "    inactivity detected.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    - df_train_label: DataFrame containing the following columns \n",
    "            [measurement_id, subject_id, on_off, tremor, dyskenisia]\n",
    "    - energy_threshold: what percentage of the max energy do we consider as inactivity?\n",
    "        For example, 1 of the max is considered as inactivity\n",
    "    - duration_threshold: how long do we want to have inactivity before we remove it? \n",
    "        For example 3000x0.02ms=1min of inactivity minimum before those candidates are removed\n",
    "    - mask_path: Path where to save the mask \n",
    "    - plot_frequency_response: Optional. {True, False}. \n",
    "                               Flag to determine if we want to plot the frequency response or not\n",
    "    -plot_accelerometer_after_removal: Optinal. {True, False}.\n",
    "                                Flag to determine if we want to plot the accelerometer after the inactivity\n",
    "                                is removed\n",
    "    \"\"\"\n",
    "    # Filter requirements.\n",
    "    order = 10\n",
    "    fs = 50.0  # sample rate, Hz\n",
    "    cutoff = 0.5  # 3.667  # desired cutoff frequency of the filter, Hz\n",
    "\n",
    "    # Get the filter coefficients so we can check its frequency response.\n",
    "    b, a = butter_highpass(cutoff, fs, order)\n",
    "\n",
    "    # Load every training file for each \"row of labels\" we have loaded in df_train_label\n",
    "    for idx in df_train_label.index:\n",
    "        # Load the training data\n",
    "        try:\n",
    "            df_train_data = pd.read_csv(path_train_data + df_train_label[\"measurement_id\"][idx] + \".csv\")\n",
    "            print('Working on ', df_train_label[\"measurement_id\"][idx])\n",
    "        except FileNotFoundError:\n",
    "            print(path_train_data)\n",
    "            print('Skipping ' + df_train_label[\"measurement_id\"][idx] +\n",
    "                  ' as it doesn\\'t exist for ' +\n",
    "                  data_real_subtype)\n",
    "            continue\n",
    "        # Set the time axis. It's not the same name for the two databases\n",
    "        x_axis_data_type = \"t\" if data_type == \"real\" else \"Timestamp\"\n",
    "        t = df_train_data[x_axis_data_type]\n",
    "\n",
    "        # Filter the data\n",
    "        # X = [:,-3], Y = [:,-2], Z = [:,-1]\n",
    "        X_filtered_data = butter_highpass_filter(df_train_data.iloc[:,-3], cutoff, fs, order)\n",
    "        Y_filtered_data = butter_highpass_filter(df_train_data.iloc[:,-2], cutoff, fs, order)\n",
    "        Z_filtered_data = butter_highpass_filter(df_train_data.iloc[:,-1], cutoff, fs, order)\n",
    "\n",
    "        ### Following section works on removing inactivity following a treshold\n",
    "        # Get the absolute max values for X, Y, Z\n",
    "        # FIXME: This could be made better but I won't lose time on this now\n",
    "        # Get in a Series format because that's what the threshold function is expecting\n",
    "        max_values = pd.Series(np.array([\n",
    "                    np.abs(X_filtered_data).max(),\n",
    "                    np.abs(Y_filtered_data).max(),\n",
    "                    np.abs(Z_filtered_data).max(),\n",
    "                ]))\n",
    "\n",
    "        # Get the threshold of the filtered data\n",
    "        df_treshold = get_df_threshold(energy_threshold, max_values)\n",
    "#         print('df threshold : ', df_treshold)\n",
    "        # Get 0/1 candidates\n",
    "        X_zeros = np.abs(X_filtered_data) <= df_treshold[0]\n",
    "        Y_zeros = np.abs(Y_filtered_data) <= df_treshold[1]\n",
    "        Z_zeros = np.abs(Z_filtered_data) <= df_treshold[2]\n",
    "        \n",
    "#         display(X_zeros)\n",
    "#         display(Y_zeros)\n",
    "#         display(Z_zeros)\n",
    "        \n",
    "        # Change data from boolean to 0 and 1s (int)\n",
    "        X_zeros = X_zeros.astype(int)\n",
    "        Y_zeros = Y_zeros.astype(int)\n",
    "        Z_zeros = Z_zeros.astype(int)\n",
    "        # AND operand to identify candidates across all axis\n",
    "        # FIXME: change name of variable bc it's not a df\n",
    "        df_zeros = X_zeros & Y_zeros & Z_zeros\n",
    "\n",
    "\n",
    "        # Check if it reaches the time treshold (like minimum 1 minute long to be removed)\n",
    "        start = 0  # Start and end of the series of 1\n",
    "        end = 0\n",
    "        indices_list = []  # List of tuples\n",
    "        howmany = 0  # How many groups we identified (not required, just nice metric)\n",
    "        count = 0  # How many 1 in a row we found\n",
    "\n",
    "        # Counts the number of 0s and 1s in the original data before we apply the threshold\n",
    "#         unique, counts = np.unique(df_zeros, return_counts=True)\n",
    "#         print(dict(zip(unique, counts)))\n",
    "\n",
    "        # Change the candidates for removal (1) to 0 if there are not enough 1s in a row to reach the\n",
    "        # threshold. For example there needs to be 3000 times 1s in a row for 1 minute of \"inactivity\"\n",
    "        # to be removed\n",
    "        for i in range(0, len(df_zeros)):\n",
    "            if df_zeros[i] == 1:\n",
    "                count = count + 1\n",
    "            else:\n",
    "                if count >= duration_threshold:\n",
    "                    start = i - count\n",
    "                    end = i - 1\n",
    "                    indices_list.append((start, end))\n",
    "                    howmany = howmany + 1\n",
    "                    count = 0\n",
    "                elif (\n",
    "                    count >= 1\n",
    "                ):  # if it doesn't reach the threshold, we change the 1 for 0 because we don't want to remove those\n",
    "                    start = i - count\n",
    "                    end = i\n",
    "                    df_zeros[start:end] = [0] * (end - start)\n",
    "                    count = 0\n",
    "\n",
    "        #print(\"There are \"+ str(howmany) + \" groups identified as candidates to be removed\")\n",
    "\n",
    "        # Counts the number of 0s and 1s in the data after we applied the threshold\n",
    "#         unique, counts = np.unique(df_zeros, return_counts=True)\n",
    "#         print(dict(zip(unique, counts)))\n",
    "\n",
    "        # Save 0/1 candidates to csv\n",
    "        # I use 1-df_zeros to swap the 0s and 1s.\n",
    "        # 1: we want to keep this measure\n",
    "        # 0: detected as inactivity so we want to remove it\n",
    "        # Previously, it was the opposite, as 1s were considered inactivity\n",
    "        df_mask_highpass = pd.DataFrame(1 - df_zeros)\n",
    "        df_mask_highpass.to_csv(\n",
    "            mask_path + df_train_label[\"measurement_id\"][idx] + \".csv\",\n",
    "            index=False,\n",
    "            header=False,\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "        # Plot the accelerometer with the removed sections\n",
    "        # The [0] is used to get a pandas.Series instead of a DataFrame\n",
    "        # We insert Timestamp again as it was removed for the filtering\n",
    "        # BUG : Should I use df_train_data or the filtered high pass dataframe?\n",
    "        if plot_accelerometer_after_removal:\n",
    "            filtered_df = df_train_data.iloc[:, -3:].multiply(df_mask_highpass[0], axis=0)\n",
    "            print('LEN FILTERED DF : ', len(filtered_df))\n",
    "            filtered_df.insert(0, x_axis_data_type, df_train_data[x_axis_data_type])\n",
    "            great_title = get_plot_title(idx, df_train_label)\n",
    "            filtered_df.plot(\n",
    "                x=x_axis_data_type, legend=True, subplots=True, title=great_title\n",
    "            )\n",
    "            display(filtered_df)\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "            plt.cla()\n",
    "            plt.close()\n",
    "\n",
    "        # Plot the frequency response, and plot both the original and filtered signals for X, Y and Z.\n",
    "        if plot_frequency_response:\n",
    "            # TODO: Make the graphs bigger\n",
    "            w, h = freqz(b, a, worN=8000)\n",
    "            plt.subplot(4, 1, 1)\n",
    "            plt.plot(0.5 * fs * w / np.pi, np.abs(h), \"b\")\n",
    "            plt.plot(cutoff, 0.5 * np.sqrt(2), \"ko\")\n",
    "            plt.axvline(cutoff, color=\"k\")\n",
    "            plt.xlim(0, 0.5 * fs)\n",
    "            plt.title(\"Highpass Filter Frequency Response\")\n",
    "            plt.xlabel(\"Frequency [Hz]\")\n",
    "            plt.grid()\n",
    "\n",
    "            # Plot X\n",
    "            plt.subplot(4, 1, 2)\n",
    "            plt.plot(t, df_train_data.iloc[:,-3], \"b-\", label=\"X\")\n",
    "            plt.plot(t, X_filtered_data, \"g-\", linewidth=2, label=\"filtered X\")\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "\n",
    "            # Plot Y\n",
    "            plt.subplot(4, 1, 3)\n",
    "            plt.plot(t, df_train_data.iloc[:,-2], \"b-\", label=\"Y\")\n",
    "            plt.plot(t, Y_filtered_data, \"g-\", linewidth=2, label=\"filtered Y\")\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(4, 1, 4)\n",
    "            plt.plot(t, df_train_data.iloc[:,-1], \"b-\", label=\"Z\")\n",
    "            plt.plot(t, Z_filtered_data, \"g-\", linewidth=2, label=\"filtered Z\")\n",
    "            plt.legend()\n",
    "\n",
    "            plt.xlabel(\"Time [sec]\")\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplots_adjust(hspace=0.7)\n",
    "            plt.show()\n",
    "\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "            plt.cla()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(measurement_id, mask_path):\n",
    "    \"\"\"\n",
    "    Apply a mask on the list of measurement_ids provided through df_train_label \n",
    "    \n",
    "    Keyword arguments:\n",
    "    - df_train_label: DataFrame containing the following columns \n",
    "            [measurement_id, subject_id, on_off, tremor, dyskenisia]\n",
    "    \"\"\"\n",
    "    # Load the training data\n",
    "    print('measurement_id ', measurement_id)\n",
    "    print('mask_path + measurement_id + \".csv\" : ', mask_path + measurement_id + \".csv\")\n",
    "    try:\n",
    "        df_train_data = pd.read_csv(path_train_data + measurement_id + \".csv\")\n",
    "        df_mask = pd.read_csv(mask_path + measurement_id + \".csv\", header=None)\n",
    "    except FileNotFoundError:\n",
    "        print('Skipping ' + df_train_label[\"measurement_id\"][idx] +\n",
    "              ' as it doesn\\'t exist for ' +\n",
    "              data_real_subtype)\n",
    "        pass\n",
    "\n",
    "\n",
    "    # multiply df_train_data by mask so the values to be removed are at 0\n",
    "    #         np.multiply(df2.iloc[:,-3:],df_mask.iloc[:,-1:])\n",
    "    df_train_data.iloc[:, -3:] = np.multiply(df_train_data.iloc[:, -3:], df_mask)#[:, -1:])\n",
    "\n",
    "    #         display(df_train_data)\n",
    "\n",
    "    # Drop the 0 values from the training DataFrame\n",
    "    df_train_data = df_train_data[(df_train_data.iloc[:, -3:].T != 0).any()]\n",
    "    df_train_data.reset_index(drop=True, inplace=True)\n",
    "    return df_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get derivative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_derivative_value(df_train_data_context, m):\n",
    "    \"\"\"\n",
    "    TODO \n",
    "    \n",
    "    Keyword arguments:\n",
    "    - df_train_data_context: TODO\n",
    "    - m: TODO \n",
    "    \"\"\"\n",
    "    # Dot Product is the sum of the point wise multiplications between a and m\n",
    "    cij = np.dot(df_train_data_context, m)\n",
    "    denum = np.dot(m, m)\n",
    "    return cij / denum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_derivative(measurement_id, derivative_path, n_zero=3, padding=False, mask_path=None):\n",
    "    \"\"\"\n",
    "    TODO \n",
    "    \n",
    "    cis-pd.training_data.velocity_original_data: Original data, which means the inactivity is untouched \n",
    "    \n",
    "    Keyword arguments:\n",
    "    - df_train_label: TODO\n",
    "    - derivative_path: TODO\n",
    "    - n_zero: TODO\n",
    "    - Padding: [False, True] \n",
    "      - If True, it will add a padding of [0,0,0] at the beginning and at the end of the training data\n",
    "      - If False, it will just use the existing values of the training data to have a (7,) vector from the\n",
    "        training data\n",
    "    - mask_path: Optinal. If provided, it will apply the high pass mask on the training data \n",
    "    \"\"\"\n",
    "    # m is a vector. For n_zero, it will be [-3, -2, -1, 0, 1, 2, 3]\n",
    "    m = np.linspace(-n_zero, n_zero, num=2 * n_zero + 1)\n",
    "\n",
    "    file_path= data_dir+derivative_path+ measurement_id + '.csv'\n",
    "    if os.path.isfile(file_path):\n",
    "        print (\"File exist : \", file_path)\n",
    "        return\n",
    "\n",
    "    # Load the training data\n",
    "    if mask_path is not None:\n",
    "        df_train_data = apply_mask(measurement_id, mask_path)\n",
    "    else: # Going to get the first derivative from the original data \n",
    "        try:\n",
    "            df_train_data = pd.read_csv(path_train_data + measurement_id + \".csv\")\n",
    "        except FileNotFoundError:\n",
    "            print('Skipping ' + df_train_label[\"measurement_id\"][idx] +\n",
    "                  ' as it doesn\\'t exist for ' +\n",
    "                  data_real_subtype)\n",
    "\n",
    "    df_velocity = []\n",
    "\n",
    "    if padding:\n",
    "        # Padding DataFrame to add 3 empty rows at the beginning and at the end of the training data\n",
    "        df_padding = []\n",
    "\n",
    "        # FIXME: This could probably be made faster but I won't lose time on this\n",
    "        df_padding.insert(0, {\"Timestamp\": -1, \"X\": 0, \"Y\": 0, \"Z\": 0})\n",
    "        df_padding.insert(0, {\"Timestamp\": -1, \"X\": 0, \"Y\": 0, \"Z\": 0})\n",
    "        df_padding.insert(0, {\"Timestamp\": -1, \"X\": 0, \"Y\": 0, \"Z\": 0})\n",
    "\n",
    "        df_train_data = pd.concat(\n",
    "            [pd.DataFrame(df_padding), df_train_data], ignore_index=True\n",
    "        )\n",
    "        df_padding = pd.DataFrame({\"Timestamp\": [-1, -1, -1],\n",
    "                                    \"X\": [0, 0, 0],\n",
    "                                    \"Y\": [0, 0, 0],\n",
    "                                    \"Z\": [0, 0, 0]})\n",
    "\n",
    "        df_train_data_padding = df_train_data.append(df_padding, ignore_index=True)\n",
    "    else:  # FIXME remove this it's just a quickfix because there is the option with or without padding\n",
    "        # and the next loop has to be on the original dataframe withtout padding\n",
    "        df_train_data_padding = df_train_data\n",
    "\n",
    "    # BUG : This df_velocity contains 3 extra rows. I'm not sure where they come from \n",
    "    for row in df_train_data[[\"X\", \"Y\", \"Z\"]].itertuples():\n",
    "        end = row.Index + n_zero\n",
    "\n",
    "        start = row.Index - n_zero\n",
    "\n",
    "        # QUICKFIX to the padding and pointers issue \n",
    "        if start == -3:\n",
    "            start = 0\n",
    "            end = 6\n",
    "        elif start == -2:\n",
    "            start = 1\n",
    "            end = 7\n",
    "        elif start == -1:\n",
    "            start = 2\n",
    "            end = 8\n",
    "        elif end > len(df_train_data) - 1 and not padding:\n",
    "            end = len(df_train_data)\n",
    "            start = len(df_train_data) - (2 * n_zero)\n",
    "\n",
    "        df_velocity.append([get_derivative_value(df_train_data_padding.loc[start:end, \"X\"], m),\n",
    "                             get_derivative_value(df_train_data_padding.loc[start:end, \"Y\"], m),\n",
    "                             get_derivative_value(df_train_data_padding.loc[start:end, \"Z\"], m)])\n",
    "\n",
    "    # Build the DataFrame with all the columns together so we can save it to CSV\n",
    "    df_velocity = pd.DataFrame(df_velocity, columns=['X','Y','Z'])\n",
    "\n",
    "    df_velocity.to_csv(\n",
    "        data_dir + derivative_path + measurement_id + \".csv\",\n",
    "        index=False,\n",
    "        header=[\"X_velocity\", \"Y_velocity\", \"Z_velocity\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write WAV File for Kaldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_wav(measurement_id, wav_path, mask_path):\n",
    "    file_path= wav_path + measurement_id + '.wav'\n",
    "    print('write_wav path : ', file_path)\n",
    "    if os.path.isfile(file_path):\n",
    "        # FIX ME: it doesn't go here?\n",
    "        print (\"File exist : \", file_path)\n",
    "        return\n",
    "    \n",
    "    df_train_data = apply_mask(measurement_id,\n",
    "                               mask_path)\n",
    "    print(df_train_data.shape)\n",
    "    # Save to WAV\n",
    "    samplerate = 8000 # Hz, we need 8kHz for Kaldi to be happy \n",
    "\n",
    "    # We're only writing to WAV file the X axis which is located at [:,1:2]\n",
    "#     print('type(wav_path) : ' ,type(wav_path))\n",
    "#     print('type(measurement_id) : ', type(measurement_id))\n",
    "#     print(df_train_data.iloc[:,1:2].shape)\n",
    "#     print(type(df_train_data.iloc[:,1:2].to_numpy()))\n",
    "    write(wav_path +\n",
    "          measurement_id + '.wav', samplerate, df_train_data.iloc[:,1:2].to_numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to print accelerometers before/after and write a wav file for 1 file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-118-d679edd4007c>, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-118-d679edd4007c>\"\u001b[0;36m, line \u001b[0;32m29\u001b[0m\n\u001b[0;31m    Plot accelerometer\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "data_type = \"cis\"\n",
    "path_train_data, df_train_label = define_data_type(data_type=data_type)\n",
    "\n",
    "list_measurement_id = ['db2e053a-0fb8-4206-891a-6f079fb14e3a']\n",
    "\n",
    "\n",
    "df_train_label = interesting_patients(df_train_label=df_train_label, list_measurement_id=list_measurement_id)\n",
    "\n",
    "# Plot the accelerometer data\n",
    "plot_accelerometer(data_type=data_type, path_accelerometer_plots=path_save_accelerometer_plots)\n",
    "\n",
    "remove_inactivity_highpass(\n",
    "    df_train_label,\n",
    "    energy_threshold=10,\n",
    "    duration_threshold=3000,\n",
    "    plot_frequency_response=True,\n",
    "    plot_accelerometer_after_removal=True,\n",
    "    mask_path='/home/sjoshi/codes/python/BeatPD/data/BeatPD/cis-pd.training_data.high_pass_mask/')\n",
    "\n",
    "\n",
    "# Apply filter \n",
    "for idx in df_train_label.index:\n",
    "    df_train_data = apply_mask(df_train_label[\"measurement_id\"][idx],\n",
    "                               mask_path='/home/sjoshi/codes/python/BeatPD/data/BeatPD/cis-pd.training_data.high_pass_mask/')\n",
    "    print('len : ', len(df_train_data))\n",
    "    great_title = get_plot_title(idx, df_train_label)\n",
    "    \n",
    "    print('AFTER REMOVAL')\n",
    "    Plot accelerometer \n",
    "    print('len : ', len(df_train_data))\n",
    "    x_axis_data_type = \"t\" if data_type == \"real\" else \"Timestamp\"\n",
    "    df_train_data.plot(\n",
    "                    x=x_axis_data_type, legend=True, subplots=True, title=great_title\n",
    "                )\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIS-PD Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contains 16 subject_id (patients) for the training set \n",
    "\n",
    "- Gender: 11 Male, 5 Female \n",
    "- Race: 15 White, 1 NA\n",
    "- Ethnicity: 15 Not Hispanic or Latino, 1 Unknown\n",
    "- Age average (standard deviation) : 62.8125 (10.8579)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIS-PD: Basic accelerometer visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_type = \"cis\"\n",
    "\n",
    "# TODO: explain\n",
    "path_train_data, df_train_label = define_data_type(data_type=data_type)\n",
    "# display(df_train_label)\n",
    "# List of interesting measurement id we want to look at\n",
    "# list_measurement_id=[#'ab5287f4-8261-47ad-8ff2-22b5fe5d246e',\n",
    "#'db2e053a-0fb8-4206-891a-6f079fb14e3a']#,\n",
    "# 'ef5b1267-c212-46c5-aab0-4f4437bc6c67',\n",
    "# '4ec74fb9-7347-435d-83dc-79ad74c3bc49',\n",
    "# '8e8539ad-8841-476b-b15c-888ce3461989',\n",
    "# '22b88456-fe8f-4138-af55-be12afca4b81',\n",
    "# 'ad84583d-e5ae-4926-b077-531a0f7d08a9',\n",
    "# 'eef56825-940a-4c3e-aebb-60838d60869e',\n",
    "# 'e0441156-c4b8-467c-8f4f-3b532d594d8f',\n",
    "# '464ac314-6c4b-4c4a-957c-28a2339150d6']\n",
    "\n",
    "list_measurement_id = [\n",
    "    \"5cf68c8e-0b7a-4b73-ad4f-015c7a20fb5a\",\n",
    "    \"cc7b822c-e310-46f0-a8ea-98c95fdb67a1\",\n",
    "    \"5163afe8-a6b0-4ea4-b2ba-9b4501dd5912\",\n",
    "    \"db2e053a-0fb8-4206-891a-6f079fb14e3a\",\n",
    "    \"2d852742-10a9-4c56-9f38-779f2cd66879\",\n",
    "    \"2e3a4c9c-ff01-4a28-bfcf-ce9b7633a39d\",  # no inactivity should be removed\n",
    "    \"3cf49c01-0499-4bad-9167-67691711204a\",  # no inactivity should be removed PAS LA??\n",
    "    \"3d0f965c-9d72-43d1-9369-1ea3acf963cc\",  # PAS LA ???\n",
    "    \"4b269cc2-8f0c-4816-adbf-10c0069b8833\",\n",
    "    \"4bc51b90-bfce-4231-85e1-5de3b4bc0745\",\n",
    "    \"4fc3c295-857f-4920-8fa5-f21bfdc7ab4f\",\n",
    "]  # bit of inactivity in the middle]\n",
    "\n",
    "list_measurement_id = [\n",
    "    \"2d852742-10a9-4c56-9f38-779f2cd66879\",\n",
    "    \"4fc3c295-857f-4920-8fa5-f21bfdc7ab4f\",\n",
    "    \"db2e053a-0fb8-4206-891a-6f079fb14e3a\",\n",
    "]\n",
    "\n",
    "# Filter df_train_label according to the measurement_id we are most interested in\n",
    "df_train_label = interesting_patients(\n",
    "    df_train_label=df_train_label, list_measurement_id=list_measurement_id\n",
    ")\n",
    "\n",
    "# Display filtered df_train_label\n",
    "display(df_train_label)\n",
    "\n",
    "# path_no_inactivity_data = remove_inactivity_pct_change(df_train_label)\n",
    "\n",
    "# Plot the accelerometer data\n",
    "plot_accelerometer(\n",
    "    data_type=data_type, path_accelerometer_plots=path_save_accelerometer_plots\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIS-PD: Create Masks for inactivity removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sjoshi/codes/python/BeatPD/data/BeatPD/cis-pd.ancillary_data/\n"
     ]
    }
   ],
   "source": [
    "data_type = \"cis\"\n",
    "# This is only to switch between training_data or ancillary_data which is additional data provided \n",
    "training_or_ancillary_data='ancillary_data' #training_data\n",
    "path_train_data, df_train_label = define_data_type(data_type, training_or_ancillary_data)\n",
    "\n",
    "print(path_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "remove_inactivity_highpass(\n",
    "    df_train_label,\n",
    "    energy_threshold=5,\n",
    "    duration_threshold=3000,\n",
    "    plot_frequency_response=False,\n",
    "    mask_path='/home/sjoshi/codes/python/BeatPD/data/BeatPD/cis-pd.'+\n",
    "    training_or_ancillary_data+'.high_pass_mask/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIS-PD: Create first derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_work = partial(\n",
    "    get_first_derivative, \n",
    "    derivative_path=\"cis-pd.training_data.velocity_original_data/\",\n",
    "    padding=True, \n",
    "    mask_path='/home/sjoshi/codes/python/BeatPD/data/BeatPD/cis-pd.training_data.high_pass_mask/'\n",
    ")\n",
    "\n",
    "num_jobs = 8\n",
    "with ProcessPoolExecutor(num_jobs) as ex:\n",
    "    results = hlist(ex.map(do_work, df_train_label['measurement_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIS-PD: Create WAV files - Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_or_ancillary='ancillary_data' \n",
    "\n",
    "do_work = partial(\n",
    "    write_wav, \n",
    "    wav_path=data_dir+'cis-pd.'+training_or_ancillary+'.wav/',\n",
    "    mask_path=data_dir+'cis-pd.'+training_or_ancillary+'.high_pass_mask/'\n",
    ")\n",
    "\n",
    "num_jobs = 8\n",
    "with ProcessPoolExecutor(num_jobs) as ex:\n",
    "    results = list(ex.map(do_work, df_train_label['measurement_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_type = \"cis\"\n",
    "\n",
    "path_train_data, df_train_label = define_data_type(data_type=data_type)\n",
    "\n",
    "list_measurement_id = [\"5cf68c8e-0b7a-4b73-ad4f-015c7a20fb5a\"]\n",
    "\n",
    "df_train_label = interesting_patients(\n",
    "    df_train_label=df_train_label, list_measurement_id=list_measurement_id\n",
    ")\n",
    "\n",
    "path_no_inactivity_data = remove_inactivity_pct_change(df_train_label)\n",
    "\n",
    "# Plot the accelerometer data\n",
    "# plot_accelerometer(data_type=data_type, path_accelerometer_plots=path_save_accelerometer_plots, path_inactivity=path_no_inactivity_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of the length of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_type = \"cis\"\n",
    "\n",
    "path_train_data, df_train_label = define_data_type(data_type=data_type)\n",
    "\n",
    "len_distribution = []\n",
    "for idx in df_train_label.index:\n",
    "        df_train_data = pd.read_csv(path_train_data + df_train_label[\"measurement_id\"][idx] + \".csv\")\n",
    "        len_distribution.append(len(df_train_data))\n",
    "\n",
    "\n",
    "num_bins = 10\n",
    "n, bins, patches = plt.hist(len_distribution, num_bins, facecolor='blue', alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min :  5803\n",
      "max :  61960\n"
     ]
    }
   ],
   "source": [
    "print('min : ', min(len_distribution))\n",
    "print('max : ', max(len_distribution))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Occurences of each symptoms for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the occurences of each symptoms for each patient\n",
    "df_occurences, df_train_label_subject_id = compute_symptoms_occurences_dataframe(\n",
    "    df_train_label=df_train_label\n",
    ")\n",
    "\n",
    "# Plot the graphs\n",
    "plot_symptoms_occurences(\n",
    "    df_occurences=df_occurences, df_train_label_subject_id=df_train_label_subject_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_fold(df_train_label, n_splits=5, subject_id=None, data_real_subtype=\"\"):\n",
    "    \"\"\"\n",
    "    Function that returns a list of X dataframes (X is according to the number of n_splits chosen)\n",
    "\n",
    "    The dataframes are the labels needed according to the split \n",
    "\n",
    "    Keyword Arguments:\n",
    "    df_train_label: Dataframe containing the labels\n",
    "    n_split: Optional. The number of folds. Default: 5\n",
    "    subject_id: Optional. Specify a subject_id to get measurement_id only for that subject_id \n",
    "    data_real_subtype: Only for REAL-PD database\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits)\n",
    "\n",
    "    # Building the dataframe to split\n",
    "    X = []\n",
    "\n",
    "    # if we want the data split for one specific subject_id\n",
    "    if subject_id:\n",
    "        df_train_label_subject_id = df_train_label.groupby(\"subject_id\")\n",
    "        X = df_train_label_subject_id.get_group(subject_id)\n",
    "\n",
    "    # if we want to have all a split for all data no matter the subject_id\n",
    "    # NOTE: I didn't make sure to have one subject_id represented in both train/test\n",
    "    else:\n",
    "        for idx in df_train_label.index:\n",
    "            X.append([df_train_label[\"measurement_id\"][idx]])\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    kf.get_n_splits(X)\n",
    "\n",
    "    # Building lists of df_train_label because we have by default 5 splits,\n",
    "    # so the lists will contain 5 DataFrames with different split indices required\n",
    "    list_df_train_label = list()\n",
    "    list_df_test_label = list()\n",
    "    split_idx = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        df_train_label = X.iloc[train_index]\n",
    "        df_test_label = X.iloc[test_index]\n",
    "\n",
    "        list_df_train_label.append(df_train_label)\n",
    "        list_df_test_label.append(df_test_label)\n",
    "\n",
    "        # name of the file according to its database and type\n",
    "        path_save_k_fold_dataframes = (\n",
    "            data_dir + data_type + \"-pd.training_data.k_fold/\" + data_real_subtype + \"/\"\n",
    "        )\n",
    "        df_train_label.to_csv(\n",
    "            path_save_k_fold_dataframes\n",
    "            + str(subject_id)\n",
    "            + \"_train_kfold_\"\n",
    "            + str(split_idx)\n",
    "            + \".csv\",\n",
    "            index=False,\n",
    "            header=[\"measurement_id\", \"subject_id\", \"on_off\", \"dyskinesia\", \"tremor\"],\n",
    "        )\n",
    "        df_test_label.to_csv(\n",
    "            path_save_k_fold_dataframes\n",
    "            + str(subject_id)\n",
    "            + \"_test_kfold_\"\n",
    "            + str(split_idx)\n",
    "            + \".csv\",\n",
    "            index=False,\n",
    "            header=[\"measurement_id\", \"subject_id\", \"on_off\", \"dyskinesia\", \"tremor\"],\n",
    "        )\n",
    "        split_idx = split_idx + 1\n",
    "    return list_df_train_label, list_df_test_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the K-Fold files for the CIS database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the data type as we have two databases\n",
    "data_type = \"cis\"\n",
    "path_train_data, df_train_label = define_data_type(data_type=data_type)\n",
    "display(df_train_label)\n",
    "\n",
    "# Group data by subject_id\n",
    "df_train_label_subject_id = df_train_label.groupby(\"subject_id\")\n",
    "\n",
    "# Go through the subject_id and k-fold their data\n",
    "# FIXME: get_k_fold could me renamed to just create the folds, save them, not return anything\n",
    "for subject_id, value in df_train_label_subject_id:\n",
    "    list_df_train_label, list_df_test_label = get_k_fold(\n",
    "        df_train_label=df_train_label, n_splits=5, subject_id=subject_id\n",
    "    )\n",
    "\n",
    "\n",
    "#### Example on how to read data\n",
    "\n",
    "# TODO\n",
    "# Read data from folders\n",
    "\n",
    "# The following is not a really good example, it would be better to have an example where we read the\n",
    "# data from the folder they're saved at\n",
    "\n",
    "\n",
    "# Iterate through the DataFrame in the list list_df_train_label_train\n",
    "# for df_train_label in list_df_train_label_train:\n",
    "#     # Go through the measurement_id for this df_train_label split to read the training data corresponding\n",
    "#     for idx in df_train_label.index:\n",
    "#         # Read the train data for this specific measurement_id\n",
    "#         # TODO: Do we want to append all of this training data together?\n",
    "#         df_train_data=pd.read_csv(path_train_data+df_train_label[\"measurement_id\"][idx]+'.csv')\n",
    "#         #display(df_train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the K-Fold data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the K-Fold Files for the REAL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate the files, you have to uncomment one data_real_subtype at a time and\n",
    "# execute this cell 3 times for the 3 subtypes.\n",
    "\n",
    "data_type = \"real\"\n",
    "# data_real_subtype='smartphone_accelerometer'\n",
    "data_real_subtype = \"smartwatch_accelerometer\"\n",
    "# data_real_subtype='smartwatch_gyroscope'\n",
    "\n",
    "path_train_data, df_train_label = define_data_type(data_type=data_type)\n",
    "\n",
    "# Group data by subject_id\n",
    "df_train_label_subject_id = df_train_label.groupby(\"subject_id\")\n",
    "\n",
    "# Go through the subject_id and k-fold their data\n",
    "for subject_id, value in df_train_label_subject_id:\n",
    "    list_df_train_label, list_df_test_label = get_k_fold(\n",
    "        df_train_label=df_train_label,\n",
    "        n_splits=5,\n",
    "        subject_id=subject_id,\n",
    "        data_real_subtype=data_real_subtype,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REAL-PD Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This database, originally named \"Parkinson@Home\" is renamed to \"Real-PD\" for this challenge. The study was made over 2 weeks, with at home monitoring. \n",
    "\n",
    "The devices used are an android phone, a motorolla watch. \n",
    "- `smartwatch_accelerometer` and `smartwatch_gyroscope` : Motorolla Watch\n",
    "- `smartphone_accelerometer` : Android phone \n",
    "\n",
    "-> Question: so is the smartwatch & smartphone accelerometer should measure the same movements? \n",
    "\n",
    "The REAL-PD database has many missing values. \n",
    "\n",
    "The subject_id `hbv013` is the only one without missing data. Other patients all have at least one missing symptom (`diskenisia`, ) or two (`on/off and tremor`, `on_off and dyskinesia`, `dyskinesia and tremor`) missing.\n",
    "\n",
    "Measurements id with no data (`on_off`, `dyskinesia` and `tremor` are all missing):\n",
    "- `b50d1b0c-2cd1-45f8-9097-0742e5cbbcc8`\n",
    "- `b598c177-4e38-4ea8-8543-bd8f7e580f96`\n",
    "- `cf841bf8-0082-4ea3-999f-1f43e39a8dc6`\n",
    "- `b1e15f8a-109f-459b-ba87-46899240ee66`\n",
    "- `6f0e2580-56ec-4743-9356-d3e4d9a0aee5`\n",
    "- `773536f6-9b70-43d0-b099-5d167d74924a`\n",
    "- `54a0e841-ad45-4ba7-ac83-1785c5f7748b`\n",
    "- `cd9ed2e2-7e04-44c7-b041-7788f133c193`\n",
    "- `a6954a91-338b-4523-9e4a-5e69a8fac206`\n",
    "\n",
    "The 3 symptoms are reported as follows in this dataset: \n",
    "- `on_off = {0,1}`\n",
    "  - `Off` : 0 (medication is wearing off) \n",
    "  - `On` : 1 (medication is working)\n",
    "  \n",
    "- `dyskinesia = {0,1,2}`\n",
    "  - Without dyskinesia: 0 \n",
    "  - Non-troublesome dyskinesia: 1 \n",
    "  - Severe dyskinesia: 2 \n",
    "  \n",
    "- `tremor = {0,1,2,3,4}` \n",
    "The description of the database mentions `tremor` is rated from 0 to 4 according to its severity, but from all the data, the maximum value of `tremor` recorded is 3. \n",
    "\n",
    "Data:\n",
    "- ancillary\n",
    "- clinical : UPDRS evaluation score \n",
    "- demographics : #TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REAL-PD: Create Masks for inactivity removal for all subtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"real\"\n",
    "#data_real_subtype='smartphone_accelerometer'\n",
    "#data_real_subtype = \"smartwatch_accelerometer\"\n",
    "#data_real_subtype='smartwatch_gyroscope'\n",
    "\n",
    "# List of interesting measurement id we want to look at\n",
    "# list_measurement_id = [\n",
    "#     \"5b4c7c81-659d-40ea-a1fd-59622074fd10\",\n",
    "#     \"ee053d95-c155-400d-ae42-fe24834ad4a9\",\n",
    "#     \"ce51ee31-8553-4321-9f83-8cd3dabe2f66\",\n",
    "#     \"e07708ff-7b8d-4070-af70-3aa81423ab5b\",\n",
    "#     #'7d3f4b7a-167f-4a26-9062-94ce9d8794c1',\n",
    "#     \"99af8d14-cd09-4107-9502-355378ba4e08\",\n",
    "#     #'7d5ac31a-cb53-40f7-8188-0b13724ea55c',\n",
    "#     \"9e43840b-dd89-498b-af1a-a62896a4d5d9\",\n",
    "#     \"e391f546-bf8a-46c7-a16c-95bc02f40629\",\n",
    "# ]\n",
    "\n",
    "# list_measurement_id = ['0c310608-1a32-4b09-b164-375d93ddb2aa']\n",
    "\n",
    "# Filter df_train_label according to the measurement_id we are most interested in\n",
    "# df_train_label = interesting_patients(df_train_label=df_train_label, list_measurement_id=list_measurement_id)\n",
    "for data_real_subtype in ['smartphone_accelerometer','smartwatch_accelerometer','smartwatch_gyroscope']:\n",
    "    training_or_ancillary='training_data' #training_data\n",
    "    path_train_data, df_train_label = define_data_type(data_type,\n",
    "                                                       training_or_ancillary,\n",
    "                                                       data_real_subtype)\n",
    "    remove_inactivity_highpass(\n",
    "        df_train_label,\n",
    "        energy_threshold=10,\n",
    "        duration_threshold=3000,\n",
    "        plot_frequency_response=False,\n",
    "        plot_accelerometer_after_removal=False,\n",
    "        mask_path=data_dir+'/real-pd.'+\n",
    "        training_or_ancillary+\n",
    "        '.high_pass_mask/'+data_real_subtype+'/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REAL-PD: Create first derivative for all subtypes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for data_real_subtype in ['smartphone_accelerometer','smartwatch_accelerometer','smartwatch_gyroscope']:\n",
    "    path_train_data, df_train_label = define_data_type(data_type=data_type)\n",
    "    print(len(df_train_label))\n",
    "    for idx in df_train_label.index:\n",
    "        try:\n",
    "#             print('where we get the file : ', path_train_data)\n",
    "            df_train_data = pd.read_csv(path_train_data + df_train_label[\"measurement_id\"][idx] + \".csv\")\n",
    "        except FileNotFoundError:\n",
    "            print('Removing ' + df_train_label[\"measurement_id\"][idx] +\n",
    "                  ' as it doesn\\'t exist for ' +\n",
    "                  data_real_subtype)\n",
    "            df_train_label = df_train_label.drop(idx)\n",
    "        print(len(df_train_label))\n",
    "    do_work = partial(\n",
    "        get_first_derivative, \n",
    "        derivative_path=\"real-pd.training_data.derivative_original_data/\"+data_real_subtype+'/',\n",
    "        padding=True, \n",
    "        mask_path=data_dir+'/real-pd.training_data.high_pass_mask/'+data_real_subtype+'/'\n",
    "    )\n",
    "\n",
    "    num_jobs = 8\n",
    "    with ProcessPoolExecutor(num_jobs) as ex:\n",
    "        results = list(ex.map(do_work, df_train_label['measurement_id']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REAL-PD: Create WAV files for all subtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "where we get the file :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data/smartphone_accelerometer/\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "data_type = \"real\"\n",
    "#'smartphone_accelerometer', 'smartwatch_accelerometer', smartwatch_gyroscope\n",
    "for data_real_subtype in ['smartphone_accelerometer']:\n",
    "    training_or_ancillary='training_data' #training_data\n",
    "    path_train_data, df_train_label = define_data_type(data_type, training_or_ancillary, data_real_subtype)\n",
    "    list_measurement_id = ['00a49337-386c-4de3-a220-4cf3c0d20a7d.wav']\n",
    "\n",
    "    # Filter df_train_label according to the measurement_id we are most interested in\n",
    "    df_train_label = interesting_patients(df_train_label=df_train_label, list_measurement_id=list_measurement_id)\n",
    "    print(len(df_train_label))\n",
    "    print('where we get the file : ', path_train_data)\n",
    "    for idx in df_train_label.index:\n",
    "        try:            \n",
    "            df_train_data = pd.read_csv(path_train_data + df_train_label[\"measurement_id\"][idx] + \".csv\")\n",
    "        except FileNotFoundError:\n",
    "            print('Removing ' + df_train_label[\"measurement_id\"][idx] +\n",
    "                  ' as it doesn\\'t exist for ' +\n",
    "                  data_real_subtype)\n",
    "            df_train_label = df_train_label.drop(idx)\n",
    "        print(len(df_train_label))\n",
    "        \n",
    "    do_work = partial(\n",
    "        write_wav, \n",
    "        wav_path=data_dir+'real-pd.'+training_or_ancillary+'.wav/'+data_real_subtype+'/',\n",
    "        mask_path=data_dir+'/real-pd.'+training_or_ancillary+'.high_pass_mask/'+data_real_subtype+'/'\n",
    "    )\n",
    "\n",
    "    num_jobs = 8\n",
    "    with ProcessPoolExecutor(num_jobs) as ex:\n",
    "        print(type(df_train_label['measurement_id']))\n",
    "        results = list(map(do_work, df_train_label['measurement_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 0c310608-1a32-4b09-b164-375d93ddb2aa as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 40a76c3e-0f0e-48e0-9874-28f89130f463 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 8759a9a9-8aab-4df1-a61e-924bf74d84f2 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 26f04641-84c6-4dbb-b55b-033dc6ffe0a1 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing f2be41e3-cc0e-4f0c-8374-52ba00c8e9ab as it doesn't exist for smartwatch_accelerometer\n",
      "Removing a88408dd-3dfe-4b94-b665-41a2ccf93a85 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 24f13d0e-8d09-4062-988b-0c2af0d74846 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing e2ee04d3-6697-464c-9dbf-dde0fd9be9a0 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 6ffba116-3422-4f5c-9ba5-6fa9499d0988 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 2daabd36-1707-473b-93f1-265ce6b7999e as it doesn't exist for smartwatch_accelerometer\n",
      "Removing e487fc20-66c9-4568-9c61-30f1b331a5c3 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 3051e77d-46ab-44e7-b03d-8b74573c5c97 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 80aba7c8-6994-45df-a022-ef61c735c72f as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 6caa0bd0-7e68-4281-9dd9-1070786ef026 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 763116a6-7386-4dcb-97dc-8cad9a6bb0f2 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing e655453f-9df0-411f-a633-2dd8144d3327 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing f2256f75-ddbd-49a2-a3de-0069905e11f7 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 287ddbef-b7bf-4ff4-b4e9-eaa128bbbe9c as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 8c3d0e87-f2c5-4924-ae88-5af7bcf9ef63 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 0dfdbb5a-218d-4b37-829e-1be00ad88ad4 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 34ea6fef-caad-4a82-9c16-029a93dfbdfc as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 1f1f26c4-e728-415a-b0f2-adad0242e5dc as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 7a5f4c7a-4fdb-4d49-9ed5-fe50ed6e28a6 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing b5e8437a-73ae-4484-b6f4-aaa32c4e539e as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 3ea73ab8-f1d1-4389-bd17-0f9c554f5419 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 972e8656-1917-41e0-85a6-6d791c501460 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing c4a8527f-c62e-4d2d-bdf3-bac924da987e as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 0863cba3-dff4-40b3-a690-bfee1023c137 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing ff1db97b-8505-4c47-9a71-eb7f595519b0 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing bc0a8345-1ef1-4faf-b1b0-41f86d4f1bab as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 59a73268-4e1a-4426-bf73-0fb22f58fa1a as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 15837ec4-8ab6-45c9-92ed-5d54e5bff02c as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 1bce9da5-c3c7-44c0-949c-72312890d260 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 57779b31-49e8-4c91-9e00-693e1140de85 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing a5be6e50-4a46-46e7-9b4e-13eb73531e0f as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 528ddcd4-800d-4b18-a086-74cf26fd9a48 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing a6954a91-338b-4523-9e4a-5e69a8fac206 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 00a49337-386c-4de3-a220-4cf3c0d20a7d as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 0741fc7f-d709-4879-b9b7-1ca56e5a5021 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing f4e4eed4-5470-4058-8737-8f75abe0853c as it doesn't exist for smartwatch_accelerometer\n",
      "Removing f8986b3d-d77c-4c5e-af03-d2adc2c3e022 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 7a5aaf70-b362-4cab-8ad2-10a5edfc4a39 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 658a2398-532b-4d0c-b8fb-67764ec58e9c as it doesn't exist for smartwatch_accelerometer\n",
      "Removing e429fc2d-08db-4b81-b9c5-865f1fe790eb as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 2898aa29-f72e-4742-bbdc-2d6af815f490 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 19e35072-c573-4030-a5b3-4b9ff118f99d as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 0a58d095-9030-4587-b5a8-359ac80d8813 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 27eccfc4-e329-4695-aee8-6d706b247191 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing de24ad87-99a0-44f9-a98f-dbb5d5e2873c as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 38e4c46d-17ac-4445-815f-0489996e80b9 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing e9e16344-54ee-4fbc-8b5f-0e7565c2de36 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing e3eb82b8-1ea1-434a-aca7-645bcc5b5fec as it doesn't exist for smartwatch_accelerometer\n",
      "Removing dfc96cd3-a11c-476a-a2e1-838596d84862 as it doesn't exist for smartwatch_accelerometer\n",
      "Removing c50831c7-fe05-4383-bf2a-defef56440fe as it doesn't exist for smartwatch_accelerometer\n",
      "Removing 73c60aa4-abe1-41b8-9a21-9754b7204c0c as it doesn't exist for smartwatch_accelerometer\n",
      "Removing d9ea3f6f-a8b3-45c9-ad5a-761dd209960f as it doesn't exist for smartwatch_accelerometer\n",
      "535\n",
      "<class 'pandas.core.series.Series'>\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/ee053d95-c155-400d-ae42-fe24834ad4a9.wav\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/ce51ee31-8553-4321-9f83-8cd3dabe2f66.wav\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/39cf118c-e2bb-4e3f-adb1-d200ee20e1fa.wav\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/ecbeea40-8770-455d-90a6-597e7f896e1b.wav\n",
      "measurement_id  ee053d95-c155-400d-ae42-fe24834ad4a9\n",
      "measurement_id  ce51ee31-8553-4321-9f83-8cd3dabe2f66\n",
      "measurement_id  39cf118c-e2bb-4e3f-adb1-d200ee20e1fa\n",
      "measurement_id  ecbeea40-8770-455d-90a6-597e7f896e1b\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/d3c89012-3ab9-4014-b577-61ff05e31968.wav\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/5c42911d-0ebd-47ba-9925-dd5ab1c0ed61.wav\n",
      "measurement_id  5c42911d-0ebd-47ba-9925-dd5ab1c0ed61\n",
      "measurement_id  d3c89012-3ab9-4014-b577-61ff05e31968\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/274f5bc8-2e4f-4d7c-a546-b65b7d6bd01e.wav\n",
      "measurement_id  274f5bc8-2e4f-4d7c-a546-b65b7d6bd01e\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/235472d5-ad2e-4c76-947e-358c9d8c1280.wav\n",
      "measurement_id  235472d5-ad2e-4c76-947e-358c9d8c1280\n",
      "(44081, 5)\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/d3171986-f881-4fa8-9c61-a72a3ad3ae26.wav\n",
      "measurement_id  d3171986-f881-4fa8-9c61-a72a3ad3ae26\n",
      "(45560, 5)\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/96090095-7b0f-423f-ac9a-d69b7d1b638c.wav\n",
      "measurement_id  96090095-7b0f-423f-ac9a-d69b7d1b638c\n",
      "(51541, 5)\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/6d160a14-7cb3-4471-a46e-e86e1c8a45b2.wav\n",
      "measurement_id  6d160a14-7cb3-4471-a46e-e86e1c8a45b2\n",
      "(59898, 5)\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/d1a2f23d-a39c-4a56-bddf-9b5e0f07eb57.wav\n",
      "measurement_id  d1a2f23d-a39c-4a56-bddf-9b5e0f07eb57\n",
      "(119805, 5)\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/c0445431-35c3-402a-9b71-1c413604327b.wav\n",
      "measurement_id  c0445431-35c3-402a-9b71-1c413604327b\n",
      "(114563, 5)\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/e07708ff-7b8d-4070-af70-3aa81423ab5b.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measurement_id  e07708ff-7b8d-4070-af70-3aa81423ab5b\n",
      "(29440, 5)\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/7d3f4b7a-167f-4a26-9062-94ce9d8794c1.wav\n",
      "measurement_id  7d3f4b7a-167f-4a26-9062-94ce9d8794c1\n",
      "(119790, 5)\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/99af8d14-cd09-4107-9502-355378ba4e08.wav\n",
      "measurement_id  99af8d14-cd09-4107-9502-355378ba4e08\n",
      "(119805, 5)\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/87db04a9-db5b-4302-811d-a3191704595c.wav\n",
      "measurement_id  87db04a9-db5b-4302-811d-a3191704595c\n",
      "(119801, 5)\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/7d5ac31a-cb53-40f7-8188-0b13724ea55c.wav\n",
      "measurement_id  7d5ac31a-cb53-40f7-8188-0b13724ea55c\n",
      "(59904, 5)\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/9e43840b-dd89-498b-af1a-a62896a4d5d9.wav\n",
      "measurement_id  9e43840b-dd89-498b-af1a-a62896a4d5d9\n",
      "(38863, 5)\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/6f220a53-1919-4a5e-a322-f8a7586fdb2a.wav\n",
      "measurement_id  6f220a53-1919-4a5e-a322-f8a7586fdb2a\n",
      "(59890, 5)\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/dd8a49e5-01d9-4b66-ac2d-9af92ad24d49.wav\n",
      "measurement_id  dd8a49e5-01d9-4b66-ac2d-9af92ad24d49\n",
      "(56760, 5)\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/3444df85-80c8-49ac-8b6c-e6ded0318c0b.wav\n",
      "measurement_id  3444df85-80c8-49ac-8b6c-e6ded0318c0b\n",
      "(18673, 5)\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/8b217686-2aa9-4bff-a6fd-4911ff4de15b.wav\n",
      "measurement_id  8b217686-2aa9-4bff-a6fd-4911ff4de15b\n",
      "(59903, 5)\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/f80dc0d3-59e2-4d79-a198-8cebda77fa69.wav\n",
      "measurement_id  f80dc0d3-59e2-4d79-a198-8cebda77fa69\n",
      "(119807, 5)\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/a1a4a86a-2ff1-4969-aa1e-476a90531f9a.wav\n",
      "measurement_id  a1a4a86a-2ff1-4969-aa1e-476a90531f9a\n",
      "(10355, 5)\n",
      "write_wav path :  /home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.training_data.wav/smartwatch_accelerometer/21361dd1-479c-4d10-a2d3-37212c5d23f2.wav\n",
      "measurement_id  21361dd1-479c-4d10-a2d3-37212c5d23f2\n",
      "(88435, 5)\n",
      "(48627, 5)\n",
      "(44273, 5)\n",
      "(55306, 5)\n",
      "(32625, 5)\n",
      "(45528, 5)\n",
      "(43307, 5)\n",
      "(44649, 5)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unsupported data type 'object'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/mpgill/anaconda3/lib/python3.7/concurrent/futures/process.py\", line 232, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/home/mpgill/anaconda3/lib/python3.7/concurrent/futures/process.py\", line 191, in _process_chunk\n    return [fn(*args) for args in chunk]\n  File \"/home/mpgill/anaconda3/lib/python3.7/concurrent/futures/process.py\", line 191, in <listcomp>\n    return [fn(*args) for args in chunk]\n  File \"<ipython-input-57-5c6af21e718d>\", line 21, in write_wav\n    measurement_id + '.wav', samplerate, df_train_data.iloc[:,1:2].to_numpy())\n  File \"/home/mpgill/anaconda3/lib/python3.7/site-packages/scipy/io/wavfile.py\", line 336, in write\n    raise ValueError(\"Unsupported data type '%s'\" % data.dtype)\nValueError: Unsupported data type 'object'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-16dbcb50432c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mProcessPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_jobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'measurement_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_work\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'measurement_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/process.py\u001b[0m in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0mcareful\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mto\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0mreferences\u001b[0m \u001b[0mto\u001b[0m \u001b[0myielded\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \"\"\"\n\u001b[0;32m--> 476\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m         \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    584\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported data type 'object'"
     ]
    }
   ],
   "source": [
    "data_type = \"real\"\n",
    "#'smartphone_accelerometer', 'smartwatch_accelerometer', smartwatch_gyroscope\n",
    "data_real_subtype ='smartwatch_accelerometer'\n",
    "\n",
    "training_or_ancillary='training_data' #training_data\n",
    "\n",
    "path_train_data, df_train_label = define_data_type(data_type, training_or_ancillary, data_real_subtype)\n",
    "# list_measurement_id = ['00a49337-386c-4de3-a220-4cf3c0d20a7d.wav']\n",
    "\n",
    "# Filter df_train_label according to the measurement_id we are most interested in\n",
    "# df_train_label = interesting_patients(df_train_label=df_train_label, list_measurement_id=list_measurement_id)\n",
    "# print(len(df_train_label))\n",
    "# print('where we get the file : ', path_train_data)\n",
    "for idx in df_train_label.index:\n",
    "    try:            \n",
    "        df_train_data = pd.read_csv(path_train_data + df_train_label[\"measurement_id\"][idx] + \".csv\")\n",
    "    except FileNotFoundError:\n",
    "        print('Removing ' + df_train_label[\"measurement_id\"][idx] +\n",
    "              ' as it doesn\\'t exist for ' +\n",
    "              data_real_subtype)\n",
    "        df_train_label = df_train_label.drop(idx)\n",
    "print(len(df_train_label))\n",
    "\n",
    "do_work = partial(\n",
    "    write_wav, \n",
    "    wav_path=data_dir+'real-pd.'+training_or_ancillary+'.wav/'+data_real_subtype+'/',\n",
    "    mask_path=data_dir+'/real-pd.'+training_or_ancillary+'.high_pass_mask/'+data_real_subtype+'/'\n",
    ")\n",
    "\n",
    "num_jobs = 8\n",
    "with ProcessPoolExecutor(num_jobs) as ex:\n",
    "    print(type(df_train_label['measurement_id']))\n",
    "    results = list(ex.map(do_work, df_train_label['measurement_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the occurences of each symptoms for each patient\n",
    "df_occurences, df_train_label_subject_id = compute_symptoms_occurences_dataframe(\n",
    "    df_train_label=df_train_label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graphs\n",
    "plot_symptoms_occurences(\n",
    "    df_occurences=df_occurences, df_train_label_subject_id=df_train_label_subject_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame for specified subject_id\n",
    "# display(df_train_label_subject_id.get_group('hbv013'))\n",
    "\n",
    "# print(display(df_train_label_subject_id.get_group('hbv013')['tremor'].value_counts()))\n",
    "\n",
    "# Graphs for a specific subject_id the 3 symptoms\n",
    "# df_train_label_subject_id.get_group('hbv014')['tremor'].value_counts().plot(kind='bar', title='tremor')\n",
    "# df_train_label_subject_id.get_group('hbv014')['dyskinesia'].value_counts().plot(kind='bar', title='dys')\n",
    "# df_train_label_subject_id.get_group('hbv014')['on_off'].value_counts().plot(kind='bar', title='on_off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests & Drafts, back-up space that's not important, just notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20 * 60 / 59848"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the default option to display all row with display(DF)\n",
    "# pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tried to do convolution instead of np.multiply and dot product to get the derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(\n",
    "    np.array(\n",
    "        [\n",
    "            [0.2, 1, 5, 9],\n",
    "            [0.4, 2, 6, 10],\n",
    "            [0.6, 3, 7, 11],\n",
    "            [0.8, 4, 8, 12],\n",
    "            [1, 5, 9, 13],\n",
    "            [1.2, 6, 10, 14],\n",
    "            [1.4, 7, 11, 15],\n",
    "            [1.6, 7, 11, 15],\n",
    "            [1.8, 8, 12, 16],\n",
    "            [2, 9, 13, 16],\n",
    "        ]\n",
    "    ),\n",
    "    columns=[\"Timestamp\", \"X\", \"Y\", \"Z\"],\n",
    ")\n",
    "display(df2)\n",
    "m = np.linspace(-3, 3, num=2 * 3 + 1)\n",
    "display(m)\n",
    "\n",
    "np.convolve(df2.loc[0:6, \"X\"], m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to filter a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of the data to find edge cases\n",
    "\n",
    "\n",
    "# Create variable with TRUE if nationality is USA\n",
    "dys = df_train_label[\"dyskinesia\"] > 1\n",
    "\n",
    "# Create variable with TRUE if age is greater than 50\n",
    "tre = df_train_label[\"on_off\"] > 0\n",
    "\n",
    "# Select all cases where nationality is USA and age is greater than 50\n",
    "df_train_label[dys & tre]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing around with pct_change function to try and remove_silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_pct_change = df_train_data.iloc[:,-3:].pct_change(periods=5)\n",
    "# #df_pct_change = df_train_data['X'].pct_change(periods=5)\n",
    "# df_pct_change.columns = ['X', 'Y','Z']\n",
    "# print('pct_change measurement id : ', df_train_label[\"measurement_id\"][idx])\n",
    "# display(df_pct_change)\n",
    "# df_pct_change = df_pct_change[df_pct_change > 0.01]\n",
    "\n",
    "cars = {\n",
    "    \"Brand\": [\n",
    "        \"Honda Civic\",\n",
    "        \"Toyota Corolla\",\n",
    "        \"Ford Focus\",\n",
    "        \"Audi A4\",\n",
    "        \"Toyota Corolla\",\n",
    "        \"Ford Focus\",\n",
    "        \"Audi A4\",\n",
    "    ],\n",
    "    \"Price\": [1, 1, 2, 3, 4, 5, 6],\n",
    "    \"Price2\": [2, 3, 4, 5, 6, 7, 8],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(cars, columns=[\"Brand\", \"Price\", \"Price2\"])\n",
    "display(df)\n",
    "\n",
    "display(df.iloc[:, -2:])\n",
    "\n",
    "display(df.iloc[:, -2:].pct_change(periods=1))\n",
    "\n",
    "df = df.iloc[:, -2:].pct_change(periods=1, fill_method=\"ffill\")\n",
    "\n",
    "df_pct_change = df[(df[\"Price\"] > 0.25) & (df[\"Price2\"] > 0.3)]\n",
    "print(\"------------\")\n",
    "display(df_pct_change)\n",
    "print(\"------------\")\n",
    "# display(df['Price'].pct_change(periods=1))\n",
    "\n",
    "# display(df['Price','Price2'].pct_change(periods=2))\n",
    "\n",
    "display(df[\"Price\"].pct_change(periods=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = {  #'Brand': ['Honda Civic','Toyota Corolla','Ford Focus','Audi A4','Toyota Corolla','Ford Focus','Audi A4'],\n",
    "    \"Price\": [1, 1, 2, 3, -10, 5, 6],\n",
    "    \"Price2\": [2, 3, 4, 5, 6, 7, 8],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(cars, columns=[\"Brand\", \"Price\", \"Price2\"])\n",
    "display(df)\n",
    "\n",
    "display(df.abs().max())\n",
    "\n",
    "display((df.abs().max() * 5) / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_inactivity_max(df_train_label):\n",
    "    last_filtered_value = pd.Series(np.zeros(3), index=[\"X\", \"Y\", \"Z\"])\n",
    "    filtered_value = pd.Series(np.zeros(3), index=[\"X\", \"Y\", \"Z\"])\n",
    "    display(last_filtered_value)\n",
    "    for idx in df_train_label.index:\n",
    "        df_allo = []\n",
    "        df_train_data = pd.read_csv(\n",
    "            path_train_data + df_train_label[\"measurement_id\"][idx] + \".csv\"\n",
    "        )\n",
    "\n",
    "        # Get the absolute max value for X, Y, Z\n",
    "        max_values = df_train_data.iloc[:, -3:].abs().max()\n",
    "\n",
    "        # Compute what is 5% of that max\n",
    "        thresold_energy = 5\n",
    "        df_treshold = (max_values * thresold_energy) / 100\n",
    "\n",
    "        # display(df_train_data)\n",
    "        # Candidates are the frames where X, Y, Z are below that treshold (5% of the max)\n",
    "        #         df_candidates = df_train_data[(df_train_data.X.abs() <= df_treshold.X) &\n",
    "        #                                      (df_train_data.Y.abs() <= df_treshold.Y) &\n",
    "        #                                      (df_train_data.Z.abs() <= df_treshold.Z)]\n",
    "        # display(df_candidates)\n",
    "        for idx2 in df_train_data.index:\n",
    "            # print('df_train_data[idx2]')\n",
    "            # display(df_train_data.iloc[idx2,-3:])\n",
    "            last_filtered_value = filtered_value\n",
    "            filtered_value = last_filtered_value + 0.004 * (\n",
    "                df_train_data.iloc[idx2, -3:] - last_filtered_value\n",
    "            )\n",
    "            y = pd.DataFrame(columns=[\"Timestamp\"])\n",
    "            y = pd.concat(\n",
    "                [y, pd.DataFrame([df_train_data.iloc[idx2, 0]], columns=[\"Timestamp\"])],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "            #             print('display y :')\n",
    "            #             display(y)\n",
    "            #             print('end display y')\n",
    "\n",
    "            #             print('display filtered value')\n",
    "            #             display(pd.DataFrame(filtered_value).transpose())\n",
    "            #             print('end display filtered value')\n",
    "            df_allo.append(\n",
    "                pd.concat([y, pd.DataFrame(filtered_value).transpose()], axis=1)\n",
    "            )\n",
    "        #             print('display df_allo')\n",
    "        #             display(df_allo)\n",
    "\n",
    "        # FIXME : change the name df_allo\n",
    "        df_allo = pd.DataFrame(df_allo, columns=(\"Timestamp\", \"X\", \"Y\", \"Z\"))\n",
    "\n",
    "        df_allo.plot(x=\"Timestamp\", legend=True, subplots=True, title=\"allo\")\n",
    "        stop()\n",
    "\n",
    "\n",
    "#         v_candidate_x = pd.DataFrame({'Candidate':list(0)})\n",
    "#         v_candidate_x = np.where(df_train_data.X.abs() <= df_treshold.X, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This didn't work because it's using pct_change between X coordinates where coincidences happens\n",
    "\n",
    "\n",
    "def remove_inactivity_pct_change(df_train_label, data_real_subtype=\"\"):\n",
    "    \"\"\"\n",
    "    Save .csv files with silence (inactivity) removed \n",
    "\n",
    "    Path used: \n",
    "    # cis-pd.training_data.no_silence/\n",
    "    # real-pd.training_data.no_silence/smartphone_accelerometer/\n",
    "    # real-pd.training_data.no_silence/smartwatch_accelerometer/\n",
    "    # real-pd.training_data.no_silence/smartwatch_gyroscope/\n",
    "    # data_type = {'cis', 'real'}\n",
    "\n",
    "    Arguments:\n",
    "    df_train_label: Dataframe with training labels\n",
    "\n",
    "    data_real_subtype: Optional. If data_type is real, data_real_subtype needs to be provided\n",
    "        data_real_subtype={smartphone_accelerometer , smartwatch_accelerometer , smartwatch_gyroscope}\n",
    "\n",
    "    Returns: \n",
    "    path_no_inactivity_data: Return the path where the files are saved because it is needed\n",
    "                          if we want to plot the accelerometer, for example\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for idx in df_train_label.index:\n",
    "        df_train_data = pd.read_csv(\n",
    "            path_train_data + df_train_label[\"measurement_id\"][idx] + \".csv\"\n",
    "        )\n",
    "        # print('measurement id : ', df_train_label[\"measurement_id\"][idx])\n",
    "        # display(df_train_data)\n",
    "        cols_to_norm = [\"x\", \"y\", \"z\"] if data_type == \"real\" else [\"X\", \"Y\", \"Z\"]\n",
    "        df_train_data[cols_to_norm] = df_train_data[cols_to_norm].apply(\n",
    "            lambda x: (x - x.min()) / (x.max() - x.min())\n",
    "        )\n",
    "        periods = 300\n",
    "        df_pct_change = df_train_data.iloc[:, -3:].pct_change(periods=periods)\n",
    "\n",
    "        df_pct_change.columns = [\"X\", \"Y\", \"Z\"]\n",
    "        # print('pct_change measurement id : ', df_train_label[\"measurement_id\"][idx])\n",
    "        # display(df_pct_change)\n",
    "\n",
    "        # Apply the treshold to the DataFrame with an AND condition, so all axis must have at least 1% of change\n",
    "        # between the periods\n",
    "        # pd.options.display.max_rows = 1000\n",
    "        print(\"----------before filter--------\")\n",
    "        display(df_pct_change.abs())\n",
    "\n",
    "        print(\"WHAT IS DETECTED AS INACTIVITY\")\n",
    "        display(\n",
    "            df_pct_change[\n",
    "                (df_pct_change.X.abs() < 0.0002)\n",
    "                | (df_pct_change.Y.abs() < 0.0002)\n",
    "                | (df_pct_change.Z.abs() < 0.0002)\n",
    "            ]\n",
    "        )\n",
    "        print(\"END OF WHAT IS DETECTED AS INACTIVITY\")\n",
    "\n",
    "        df_pct_change = df_pct_change[\n",
    "            (df_pct_change.X.abs() >= 0.0002)\n",
    "            & (df_pct_change.Y.abs() >= 0.0002)\n",
    "            & (df_pct_change.Z.abs() >= 0.0002)\n",
    "        ]\n",
    "        print(\"----------after filter--------\")\n",
    "        display(df_pct_change)\n",
    "\n",
    "        filter_df = df_train_data[\n",
    "            df_train_data.index.isin(df_pct_change.index.to_list())\n",
    "        ]\n",
    "\n",
    "        # Counts the number of time where we had to remove inactivity from a dataframe to know how often\n",
    "        # the inactivity zones appear.\n",
    "        print(\"len(filter_df)+periods \", str(len(filter_df) + periods))\n",
    "        print(\"len(df_train_data) \", str(len(df_train_data)))\n",
    "        if len(filter_df) + periods != len(df_train_data):\n",
    "            count = count + 1\n",
    "\n",
    "        # To provide the name of the header for the Dataframe, we get the name of the x axis as it depends\n",
    "        # on the data_type and then we insert it at the first position before the X,Y,Z axis\n",
    "        x_axis_data_type = \"t\" if data_type == \"real\" else \"Timestamp\"\n",
    "        cols_to_norm.insert(0, x_axis_data_type)\n",
    "\n",
    "        # filter_df.plot(x='Timestamp',legend=True, subplots=True,title='allo')\n",
    "\n",
    "        # Save the dataframe in a file with the measurement_id as the name of the file\n",
    "        path_no_inactivity_data = (\n",
    "            data_dir\n",
    "            + data_type\n",
    "            + \"-pd.training_data.no_silence/\"\n",
    "            + data_real_subtype\n",
    "            + \"/\"\n",
    "        )\n",
    "        filter_df.to_csv(\n",
    "            path_no_inactivity_data + df_train_label[\"measurement_id\"][idx] + \".csv\",\n",
    "            index=False,\n",
    "            header=cols_to_norm,\n",
    "        )\n",
    "    print(\n",
    "        \"Inactivity zones were detected \",\n",
    "        str(count),\n",
    "        \" times out of \",\n",
    "        str(len(df_train_label.index)),\n",
    "    )\n",
    "    return path_no_inactivity_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_zeros = pd.DataFrame([False,True,False,False,False,True,False]).astype(int)\n",
    "\n",
    "df_zeros = np.array([0, 0, 0, 1, 0, 1, 1, 1, 1], dtype=bool)\n",
    "\n",
    "display(df_zeros.astype(int))\n",
    "count = 0\n",
    "duration_threshold = 2\n",
    "indices_list = []  # List of tuples\n",
    "howmany = 0\n",
    "for i in range(0, len(df_zeros)):\n",
    "    if df_zeros[i] == 1:\n",
    "        count = count + 1\n",
    "        print(\"1 à lindex\", i)\n",
    "    else:\n",
    "        if count >= duration_threshold:\n",
    "            print(\"threshold atteint start \", start, \" end at \", end)\n",
    "            start = i - count\n",
    "            end = i - 1\n",
    "            indices_list.append((start, end))\n",
    "            howmany = howmany + 1\n",
    "            count = 0\n",
    "        # if it doesn't reach the threshold, we change the 1 for 0 because we don't want to remove those\n",
    "        elif count >= 1:\n",
    "            print(\"Effacer les 1 de \", start, \" a \", end)\n",
    "            df_zeros[i - count : i] = [0] * (end - start)\n",
    "            count = 0\n",
    "\n",
    "display(df_zeros.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
