{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEAT-PD Challenge\n",
    "\n",
    "Challenge website : https://www.synapse.org/#!Synapse:syn20825169/wiki/596118\n",
    "\n",
    "Data information : https://www.synapse.org/#!Synapse:syn20825169/wiki/600405\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas/Doubts [Laureano]\n",
    "\n",
    "VAD like thing to remove unwanted data?\n",
    "modified MFCC?\n",
    "X,Y,Z = relative positions or acceleration?\n",
    "\n",
    "Imp: Predict per person. Maybe UBM like thing and adapt it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Imports for the high pass signal\n",
    "from scipy.signal import butter, freqz, lfilter\n",
    "\n",
    "# KFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Import required modules\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "\n",
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "# FIXME : Move this to data?\n",
    "path_save_accelerometer_plots = (\"/home/sjoshi/codes/python/BeatPD/code/accelerometer_plots/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_data_type(data_type):\n",
    "    \"\"\"\n",
    "    Setup file names\n",
    "    \n",
    "    Keyword arguments:\n",
    "    data_type = {cis , real}\n",
    "\n",
    "    If data_type is real, data_real_subtype will have to be declared as well \n",
    "    data_real_subtype={smartphone_accelerometer, smartwatch_accelerometer, smartwatch_gyroscope}\n",
    "    \"\"\"\n",
    "    if data_type == \"cis\":\n",
    "        path_train_labels = (\n",
    "            data_dir\n",
    "            + data_type\n",
    "            + \"-pd.data_labels/\"\n",
    "            + data_type.upper()\n",
    "            + \"-PD_Training_Data_IDs_Labels.csv\"\n",
    "        )\n",
    "        path_train_data = data_dir + data_type + \"-pd.training_data/\"\n",
    "\n",
    "    if data_type == \"real\":\n",
    "        path_train_labels = (\n",
    "            data_dir\n",
    "            + data_type\n",
    "            + \"-pd.data_labels/\"\n",
    "            + data_type.upper()\n",
    "            + \"-PD_Training_Data_IDs_Labels.csv\"\n",
    "        )\n",
    "        path_train_data = (\n",
    "            data_dir + data_type + \"-pd.training_data/\" + data_real_subtype + \"/\"\n",
    "        )\n",
    "\n",
    "    # Display labels\n",
    "    df_train_label = pd.read_csv(path_train_labels)\n",
    "    return path_train_data, df_train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_title(idx, df_train_label):\n",
    "    \"\"\"\n",
    "    Create a title that identifies the plotted graph with the measurement_id, \n",
    "    subject_id, on_off label, dyskinesia and tremor labels.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    - idx: \n",
    "    - df_train_label: DataFrame containing the following columns \n",
    "            [measurement_id, subject_id, on_off, tremor, dyskenisia]\n",
    "    \n",
    "    Returns: A string concatenating all the values mentioned \n",
    "    \"\"\"\n",
    "    # Following val_* variables are only used to format a cute title for the charts\n",
    "    val_subject_id = df_train_label.loc[[idx]][\"subject_id\"].values[0]\n",
    "    val_on_off = df_train_label.loc[[idx]][\"on_off\"].values[0]\n",
    "    val_dyskinesia = df_train_label.loc[[idx]][\"dyskinesia\"].values[0]\n",
    "    val_tremor = df_train_label.loc[[idx]][\"tremor\"].values[0]\n",
    "    return \"{0} = on_off: {1}, dyskinesia: {2}, tremor: {3}\".format(\n",
    "        val_subject_id, val_on_off, val_dyskinesia, val_tremor\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accelerometer(data_type, path_accelerometer_plots, path_inactivity=None):\n",
    "    \"\"\"\n",
    "    Plots the accelerometer data. There will be 3 subplots for each axis (X, Y, Z)\n",
    "    \n",
    "    Keyword arguments: \n",
    "    - data_type={cis , real} : It depends on which database is used \n",
    "    - path_accelerometer_plots: Path where the accelerometer plots are going to be saved \n",
    "    - path_inactivity: Path where the dataframe with inactivity  removed are \n",
    "    \"\"\"\n",
    "    # Iterating through all the indexes contained in df_train_label\n",
    "    for idx in df_train_label.index:\n",
    "        if path_inactivity is None:\n",
    "            df_train_data = pd.read_csv(\n",
    "                path_train_data + df_train_label[\"measurement_id\"][idx] + \".csv\"\n",
    "            )\n",
    "        else:\n",
    "            df_train_data = pd.read_csv(\n",
    "                path_inactivity + df_train_label[\"measurement_id\"][idx] + \".csv\"\n",
    "            )\n",
    "\n",
    "        # FIXME: BUG ?  why the following goes to 1000xxx sometimes? It should be max 59xxx\n",
    "        print(\"measurement_id : \", df_train_label[\"measurement_id\"][idx])\n",
    "        # Following val_* variables are only used to format a cute title for the charts\n",
    "        great_title = get_plot_title(idx, df_train_label)\n",
    "\n",
    "        # The time doesn't have the same name depending on the data_type\n",
    "        x_axis_data_type = \"t\" if data_type == \"real\" else \"Timestamp\"\n",
    "\n",
    "        # Normalize the data\n",
    "        cols_to_norm = [\"x\", \"y\", \"z\"] if data_type == \"real\" else [\"X\", \"Y\", \"Z\"]\n",
    "        df_train_data[cols_to_norm] = df_train_data[cols_to_norm].apply(\n",
    "            lambda x: (x - x.min()) / (x.max() - x.min())\n",
    "        )\n",
    "\n",
    "        df_train_data.plot(\n",
    "            x=x_axis_data_type, legend=True, subplots=True, title=great_title\n",
    "        )\n",
    "\n",
    "        # Save plotted graph with the measurement_id as name of the file\n",
    "        plt.savefig(\n",
    "            path_accelerometer_plots + df_train_label[\"measurement_id\"][idx] + \".png\"\n",
    "        )\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_missing_values(df_train_label):\n",
    "    \"\"\"\n",
    "    Filling NaN values with -1. \n",
    "    \n",
    "    Keyword arguments:\n",
    "    - df_train_label: DataFrame containing the following columns \n",
    "            [measurement_id, subject_id, on_off, tremor, dyskenisia]\n",
    "    \"\"\"\n",
    "    # Replace NaN values with -1.0 because otherwise plotting triggers an error\n",
    "    df_train_label = df_train_label.fillna(value=-1.0)\n",
    "    return df_train_label\n",
    "\n",
    "\n",
    "def compute_symptoms_occurences_dataframe(df_train_label):\n",
    "    \"\"\"\n",
    "    Computes how many times the symptoms are occuring for a single subject_id \n",
    "    \n",
    "    Keyword arguments:\n",
    "    - df_train_label: DataFrame containing the following columns \n",
    "            [measurement_id, subject_id, on_off, tremor, dyskenisia]\n",
    "    \"\"\"\n",
    "    df_train_label = prepro_missing_values(df_train_label=df_train_label)\n",
    "\n",
    "    # Group data by subject_id\n",
    "    df_train_label_subject_id = df_train_label.groupby(\"subject_id\")\n",
    "\n",
    "    df_occurences = []\n",
    "    symptoms = [\"on_off\", \"dyskinesia\", \"tremor\"]\n",
    "\n",
    "    for key, value in df_train_label_subject_id:\n",
    "        for symptom in symptoms:\n",
    "            # Pour un patient, prendre les 3 dernieres colonnes, et pour 1 symptome, calculer le nb d'occurences\n",
    "            counter = (\n",
    "                df_train_label_subject_id.get_group(key)\n",
    "                .iloc[:, -3:][symptom]\n",
    "                .value_counts()\n",
    "            )\n",
    "\n",
    "            for symptom_value, symptom_occurence in counter.items():\n",
    "                df_occurences.append(\n",
    "                    (\n",
    "                        {\n",
    "                            \"subject_id\": key,\n",
    "                            \"symptom\": symptom,\n",
    "                            \"symptom_value\": symptom_value,\n",
    "                            \"occurence\": symptom_occurence,\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    df_occurences = pd.DataFrame(\n",
    "        df_occurences, columns=(\"subject_id\", \"symptom\", \"symptom_value\", \"occurence\")\n",
    "    )\n",
    "\n",
    "    return df_occurences, df_train_label_subject_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_symptoms_occurences(df_occurences, df_train_label_subject_id):\n",
    "    \"\"\"\n",
    "    This function plots the occurences of symptoms according to subject_id \n",
    "\n",
    "    Keyword Arguments: \n",
    "    - df_occurences: contains the df with occurences computed in compute_symptoms_occurences_dataframe\n",
    "    - df_train_label_subject_id: contains df_train_label grouped by subject_id \n",
    "    \"\"\"\n",
    "    # There will be one graph plotted for each patient, for each of the 3 symptoms\n",
    "    nb_subjects_id = (\n",
    "        df_occurences.subject_id.nunique()\n",
    "    )  # nb of unique patients in the label file\n",
    "    print(\"Nb subject_id : \", nb_subjects_id)\n",
    "    height = 30 if nb_subjects_id > 10 else 10\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=nb_subjects_id, ncols=3, figsize=(10, height), sharey=True\n",
    "    )  # 3 cols for the 3 symptoms\n",
    "\n",
    "    # Quick fix to plot the graphs at the right place. Starts at -1 because in the first for loop\n",
    "    # it is incremented\n",
    "    patient = -1\n",
    "\n",
    "    # Plot for all subject_id 3 bar plots for all the symptoms and their occurences\n",
    "    # Reminder that NaN values (missing values) were replaced with -1 and are shown as such in the plots\n",
    "    symptoms = [\"on_off\", \"dyskinesia\", \"tremor\"]\n",
    "    for key, value in df_train_label_subject_id:\n",
    "        patient = patient + 1  # value used to position the plots (row)\n",
    "        symptom_no = 0  # value only used to position the plots (col)\n",
    "        for symptom in symptoms:\n",
    "            subject_symptom = \" \".join(\n",
    "                [str(key), symptom]\n",
    "            )  # variable used to create a title for each plot\n",
    "            df_train_label_subject_id.get_group(key)[symptom].value_counts().plot(\n",
    "                kind=\"bar\",\n",
    "                x=symptom,\n",
    "                title=subject_symptom,\n",
    "                ax=axes[patient, symptom_no],\n",
    "                sharey=True,\n",
    "            )\n",
    "\n",
    "            fig.tight_layout()\n",
    "            plt.tight_layout()\n",
    "            symptom_no = symptom_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interesting_patients(df_train_label, list_measurement_id):\n",
    "    \"\"\"\n",
    "    Filters df_train_label according to a list of measurement_id we are interested in analyzing\n",
    "\n",
    "    Keyword Arguments:\n",
    "    - df_train_label: Labels DataFrame containing the following columns \n",
    "            [measurement_id, subject_id, on_off, tremor, dyskenisia]\n",
    "    - list_measurement_id: list of measurement_id \n",
    "\n",
    "    Returns:\n",
    "    - df_train_label: filtered df_train_label containing only the measurements_id we are interested in \n",
    "    \"\"\"\n",
    "    filter_measurement_id = df_train_label.measurement_id.isin(list_measurement_id)\n",
    "\n",
    "    df_train_label = df_train_label[filter_measurement_id]\n",
    "    # display(df_train_label)\n",
    "    return df_train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "possible to have participant characteristics from additional db data? ex https://ieeexplore.ieee.org/abstract/document/7911257"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIP : Function to remove silence (inactivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_threshold(threshold_energy, max_values):\n",
    "    \"\"\"\n",
    "    This function returns a dataframe of shape (3,1) containing what is the treshold for each X,Y,Z axis \n",
    "    depending on threshold_energy. \n",
    "\n",
    "    For example if threshold_energy=10, the function is going to return what is 10% of the max_values\n",
    "\n",
    "    Arguments:\n",
    "    - threshold_energy: Percentage of the max values we want to use as treshold \n",
    "    - max_values: Dataframe of the max values \n",
    "    \"\"\"\n",
    "    return (max_values * threshold_energy) / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean offset removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_inactivity_mean_offset(df_train_label):\n",
    "    \"\"\"\n",
    "    Removal of inactivity segments detected with a threshold after the energy is centered over the mean\n",
    "    This function is expected to be less efficient than removal of inactivity with the highpass threshold \n",
    "    \n",
    "    Keyword arguments: \n",
    "    - df_train_label: DataFrame containing the following columns \n",
    "            [measurement_id, subject_id, on_off, tremor, dyskenisia]\n",
    "    \"\"\"\n",
    "    for idx in df_train_label.index:\n",
    "        df_train_data = pd.read_csv(\n",
    "            path_train_data + df_train_label[\"measurement_id\"][idx] + \".csv\"\n",
    "        )\n",
    "\n",
    "        ### Following section works on removing offset with mean\n",
    "        inputData_DCRemoved = df_train_data.iloc[:, -3:] - df_train_data.iloc[\n",
    "            :, -3:\n",
    "        ].mean(axis=0)\n",
    "\n",
    "        # inputData with DC Removed only has NaN for the Timestamp values, so we are adding them\n",
    "        inputData_DCRemoved.insert(0, \"Timestamp\", df_train_data[\"Timestamp\"])\n",
    "\n",
    "        # Plot the graph\n",
    "        great_title = get_plot_title(idx, df_train_label)\n",
    "        print(df_train_label[\"measurement_id\"][idx])\n",
    "        inputData_DCRemoved.plot(\n",
    "            x=\"Timestamp\", legend=True, subplots=True, title=great_title\n",
    "        )\n",
    "\n",
    "        ### Following section works on removing inactivity following a treshold\n",
    "        # Get the absolute max values for X, Y, Z\n",
    "        max_values = inputData_DCRemoved.iloc[:, -3:].abs().max()\n",
    "\n",
    "        # Compute what is X% of that max\n",
    "        df_treshold = get_df_threshold(10, max_values)\n",
    "\n",
    "        df_candidates = inputData_DCRemoved[\n",
    "            (inputData_DCRemoved.X.abs() <= df_treshold[\"X\"])\n",
    "            & (inputData_DCRemoved.Y.abs() <= df_treshold[\"Y\"])\n",
    "            & (inputData_DCRemoved.Z.abs() <= df_treshold[\"Z\"])\n",
    "        ]\n",
    "\n",
    "        print(\"Candidates to be removed:\")\n",
    "        display(df_candidates)\n",
    "\n",
    "        filter_df = inputData_DCRemoved[\n",
    "            ~inputData_DCRemoved.isin(df_candidates)\n",
    "        ].dropna(how=\"all\")\n",
    "        great_title = \"filter_df for : \" + great_title\n",
    "        filter_df.plot(x=\"Timestamp\", legend=True, subplots=True, title=great_title)\n",
    "\n",
    "        # FIXME: This function is not done. As it's not the priority, i'm switching to work on highpass filter\n",
    "        # It doesn't remove the identified candidates or save the 0/1 vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High pass filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source of the code about the highpass: https://gist.github.com/junzis/e06eca03747fc194e322\n",
    "# A bit more explanation here: https://medium.com/analytics-vidhya/how-to-filter-noise-with-a-low-pass-filter-python-885223e5e9b7\n",
    "\n",
    "\n",
    "def butter_highpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype=\"high\", analog=False)\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def butter_highpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_highpass(cutoff, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "def remove_inactivity_highpass(\n",
    "    df_train_label,\n",
    "    energy_threshold,\n",
    "    duration_threshold,\n",
    "    mask_path,\n",
    "    plot_frequency_response=False,\n",
    "    plot_accelerometer_after_removal=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Removes inactivity according to a high pass filter. It will only be applied to the measurement_id provided\n",
    "    in the df_train_label variable. A first condition for a measurement to be removed at a certain timestamp\n",
    "    is that first, the energy is less than the energy_treshold. After that, we identify candidates with a vector\n",
    "    where 0 represents a timestamp we want to keep, and 1 represents timestamps we detected as below the minimum \n",
    "    energy threshold. \n",
    "    \n",
    "    The second threshold, called duration_threshold, represents the condition that there must be a minimum \n",
    "    number of consecutives candidates to be removed befoer the candidates will be indeed removed and confirmed\n",
    "    as inactivity. For example, we could decide to only remove sections that are at least 1 minute long of \n",
    "    inactivity detected.\n",
    "    \n",
    "    Keyword Arguments:\n",
    "    - df_train_label: DataFrame containing the following columns \n",
    "            [measurement_id, subject_id, on_off, tremor, dyskenisia]\n",
    "    - energy_threshold: what percentage of the max energy do we consider as inactivity?\n",
    "        For example, 1 of the max is considered as inactivity\n",
    "    - duration_threshold: how long do we want to have inactivity before we remove it? \n",
    "        For example 3000x0.02ms=1min of inactivity minimum before those candidates are removed\n",
    "    - mask_path: Path where to save the mask \n",
    "    - plot_frequency_response: Optional. {True, False}. \n",
    "                               Flag to determine if we want to plot the frequency response or not\n",
    "    -plot_accelerometer_after_removal: Optinal. {True, False}.\n",
    "                                Flag to determine if we want to plot the accelerometer after the inactivity\n",
    "                                is removed\n",
    "    \"\"\"\n",
    "    # Filter requirements.\n",
    "    order = 10\n",
    "    fs = 50.0  # sample rate, Hz\n",
    "    cutoff = 0.5  # 3.667  # desired cutoff frequency of the filter, Hz\n",
    "\n",
    "    # Get the filter coefficients so we can check its frequency response.\n",
    "    b, a = butter_highpass(cutoff, fs, order)\n",
    "\n",
    "    # Load every training file for each \"row of labels\" we have loaded in df_train_label\n",
    "    for idx in df_train_label.index:\n",
    "        print('Working on ', df_train_label[\"measurement_id\"][idx])\n",
    "        # Load the training data\n",
    "        df_train_data = pd.read_csv(path_train_data + df_train_label[\"measurement_id\"][idx] + \".csv\")\n",
    "\n",
    "        # Set the time axis. It's not the same name for the two databases\n",
    "        x_axis_data_type = \"t\" if data_type == \"real\" else \"Timestamp\"\n",
    "        t = df_train_data[x_axis_data_type]\n",
    "\n",
    "        # Filter the data\n",
    "        X_filtered_data = butter_highpass_filter(df_train_data[\"X\"], cutoff, fs, order)\n",
    "        Y_filtered_data = butter_highpass_filter(df_train_data[\"Y\"], cutoff, fs, order)\n",
    "        Z_filtered_data = butter_highpass_filter(df_train_data[\"Z\"], cutoff, fs, order)\n",
    "\n",
    "        ### Following section works on removing inactivity following a treshold\n",
    "        # Get the absolute max values for X, Y, Z\n",
    "        # FIXME: This could be made better but I won't lose time on this now\n",
    "        # Get in a Series format because that's what the threshold function is expecting\n",
    "        max_values = pd.Series(np.array([\n",
    "                    np.abs(X_filtered_data).max(),\n",
    "                    np.abs(Y_filtered_data).max(),\n",
    "                    np.abs(Z_filtered_data).max(),\n",
    "                ]))\n",
    "\n",
    "        # Get the threshold of the filtered data\n",
    "        df_treshold = get_df_threshold(energy_threshold, max_values)\n",
    "\n",
    "        # Get 0/1 candidates\n",
    "        X_zeros = np.abs(X_filtered_data) <= df_treshold[0]\n",
    "        Y_zeros = np.abs(Y_filtered_data) <= df_treshold[1]\n",
    "        Z_zeros = np.abs(Z_filtered_data) <= df_treshold[2]\n",
    "\n",
    "        # Change data from boolean to 0 and 1s (int)\n",
    "        X_zeros = X_zeros.astype(int)\n",
    "        Y_zeros = Y_zeros.astype(int)\n",
    "        Z_zeros = Z_zeros.astype(int)\n",
    "        # AND operand to identify candidates across all axis\n",
    "        # FIXME: change name of variable bc it's not a df\n",
    "        df_zeros = X_zeros & Y_zeros & Z_zeros\n",
    "\n",
    "\n",
    "        # Check if it reaches the time treshold (like minimum 1 minute long to be removed)\n",
    "        start = 0  # Start and end of the series of 1\n",
    "        end = 0\n",
    "        indices_list = []  # List of tuples\n",
    "        howmany = 0  # How many groups we identified (not required, just nice metric)\n",
    "        count = 0  # How many 1 in a row we found\n",
    "\n",
    "        # Counts the number of 0s and 1s in the original data before we apply the threshold\n",
    "#         unique, counts = np.unique(df_zeros, return_counts=True)\n",
    "#         print(dict(zip(unique, counts)))\n",
    "\n",
    "        # Change the candidates for removal (1) to 0 if there are not enough 1s in a row to reach the\n",
    "        # threshold. For example there needs to be 3000 times 1s in a row for 1 minute of \"inactivity\"\n",
    "        # to be removed\n",
    "        for i in range(0, len(df_zeros)):\n",
    "            if df_zeros[i] == 1:\n",
    "                count = count + 1\n",
    "            else:\n",
    "                if count >= duration_threshold:\n",
    "                    start = i - count\n",
    "                    end = i - 1\n",
    "                    indices_list.append((start, end))\n",
    "                    howmany = howmany + 1\n",
    "                    count = 0\n",
    "                elif (\n",
    "                    count >= 1\n",
    "                ):  # if it doesn't reach the threshold, we change the 1 for 0 because we don't want to remove those\n",
    "                    start = i - count\n",
    "                    end = i\n",
    "                    df_zeros[start:end] = [0] * (end - start)\n",
    "                    count = 0\n",
    "\n",
    "        #print(\"There are \"+ str(howmany) + \" groups identified as candidates to be removed\")\n",
    "\n",
    "        # Counts the number of 0s and 1s in the data after we applied the threshold\n",
    "#         unique, counts = np.unique(df_zeros, return_counts=True)\n",
    "#         print(dict(zip(unique, counts)))\n",
    "\n",
    "        # Save 0/1 candidates to csv\n",
    "        # I use 1-df_zeros to swap the 0s and 1s.\n",
    "        # 1: we want to keep this measure\n",
    "        # 0: detected as inactivity so we want to remove it\n",
    "        # Previously, it was the opposite, as 1s were considered inactivity\n",
    "        df_mask_highpass = pd.DataFrame(1 - df_zeros)\n",
    "        df_mask_highpass.to_csv(\n",
    "            mask_path + df_train_label[\"measurement_id\"][idx] + \".csv\",\n",
    "            index=False,\n",
    "            header=False,\n",
    "        )\n",
    "\n",
    "        # Plot the accelerometer with the removed sections\n",
    "        # The [0] is used to get a pandas.Series instead of a DataFrame\n",
    "        # We insert Timestamp again as it was removed for the filtering\n",
    "        # BUG : Should I use df_train_data or the filtered high pass dataframe?\n",
    "        if plot_accelerometer_after_removal:\n",
    "            filtered_df = df_train_data.iloc[:, -3:].multiply(df_mask_highpass[0], axis=0)\n",
    "            filtered_df.insert(0, x_axis_data_type, df_train_data[x_axis_data_type])\n",
    "            great_title = get_plot_title(idx, df_train_label)\n",
    "            filtered_df.plot(\n",
    "                x=x_axis_data_type, legend=True, subplots=True, title=great_title\n",
    "            )\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "            plt.cla()\n",
    "            plt.close()\n",
    "\n",
    "        # Plot the frequency response, and plot both the original and filtered signals for X, Y and Z.\n",
    "        if plot_frequency_response:\n",
    "            # TODO: Make the graphs bigger\n",
    "            w, h = freqz(b, a, worN=8000)\n",
    "            plt.subplot(4, 1, 1)\n",
    "            plt.plot(0.5 * fs * w / np.pi, np.abs(h), \"b\")\n",
    "            plt.plot(cutoff, 0.5 * np.sqrt(2), \"ko\")\n",
    "            plt.axvline(cutoff, color=\"k\")\n",
    "            plt.xlim(0, 0.5 * fs)\n",
    "            plt.title(\"Highpass Filter Frequency Response\")\n",
    "            plt.xlabel(\"Frequency [Hz]\")\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(4, 1, 2)\n",
    "            plt.plot(t, df_train_data[\"X\"], \"b-\", label=\"X\")\n",
    "            plt.plot(t, X_filtered_data, \"g-\", linewidth=2, label=\"filtered X\")\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(4, 1, 3)\n",
    "            plt.plot(t, df_train_data[\"Y\"], \"b-\", label=\"Y\")\n",
    "            plt.plot(t, Y_filtered_data, \"g-\", linewidth=2, label=\"filtered Y\")\n",
    "            plt.legend()\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(4, 1, 4)\n",
    "            plt.plot(t, df_train_data[\"Z\"], \"b-\", label=\"Z\")\n",
    "            plt.plot(t, Z_filtered_data, \"g-\", linewidth=2, label=\"filtered Z\")\n",
    "            plt.legend()\n",
    "\n",
    "            plt.xlabel(\"Time [sec]\")\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplots_adjust(hspace=0.7)\n",
    "            plt.show()\n",
    "\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "            plt.cla()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOT DONE ?...\n",
    "# def apply_mask(df_train_label, mask_path):\n",
    "#     \"\"\"\n",
    "#     Apply a mask on the list of measurement_ids provided through df_train_label \n",
    "    \n",
    "#     Keyword arguments:\n",
    "#     - df_train_label: DataFrame containing the following columns \n",
    "#             [measurement_id, subject_id, on_off, tremor, dyskenisia]\n",
    "#     \"\"\"\n",
    "#     # Load every training file for each \"row of labels\" we have loaded in df_train_label\n",
    "#     for idx in df_train_label.index:\n",
    "#         # Load the training data\n",
    "#         df_train_data = pd.read_csv(path_train_data + df_train_label[\"measurement_id\"][idx] + \".csv\")\n",
    "#         df_mask = pd.read_csv(mask_path + df_train_label[\"measurement_id\"][idx] + \".csv\")\n",
    "\n",
    "#         # multiply df_train_data by mask so the values to be removed are at 0\n",
    "#         #         np.multiply(df2.iloc[:,-3:],df_mask.iloc[:,-1:])\n",
    "#         df_train_data.iloc[:, -3:] = np.multiply(df_train_data.iloc[:, -3:], df_mask[:, -1:])\n",
    "        \n",
    "#         #         display(df_train_data)\n",
    "#         # Check if that makes me keep the timestamp column\n",
    "\n",
    "#         # Drop the 0 values from the training DataFrame\n",
    "#         df_train_data = df[(df.T != 0).any()]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(measurement_id, mask_path):\n",
    "    \"\"\"\n",
    "    Apply a mask on the list of measurement_ids provided through df_train_label \n",
    "    \n",
    "    Keyword arguments:\n",
    "    - df_train_label: DataFrame containing the following columns \n",
    "            [measurement_id, subject_id, on_off, tremor, dyskenisia]\n",
    "    \"\"\"\n",
    "    # Load the training data\n",
    "    print('measurement_id ', measurement_id)\n",
    "    df_train_data = pd.read_csv(path_train_data + measurement_id + \".csv\")\n",
    "    df_mask = pd.read_csv(mask_path + measurement_id + \".csv\", header=None)\n",
    "\n",
    "    # multiply df_train_data by mask so the values to be removed are at 0\n",
    "    #         np.multiply(df2.iloc[:,-3:],df_mask.iloc[:,-1:])\n",
    "    df_train_data.iloc[:, -3:] = np.multiply(df_train_data.iloc[:, -3:], df_mask)#[:, -1:])\n",
    "\n",
    "    #         display(df_train_data)\n",
    "\n",
    "    # Drop the 0 values from the training DataFrame\n",
    "    df_train_data = df_train_data[(df_train_data.iloc[:, -3:].T != 0).any()]\n",
    "    df_train_data.reset_index(drop=True, inplace=True)\n",
    "    return df_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get velocity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_derivative_value(df_train_data_context, m):\n",
    "    \"\"\"\n",
    "    TODO \n",
    "    \n",
    "    Keyword arguments:\n",
    "    - df_train_data_context: TODO\n",
    "    - m: TODO \n",
    "    \"\"\"\n",
    "    # Dot Product is the sum of the point wise multiplications between a and m\n",
    "    cij = np.dot(df_train_data_context, m)\n",
    "    denum = np.dot(m, m)\n",
    "    return cij / denum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_derivative(df_train_label, derivative_path, n_zero=3, padding=False, mask_path=None):\n",
    "    \"\"\"\n",
    "    TODO \n",
    "    \n",
    "    cis-pd.training_data.velocity_original_data: Original data, which means the inactivity is untouched \n",
    "    \n",
    "    Keyword arguments:\n",
    "    - df_train_label: TODO\n",
    "    - derivative_path: TODO\n",
    "    - n_zero: TODO\n",
    "    - Padding: [False, True] \n",
    "      - If True, it will add a padding of [0,0,0] at the beginning and at the end of the training data\n",
    "      - If False, it will just use the existing values of the training data to have a (7,) vector from the\n",
    "        training data\n",
    "    - mask_path: Optinal. If provided, it will apply the high pass mask on the training data \n",
    "    \"\"\"\n",
    "    # m is a vector. For n_zero, it will be [-3, -2, -1, 0, 1, 2, 3]\n",
    "    m = np.linspace(-n_zero, n_zero, num=2 * n_zero + 1)\n",
    "\n",
    "    for idx in df_train_label.index:\n",
    "        if os.path.isfile(data_dir+derivative_path+ df_train_label[\"measurement_id\"][idx]+ '.csv'):\n",
    "            print (\"File exist : \", idx)\n",
    "            continue\n",
    "        \n",
    "        # Load the training data\n",
    "        if mask_path is not None:\n",
    "            df_train_data = apply_mask(df_train_label[\"measurement_id\"][idx], mask_path)\n",
    "        else: # Going to get the first derivative from the original data \n",
    "            df_train_data = pd.read_csv(path_train_data + df_train_label[\"measurement_id\"][idx] + \".csv\")\n",
    "        \n",
    "        df_velocity= []\n",
    "#         df_velocity_X = []\n",
    "#         df_velocity_Y = []\n",
    "#         df_velocity_Z = []\n",
    "\n",
    "        if padding:\n",
    "            # Padding DataFrame to add 3 empty rows at the beginning and at the end of the training data\n",
    "            df_padding = []\n",
    "\n",
    "            # FIXME: This could probably be made faster but I won't lose time on this\n",
    "            df_padding.insert(0, {\"Timestamp\": -1, \"X\": 0, \"Y\": 0, \"Z\": 0})\n",
    "            df_padding.insert(0, {\"Timestamp\": -1, \"X\": 0, \"Y\": 0, \"Z\": 0})\n",
    "            df_padding.insert(0, {\"Timestamp\": -1, \"X\": 0, \"Y\": 0, \"Z\": 0})\n",
    "\n",
    "            df_train_data = pd.concat(\n",
    "                [pd.DataFrame(df_padding), df_train_data], ignore_index=True\n",
    "            )\n",
    "            df_padding = pd.DataFrame({\"Timestamp\": [-1, -1, -1],\n",
    "                                        \"X\": [0, 0, 0],\n",
    "                                        \"Y\": [0, 0, 0],\n",
    "                                        \"Z\": [0, 0, 0]})\n",
    "\n",
    "            df_train_data_padding = df_train_data.append(df_padding, ignore_index=True)\n",
    "        else:  # FIXME remove this it's just a quickfix because there is the option with or without padding\n",
    "            # and the next loop has to be on the original dataframe withtout padding\n",
    "            df_train_data_padding = df_train_data\n",
    "\n",
    "        # BUG : This df_velocity contains 3 extra rows. I'm not sure where they come from \n",
    "        for row in df_train_data[[\"X\", \"Y\", \"Z\"]].itertuples():\n",
    "            end = row.Index + n_zero\n",
    "\n",
    "            start = row.Index - n_zero\n",
    "\n",
    "            # QUICKFIX to the padding and pointers issue \n",
    "            if start == -3:\n",
    "                start = 0\n",
    "                end = 6\n",
    "            elif start == -2:\n",
    "                start = 1\n",
    "                end = 7\n",
    "            elif start == -1:\n",
    "                start = 2\n",
    "                end = 8\n",
    "            elif end > len(df_train_data) - 1 and not padding:\n",
    "                end = len(df_train_data)\n",
    "                start = len(df_train_data) - (2 * n_zero)\n",
    "                      \n",
    "            df_velocity.append([get_derivative_value(df_train_data_padding.loc[start:end, \"X\"], m),\n",
    "                                 get_derivative_value(df_train_data_padding.loc[start:end, \"Y\"], m),\n",
    "                                 get_derivative_value(df_train_data_padding.loc[start:end, \"Z\"], m)])\n",
    "            \n",
    "        # Build the DataFrame with all the columns together so we can save it to CSV\n",
    "        df_velocity = pd.DataFrame(df_velocity, columns=['X','Y','Z'])\n",
    "\n",
    "        df_velocity.to_csv(\n",
    "            data_dir + derivative_path + df_train_label[\"measurement_id\"][idx] + \".csv\",\n",
    "            index=False,\n",
    "            header=[\"X_velocity\", \"Y_velocity\", \"Z_velocity\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_first_derivative(df_train_label, derivative_path, n_zero=3, padding=False, mask_path=None):\n",
    "#     \"\"\"\n",
    "#     TODO \n",
    "    \n",
    "#     cis-pd.training_data.velocity_original_data: Original data, which means the inactivity is untouched \n",
    "    \n",
    "#     Keyword arguments:\n",
    "#     - df_train_label: TODO\n",
    "#     - derivative_path: TODO\n",
    "#     - n_zero: TODO\n",
    "#     - Padding: [False, True] \n",
    "#       - If True, it will add a padding of [0,0,0] at the beginning and at the end of the training data\n",
    "#       - If False, it will just use the existing values of the training data to have a (7,) vector from the\n",
    "#         training data\n",
    "#     - mask_path: Optinal. If provided, it will apply the high pass mask on the training data \n",
    "#     \"\"\"\n",
    "#     # m is a vector. For n_zero, it will be [-3, -2, -1, 0, 1, 2, 3]\n",
    "#     m = np.linspace(-n_zero, n_zero, num=2 * n_zero + 1)\n",
    "\n",
    "#     for idx in df_train_label.index:\n",
    "#         # Load the training data\n",
    "# #         FIXME : We want the data after it was filtered with the high pass\n",
    "        \n",
    "#         if mask_path is not None:\n",
    "#             df_train_data = apply_mask(df_train_label[\"measurement_id\"][idx], mask_path)\n",
    "#         else: # Going to get the first derivative from the original data \n",
    "#             df_train_data = pd.read_csv(path_train_data + df_train_label[\"measurement_id\"][idx] + \".csv\")\n",
    "        \n",
    "#         df_velocity_X = []\n",
    "#         df_velocity_Y = []\n",
    "#         df_velocity_Z = []\n",
    "\n",
    "#         if padding:\n",
    "#             # Padding DataFrame to add 3 empty rows at the beginning and at the end of the training data\n",
    "#             df_padding = []\n",
    "\n",
    "#             # FIXME: This could probably be made faster but I won't lose time on this\n",
    "#             df_padding.insert(0, {\"Timestamp\": -1, \"X\": 0, \"Y\": 0, \"Z\": 0})\n",
    "#             df_padding.insert(0, {\"Timestamp\": -1, \"X\": 0, \"Y\": 0, \"Z\": 0})\n",
    "#             df_padding.insert(0, {\"Timestamp\": -1, \"X\": 0, \"Y\": 0, \"Z\": 0})\n",
    "\n",
    "#             df_train_data = pd.concat(\n",
    "#                 [pd.DataFrame(df_padding), df_train_data], ignore_index=True\n",
    "#             )\n",
    "#             df_padding = pd.DataFrame({\"Timestamp\": [-1, -1, -1],\n",
    "#                                         \"X\": [0, 0, 0],\n",
    "#                                         \"Y\": [0, 0, 0],\n",
    "#                                         \"Z\": [0, 0, 0]})\n",
    "\n",
    "#             df_train_data_padding = df_train_data.append(df_padding, ignore_index=True)\n",
    "#         else:  # FIXME remove this it's just a quickfix because there is the option with or without padding\n",
    "#             # and the next loop has to be on the original dataframe withtout padding\n",
    "#             df_train_data_padding = df_train_data\n",
    "\n",
    "#         # BUG : This df_velocity contains 3 extra rows. I'm not sure where they come from \n",
    "#         for row in df_train_data[[\"X\", \"Y\", \"Z\"]].itertuples():\n",
    "#             end = row.Index + n_zero\n",
    "\n",
    "#             start = row.Index - n_zero\n",
    "\n",
    "#             # QUICKFIX to the padding and pointers issue \n",
    "#             if start == -3:\n",
    "#                 start = 0\n",
    "#                 end = 6\n",
    "#             elif start == -2:\n",
    "#                 start = 1\n",
    "#                 end = 7\n",
    "#             elif start == -1:\n",
    "#                 start = 2\n",
    "#                 end = 8\n",
    "#             elif end > len(df_train_data) - 1 and not padding:\n",
    "#                 end = len(df_train_data)\n",
    "#                 start = len(df_train_data) - (2 * n_zero)\n",
    "   \n",
    "#             df_velocity_X.append(get_derivative_value(df_train_data_padding.loc[start:end, \"X\"], m))\n",
    "#             df_velocity_Y.append(get_derivative_value(df_train_data_padding.loc[start:end, \"Y\"], m))\n",
    "#             df_velocity_Z.append(get_derivative_value(df_train_data_padding.loc[start:end, \"Z\"], m))\n",
    "# # df3 = pd.DataFrame([[2.3,1,2,3]], columns=['Timestamp','X','Y','Z'])\n",
    "#         # Build the DataFrame with all the columns together so we can save it to CSV\n",
    "#         df_velocity = pd.DataFrame({\"X_velocity\": df_velocity_X,\n",
    "#                                     \"Y_velocity\": df_velocity_Y,\n",
    "#                                     \"Z_velocity\": df_velocity_Z,})\n",
    "\n",
    "#         df_velocity.to_csv(\n",
    "#             data_dir + derivative_path + df_train_label[\"measurement_id\"][idx] + \".csv\",\n",
    "#             index=False,\n",
    "#             header=[\"X_velocity\", \"Y_velocity\", \"Z_velocity\"],\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exist :  0\n",
      "File exist :  1\n",
      "File exist :  2\n",
      "File exist :  3\n",
      "File exist :  4\n",
      "File exist :  5\n",
      "File exist :  6\n",
      "File exist :  7\n",
      "File exist :  8\n",
      "File exist :  9\n",
      "File exist :  10\n",
      "File exist :  11\n",
      "File exist :  12\n",
      "File exist :  13\n",
      "File exist :  14\n",
      "File exist :  15\n",
      "File exist :  16\n",
      "File exist :  17\n",
      "File exist :  18\n",
      "File exist :  19\n",
      "File exist :  20\n",
      "File exist :  21\n",
      "File exist :  22\n",
      "File exist :  23\n",
      "File exist :  24\n",
      "File exist :  25\n",
      "File exist :  26\n",
      "File exist :  27\n",
      "File exist :  28\n",
      "File exist :  29\n",
      "File exist :  30\n",
      "File exist :  31\n",
      "File exist :  32\n",
      "File exist :  33\n",
      "File exist :  34\n",
      "File exist :  35\n",
      "File exist :  36\n",
      "File exist :  37\n",
      "File exist :  38\n",
      "File exist :  39\n",
      "File exist :  40\n",
      "File exist :  41\n",
      "File exist :  42\n",
      "File exist :  43\n",
      "File exist :  44\n",
      "File exist :  45\n",
      "File exist :  46\n",
      "File exist :  47\n",
      "File exist :  48\n",
      "File exist :  49\n",
      "File exist :  50\n",
      "File exist :  51\n",
      "File exist :  52\n",
      "File exist :  53\n",
      "File exist :  54\n",
      "File exist :  55\n",
      "File exist :  56\n",
      "File exist :  57\n",
      "File exist :  58\n",
      "File exist :  59\n",
      "File exist :  60\n",
      "File exist :  61\n",
      "File exist :  62\n",
      "File exist :  63\n",
      "File exist :  64\n",
      "File exist :  65\n",
      "File exist :  66\n",
      "File exist :  67\n",
      "File exist :  68\n",
      "File exist :  69\n",
      "File exist :  70\n",
      "File exist :  71\n",
      "File exist :  72\n",
      "File exist :  73\n",
      "File exist :  74\n",
      "File exist :  75\n",
      "File exist :  76\n",
      "File exist :  77\n",
      "File exist :  78\n",
      "File exist :  79\n",
      "File exist :  80\n",
      "File exist :  81\n",
      "File exist :  82\n",
      "File exist :  83\n",
      "File exist :  84\n",
      "File exist :  85\n",
      "File exist :  86\n",
      "File exist :  87\n",
      "File exist :  88\n",
      "File exist :  89\n",
      "File exist :  90\n",
      "File exist :  91\n",
      "File exist :  92\n",
      "File exist :  93\n",
      "File exist :  94\n",
      "File exist :  95\n",
      "File exist :  96\n",
      "File exist :  97\n",
      "File exist :  98\n",
      "File exist :  99\n",
      "File exist :  100\n",
      "File exist :  101\n",
      "File exist :  102\n",
      "File exist :  103\n",
      "File exist :  104\n",
      "File exist :  105\n",
      "File exist :  106\n",
      "File exist :  107\n",
      "File exist :  108\n",
      "File exist :  109\n",
      "File exist :  110\n",
      "File exist :  111\n",
      "File exist :  112\n",
      "File exist :  113\n",
      "File exist :  114\n",
      "File exist :  115\n",
      "File exist :  116\n",
      "File exist :  117\n",
      "File exist :  118\n",
      "File exist :  119\n",
      "File exist :  120\n",
      "File exist :  121\n",
      "File exist :  122\n",
      "File exist :  123\n",
      "File exist :  124\n",
      "File exist :  125\n",
      "File exist :  126\n",
      "File exist :  127\n",
      "File exist :  128\n",
      "File exist :  129\n",
      "File exist :  130\n",
      "File exist :  131\n",
      "File exist :  132\n",
      "File exist :  133\n",
      "File exist :  134\n",
      "File exist :  135\n",
      "File exist :  136\n",
      "File exist :  137\n",
      "File exist :  138\n",
      "File exist :  139\n",
      "File exist :  140\n",
      "File exist :  141\n",
      "File exist :  142\n",
      "File exist :  143\n",
      "File exist :  144\n",
      "File exist :  145\n",
      "File exist :  146\n",
      "File exist :  147\n",
      "File exist :  148\n",
      "File exist :  149\n",
      "File exist :  150\n",
      "File exist :  151\n",
      "File exist :  152\n",
      "File exist :  153\n",
      "File exist :  154\n",
      "File exist :  155\n",
      "File exist :  156\n",
      "File exist :  157\n",
      "File exist :  158\n",
      "File exist :  159\n",
      "File exist :  160\n",
      "File exist :  161\n",
      "File exist :  162\n",
      "File exist :  163\n",
      "File exist :  164\n",
      "File exist :  165\n",
      "File exist :  166\n",
      "File exist :  167\n",
      "File exist :  168\n",
      "File exist :  169\n",
      "File exist :  170\n",
      "File exist :  171\n",
      "File exist :  172\n",
      "File exist :  173\n",
      "File exist :  174\n",
      "File exist :  175\n",
      "File exist :  176\n",
      "File exist :  177\n",
      "File exist :  178\n",
      "File exist :  179\n",
      "File exist :  180\n",
      "File exist :  181\n",
      "File exist :  182\n",
      "File exist :  183\n",
      "File exist :  184\n",
      "File exist :  185\n",
      "File exist :  186\n",
      "File exist :  187\n",
      "File exist :  188\n",
      "File exist :  189\n",
      "File exist :  190\n",
      "File exist :  191\n",
      "File exist :  192\n",
      "File exist :  193\n",
      "File exist :  194\n",
      "File exist :  195\n",
      "File exist :  196\n",
      "File exist :  197\n",
      "File exist :  198\n",
      "File exist :  199\n",
      "File exist :  200\n",
      "File exist :  201\n",
      "File exist :  202\n",
      "File exist :  203\n",
      "File exist :  204\n",
      "File exist :  205\n",
      "File exist :  206\n",
      "File exist :  207\n",
      "File exist :  208\n",
      "File exist :  209\n",
      "File exist :  210\n",
      "File exist :  211\n",
      "File exist :  212\n",
      "File exist :  213\n",
      "File exist :  214\n",
      "File exist :  215\n",
      "File exist :  216\n",
      "File exist :  217\n",
      "File exist :  218\n",
      "File exist :  219\n",
      "File exist :  220\n",
      "File exist :  221\n",
      "File exist :  222\n",
      "File exist :  223\n",
      "File exist :  224\n",
      "File exist :  225\n",
      "File exist :  226\n",
      "File exist :  227\n",
      "File exist :  228\n",
      "File exist :  229\n",
      "File exist :  230\n",
      "File exist :  231\n",
      "File exist :  232\n",
      "File exist :  233\n",
      "File exist :  234\n",
      "File exist :  235\n",
      "File exist :  236\n",
      "File exist :  237\n",
      "File exist :  238\n",
      "File exist :  239\n",
      "File exist :  240\n",
      "File exist :  241\n",
      "File exist :  242\n",
      "File exist :  243\n",
      "File exist :  244\n",
      "File exist :  245\n",
      "File exist :  246\n",
      "File exist :  247\n",
      "File exist :  248\n",
      "File exist :  249\n",
      "File exist :  250\n",
      "File exist :  251\n",
      "File exist :  252\n",
      "File exist :  253\n",
      "File exist :  254\n",
      "File exist :  255\n",
      "File exist :  256\n",
      "File exist :  257\n",
      "File exist :  258\n",
      "File exist :  259\n",
      "File exist :  260\n",
      "File exist :  261\n",
      "File exist :  262\n",
      "File exist :  263\n",
      "File exist :  264\n",
      "File exist :  265\n",
      "File exist :  266\n",
      "File exist :  267\n",
      "File exist :  268\n",
      "File exist :  269\n",
      "File exist :  270\n",
      "File exist :  271\n",
      "File exist :  272\n",
      "File exist :  273\n",
      "File exist :  274\n",
      "File exist :  275\n",
      "File exist :  276\n",
      "File exist :  277\n",
      "File exist :  278\n",
      "File exist :  279\n",
      "File exist :  280\n",
      "File exist :  281\n",
      "File exist :  282\n",
      "File exist :  283\n",
      "File exist :  284\n",
      "File exist :  285\n",
      "File exist :  286\n",
      "File exist :  287\n",
      "File exist :  288\n",
      "File exist :  289\n",
      "File exist :  290\n",
      "File exist :  291\n",
      "File exist :  292\n",
      "File exist :  293\n",
      "File exist :  294\n",
      "File exist :  295\n",
      "File exist :  296\n",
      "File exist :  297\n",
      "File exist :  298\n",
      "File exist :  299\n",
      "File exist :  300\n",
      "File exist :  301\n",
      "File exist :  302\n",
      "File exist :  303\n",
      "File exist :  304\n",
      "File exist :  305\n",
      "File exist :  306\n",
      "File exist :  307\n",
      "File exist :  308\n",
      "File exist :  309\n",
      "File exist :  310\n",
      "File exist :  311\n",
      "File exist :  312\n",
      "File exist :  313\n",
      "File exist :  314\n",
      "File exist :  315\n",
      "File exist :  316\n",
      "File exist :  317\n",
      "File exist :  318\n",
      "File exist :  319\n",
      "File exist :  320\n",
      "File exist :  321\n",
      "File exist :  322\n",
      "File exist :  323\n",
      "File exist :  324\n",
      "File exist :  325\n",
      "File exist :  326\n",
      "File exist :  327\n",
      "File exist :  328\n",
      "File exist :  329\n",
      "File exist :  330\n",
      "File exist :  331\n",
      "File exist :  332\n",
      "File exist :  333\n",
      "File exist :  334\n",
      "File exist :  335\n",
      "File exist :  336\n",
      "File exist :  337\n",
      "File exist :  338\n",
      "File exist :  339\n",
      "File exist :  340\n",
      "File exist :  341\n",
      "File exist :  342\n",
      "File exist :  343\n",
      "File exist :  344\n",
      "File exist :  345\n",
      "File exist :  346\n",
      "File exist :  347\n",
      "File exist :  348\n",
      "File exist :  349\n",
      "File exist :  350\n",
      "File exist :  351\n",
      "File exist :  352\n",
      "File exist :  353\n",
      "File exist :  354\n",
      "File exist :  355\n",
      "File exist :  356\n",
      "File exist :  357\n",
      "File exist :  358\n",
      "File exist :  359\n",
      "File exist :  360\n",
      "File exist :  361\n",
      "File exist :  362\n",
      "File exist :  363\n",
      "File exist :  364\n",
      "File exist :  365\n",
      "File exist :  366\n",
      "File exist :  367\n",
      "File exist :  368\n",
      "File exist :  369\n",
      "File exist :  370\n",
      "File exist :  371\n",
      "File exist :  372\n",
      "File exist :  373\n",
      "File exist :  374\n",
      "File exist :  375\n",
      "File exist :  376\n",
      "File exist :  377\n",
      "File exist :  378\n",
      "File exist :  379\n",
      "File exist :  380\n",
      "File exist :  381\n",
      "File exist :  382\n",
      "File exist :  383\n",
      "File exist :  384\n",
      "File exist :  385\n",
      "File exist :  386\n",
      "File exist :  387\n",
      "File exist :  388\n",
      "File exist :  389\n",
      "File exist :  390\n",
      "measurement_id  2ff97448-e9e1-4530-a660-cf1b3f31c1ae\n",
      "measurement_id  cc7b73aa-90b0-4ec3-8c7b-2f7e54f7806e\n"
     ]
    }
   ],
   "source": [
    "data_type = \"cis\"\n",
    "\n",
    "# TODO: explain\n",
    "path_train_data, df_train_label = define_data_type(data_type=data_type)\n",
    "\n",
    "# list_measurement_id=['2d852742-10a9-4c56-9f38-779f2cd66879',\n",
    "# '4fc3c295-857f-4920-8fa5-f21bfdc7ab4f',\n",
    "# 'db2e053a-0fb8-4206-891a-6f079fb14e3a']\n",
    "\n",
    "# Filter df_train_label according to the measurement_id we are most interested in\n",
    "# df_train_label = interesting_patients(df_train_label=df_train_label, list_measurement_id=list_measurement_id)\n",
    "\n",
    "# TODO Get first derivative of the data with high pass\n",
    "get_first_derivative(df_train_label, \n",
    "                     \"cis-pd.training_data.velocity_original_data/\", \n",
    "                     padding=True, \n",
    "                     mask_path='/home/sjoshi/codes/python/BeatPD/data/BeatPD/cis-pd.training_data.high_pass_mask/')\n",
    "\n",
    "# n zero vector / k\n",
    "# k = convolution of the whole signal with v\n",
    "# dont have to separate the signal in chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIS-PD Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contains 16 subject_id (patients) for the training set \n",
    "\n",
    "- Gender: 11 Male, 5 Female \n",
    "- Race: 15 White, 1 NA\n",
    "- Ethnicity: 15 Not Hispanic or Latino, 1 Unknown\n",
    "- Age average (standard deviation) : 62.8125 (10.8579)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_type = \"cis\"\n",
    "\n",
    "# TODO: explain\n",
    "path_train_data, df_train_label = define_data_type(data_type=data_type)\n",
    "# display(df_train_label)\n",
    "# List of interesting measurement id we want to look at\n",
    "# list_measurement_id=[#'ab5287f4-8261-47ad-8ff2-22b5fe5d246e',\n",
    "#'db2e053a-0fb8-4206-891a-6f079fb14e3a']#,\n",
    "# 'ef5b1267-c212-46c5-aab0-4f4437bc6c67',\n",
    "# '4ec74fb9-7347-435d-83dc-79ad74c3bc49',\n",
    "# '8e8539ad-8841-476b-b15c-888ce3461989',\n",
    "# '22b88456-fe8f-4138-af55-be12afca4b81',\n",
    "# 'ad84583d-e5ae-4926-b077-531a0f7d08a9',\n",
    "# 'eef56825-940a-4c3e-aebb-60838d60869e',\n",
    "# 'e0441156-c4b8-467c-8f4f-3b532d594d8f',\n",
    "# '464ac314-6c4b-4c4a-957c-28a2339150d6']\n",
    "\n",
    "list_measurement_id = [\n",
    "    \"5cf68c8e-0b7a-4b73-ad4f-015c7a20fb5a\",\n",
    "    \"cc7b822c-e310-46f0-a8ea-98c95fdb67a1\",\n",
    "    \"5163afe8-a6b0-4ea4-b2ba-9b4501dd5912\",\n",
    "    \"db2e053a-0fb8-4206-891a-6f079fb14e3a\",\n",
    "    \"2d852742-10a9-4c56-9f38-779f2cd66879\",\n",
    "    \"2e3a4c9c-ff01-4a28-bfcf-ce9b7633a39d\",  # no inactivity should be removed\n",
    "    \"3cf49c01-0499-4bad-9167-67691711204a\",  # no inactivity should be removed PAS LA??\n",
    "    \"3d0f965c-9d72-43d1-9369-1ea3acf963cc\",  # PAS LA ???\n",
    "    \"4b269cc2-8f0c-4816-adbf-10c0069b8833\",\n",
    "    \"4bc51b90-bfce-4231-85e1-5de3b4bc0745\",\n",
    "    \"4fc3c295-857f-4920-8fa5-f21bfdc7ab4f\",\n",
    "]  # bit of inactivity in the middle]\n",
    "\n",
    "list_measurement_id = [\n",
    "    \"2d852742-10a9-4c56-9f38-779f2cd66879\",\n",
    "    \"4fc3c295-857f-4920-8fa5-f21bfdc7ab4f\",\n",
    "    \"db2e053a-0fb8-4206-891a-6f079fb14e3a\",\n",
    "]\n",
    "\n",
    "# Filter df_train_label according to the measurement_id we are most interested in\n",
    "df_train_label = interesting_patients(\n",
    "    df_train_label=df_train_label, list_measurement_id=list_measurement_id\n",
    ")\n",
    "\n",
    "# Display filtered df_train_label\n",
    "display(df_train_label)\n",
    "\n",
    "# path_no_inactivity_data = remove_inactivity_pct_change(df_train_label)\n",
    "\n",
    "# Plot the accelerometer data\n",
    "plot_accelerometer(\n",
    "    data_type=data_type, path_accelerometer_plots=path_save_accelerometer_plots\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on  cc7b822c-e310-46f0-a8ea-98c95fdb67a1\n",
      "X_zeros.shape  (44940,)\n",
      "X_zeros.shape 2  (44940,)\n",
      "df_zeros.shape  (44940,)\n",
      "df_zeros.shape 2 (44940,)\n",
      "df_mask_highpass.shape  (44940, 1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-19372e25f5e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mduration_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mplot_frequency_response\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     mask_path='/home/sjoshi/codes/python/BeatPD/data/BeatPD/cis-pd.training_data.high_pass_mask/')\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# remove_inactivity_mean_offset(df_train_label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-0c0b02c531ed>\u001b[0m in \u001b[0;36mremove_inactivity_highpass\u001b[0;34m(df_train_label, energy_threshold, duration_threshold, mask_path, plot_frequency_response, plot_accelerometer_after_removal)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;31m#         )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'df_mask_highpass.shape '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_mask_highpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;31m# Plot the accelerometer with the removed sections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# The [0] is used to get a pandas.Series instead of a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "data_type = \"cis\"\n",
    "\n",
    "# TODO: explain\n",
    "path_train_data, df_train_label = define_data_type(data_type=data_type)\n",
    "\n",
    "# list_measurement_id = [\"5cf68c8e-0b7a-4b73-ad4f-015c7a20fb5a\"]\n",
    "\n",
    "# list_measurement_id = [\"2e3a4c9c-ff01-4a28-bfcf-ce9b7633a39d\"]\n",
    "\n",
    "# list_measurement_id = [\n",
    "#     \"2d852742-10a9-4c56-9f38-779f2cd66879\",\n",
    "#     \"4fc3c295-857f-4920-8fa5-f21bfdc7ab4f\",\n",
    "#     \"db2e053a-0fb8-4206-891a-6f079fb14e3a\",\n",
    "# ]\n",
    "\n",
    "# list_measurement_id = [\"db2e053a-0fb8-4206-891a-6f079fb14e3a\"]\n",
    "\n",
    "# list_measurement_id=['2d852742-10a9-4c56-9f38-779f2cd66879']\n",
    "# list_measurement_id=['5cf68c8e-0b7a-4b73-ad4f-015c7a20fb5a',\n",
    "#                      'cc7b822c-e310-46f0-a8ea-98c95fdb67a1',\n",
    "#                      '5163afe8-a6b0-4ea4-b2ba-9b4501dd5912',\n",
    "#                     'db2e053a-0fb8-4206-891a-6f079fb14e3a',\n",
    "#                     '2d852742-10a9-4c56-9f38-779f2cd66879',\n",
    "#                     '2e3a4c9c-ff01-4a28-bfcf-ce9b7633a39d', # no inactivty should be removed\n",
    "#                     '3cf49c01-0499-4bad-9167-67691711204a']\n",
    "\n",
    "# df_train_label = interesting_patients(df_train_label=df_train_label, list_measurement_id=list_measurement_id)\n",
    "\n",
    "remove_inactivity_highpass(\n",
    "    df_train_label,\n",
    "    energy_threshold=5,\n",
    "    duration_threshold=3000,\n",
    "    plot_frequency_response=False,\n",
    "    mask_path='/home/sjoshi/codes/python/BeatPD/data/BeatPD/cis-pd.training_data.high_pass_mask/')\n",
    "\n",
    "# remove_inactivity_mean_offset(df_train_label)\n",
    "# path_no_inactivity_data = remove_inactivity_max(df_train_label)\n",
    "\n",
    "# plot_accelerometer(data_type=data_type, path_accelerometer_plots=path_save_accelerometer_plots, path_inactivity=path_no_inactivity_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_type = \"cis\"\n",
    "\n",
    "# TODO: explain\n",
    "path_train_data, df_train_label = define_data_type(data_type=data_type)\n",
    "\n",
    "# list_measurement_id=['5cf68c8e-0b7a-4b73-ad4f-015c7a20fb5a',\n",
    "#                      'cc7b822c-e310-46f0-a8ea-98c95fdb67a1',\n",
    "#                      '5163afe8-a6b0-4ea4-b2ba-9b4501dd5912',\n",
    "#                     'db2e053a-0fb8-4206-891a-6f079fb14e3a',\n",
    "#                     '2d852742-10a9-4c56-9f38-779f2cd66879',\n",
    "#                     '2e3a4c9c-ff01-4a28-bfcf-ce9b7633a39d', # no inactivity should be removed\n",
    "#                     '3cf49c01-0499-4bad-9167-67691711204a',# no inactivty should be removed\n",
    "#                     '3d0f965c-9d72-43d1-9369-1ea3acf963cc',\n",
    "#                     '4b269cc2-8f0c-4816-adbf-10c0069b8833',\n",
    "#                     '4bc51b90-bfce-4231-85e1-5de3b4bc0745',\n",
    "#                     '4fc3c295-857f-4920-8fa5-f21bfdc7ab4f'] #bit of inactivity in the middle]\n",
    "\n",
    "list_measurement_id = [\"5cf68c8e-0b7a-4b73-ad4f-015c7a20fb5a\"]\n",
    "\n",
    "df_train_label = interesting_patients(\n",
    "    df_train_label=df_train_label, list_measurement_id=list_measurement_id\n",
    ")\n",
    "\n",
    "path_no_inactivity_data = remove_inactivity_pct_change(df_train_label)\n",
    "\n",
    "# Plot the accelerometer data\n",
    "# plot_accelerometer(data_type=data_type, path_accelerometer_plots=path_save_accelerometer_plots, path_inactivity=path_no_inactivity_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Occurences of each symptoms for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the occurences of each symptoms for each patient\n",
    "df_occurences, df_train_label_subject_id = compute_symptoms_occurences_dataframe(\n",
    "    df_train_label=df_train_label\n",
    ")\n",
    "\n",
    "# Plot the graphs\n",
    "plot_symptoms_occurences(\n",
    "    df_occurences=df_occurences, df_train_label_subject_id=df_train_label_subject_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_fold(df_train_label, n_splits=5, subject_id=None, data_real_subtype=\"\"):\n",
    "    \"\"\"\n",
    "    Function that returns a list of X dataframes (X is according to the number of n_splits chosen)\n",
    "\n",
    "    The dataframes are the labels needed according to the split \n",
    "\n",
    "    Keyword Arguments:\n",
    "    df_train_label: Dataframe containing the labels\n",
    "    n_split: Optional. The number of folds. Default: 5\n",
    "    subject_id: Optional. Specify a subject_id to get measurement_id only for that subject_id \n",
    "    data_real_subtype: Only for REAL-PD database\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits)\n",
    "\n",
    "    # Building the dataframe to split\n",
    "    X = []\n",
    "\n",
    "    # if we want the data split for one specific subject_id\n",
    "    if subject_id:\n",
    "        df_train_label_subject_id = df_train_label.groupby(\"subject_id\")\n",
    "        X = df_train_label_subject_id.get_group(subject_id)\n",
    "\n",
    "    # if we want to have all a split for all data no matter the subject_id\n",
    "    # NOTE: I didn't make sure to have one subject_id represented in both train/test\n",
    "    else:\n",
    "        for idx in df_train_label.index:\n",
    "            X.append([df_train_label[\"measurement_id\"][idx]])\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    kf.get_n_splits(X)\n",
    "\n",
    "    # Building lists of df_train_label because we have by default 5 splits,\n",
    "    # so the lists will contain 5 DataFrames with different split indices required\n",
    "    list_df_train_label = list()\n",
    "    list_df_test_label = list()\n",
    "    split_idx = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        df_train_label = X.iloc[train_index]\n",
    "        df_test_label = X.iloc[test_index]\n",
    "\n",
    "        list_df_train_label.append(df_train_label)\n",
    "        list_df_test_label.append(df_test_label)\n",
    "\n",
    "        # name of the file according to its database and type\n",
    "        path_save_k_fold_dataframes = (\n",
    "            data_dir + data_type + \"-pd.training_data.k_fold/\" + data_real_subtype + \"/\"\n",
    "        )\n",
    "        df_train_label.to_csv(\n",
    "            path_save_k_fold_dataframes\n",
    "            + str(subject_id)\n",
    "            + \"_train_kfold_\"\n",
    "            + str(split_idx)\n",
    "            + \".csv\",\n",
    "            index=False,\n",
    "            header=[\"measurement_id\", \"subject_id\", \"on_off\", \"dyskinesia\", \"tremor\"],\n",
    "        )\n",
    "        df_test_label.to_csv(\n",
    "            path_save_k_fold_dataframes\n",
    "            + str(subject_id)\n",
    "            + \"_test_kfold_\"\n",
    "            + str(split_idx)\n",
    "            + \".csv\",\n",
    "            index=False,\n",
    "            header=[\"measurement_id\", \"subject_id\", \"on_off\", \"dyskinesia\", \"tremor\"],\n",
    "        )\n",
    "        split_idx = split_idx + 1\n",
    "    return list_df_train_label, list_df_test_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the K-Fold files for the CIS database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the data type as we have two databases\n",
    "data_type = \"cis\"\n",
    "path_train_data, df_train_label = define_data_type(data_type=data_type)\n",
    "display(df_train_label)\n",
    "\n",
    "# Group data by subject_id\n",
    "df_train_label_subject_id = df_train_label.groupby(\"subject_id\")\n",
    "\n",
    "# Go through the subject_id and k-fold their data\n",
    "# FIXME: get_k_fold could me renamed to just create the folds, save them, not return anything\n",
    "for subject_id, value in df_train_label_subject_id:\n",
    "    list_df_train_label, list_df_test_label = get_k_fold(\n",
    "        df_train_label=df_train_label, n_splits=5, subject_id=subject_id\n",
    "    )\n",
    "\n",
    "\n",
    "#### Example on how to read data\n",
    "\n",
    "# TODO\n",
    "# Read data from folders\n",
    "\n",
    "# The following is not a really good example, it would be better to have an example where we read the\n",
    "# data from the folder they're saved at\n",
    "\n",
    "\n",
    "# Iterate through the DataFrame in the list list_df_train_label_train\n",
    "# for df_train_label in list_df_train_label_train:\n",
    "#     # Go through the measurement_id for this df_train_label split to read the training data corresponding\n",
    "#     for idx in df_train_label.index:\n",
    "#         # Read the train data for this specific measurement_id\n",
    "#         # TODO: Do we want to append all of this training data together?\n",
    "#         df_train_data=pd.read_csv(path_train_data+df_train_label[\"measurement_id\"][idx]+'.csv')\n",
    "#         #display(df_train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the K-Fold data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the K-Fold Files for the REAL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate the files, you have to uncomment one data_real_subtype at a time and\n",
    "# execute this cell 3 times for the 3 subtypes.\n",
    "\n",
    "data_type = \"real\"\n",
    "# data_real_subtype='smartphone_accelerometer'\n",
    "data_real_subtype = \"smartwatch_accelerometer\"\n",
    "# data_real_subtype='smartwatch_gyroscope'\n",
    "\n",
    "path_train_data, df_train_label = define_data_type(data_type=data_type)\n",
    "\n",
    "# Group data by subject_id\n",
    "df_train_label_subject_id = df_train_label.groupby(\"subject_id\")\n",
    "\n",
    "# Go through the subject_id and k-fold their data\n",
    "for subject_id, value in df_train_label_subject_id:\n",
    "    list_df_train_label, list_df_test_label = get_k_fold(\n",
    "        df_train_label=df_train_label,\n",
    "        n_splits=5,\n",
    "        subject_id=subject_id,\n",
    "        data_real_subtype=data_real_subtype,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REAL-PD Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This database, originally named \"Parkinson@Home\" is renamed to \"Real-PD\" for this challenge. The study was made over 2 weeks, with at home monitoring. \n",
    "\n",
    "The devices used are an android phone, a motorolla watch. \n",
    "- `smartwatch_accelerometer` and `smartwatch_gyroscope` : Motorolla Watch\n",
    "- `smartphone_accelerometer` : Android phone \n",
    "\n",
    "-> Question: so is the smartwatch & smartphone accelerometer should measure the same movements? \n",
    "\n",
    "The REAL-PD database has many missing values. \n",
    "\n",
    "The subject_id `hbv013` is the only one without missing data. Other patients all have at least one missing symptom (`diskenisia`, ) or two (`on/off and tremor`, `on_off and dyskinesia`, `dyskinesia and tremor`) missing.\n",
    "\n",
    "Measurements id with no data (`on_off`, `dyskinesia` and `tremor` are all missing):\n",
    "- `b50d1b0c-2cd1-45f8-9097-0742e5cbbcc8`\n",
    "- `b598c177-4e38-4ea8-8543-bd8f7e580f96`\n",
    "- `cf841bf8-0082-4ea3-999f-1f43e39a8dc6`\n",
    "- `b1e15f8a-109f-459b-ba87-46899240ee66`\n",
    "- `6f0e2580-56ec-4743-9356-d3e4d9a0aee5`\n",
    "- `773536f6-9b70-43d0-b099-5d167d74924a`\n",
    "- `54a0e841-ad45-4ba7-ac83-1785c5f7748b`\n",
    "- `cd9ed2e2-7e04-44c7-b041-7788f133c193`\n",
    "- `a6954a91-338b-4523-9e4a-5e69a8fac206`\n",
    "\n",
    "The 3 symptoms are reported as follows in this dataset: \n",
    "- `on_off = {0,1}`\n",
    "  - `Off` : 0 (medication is wearing off) \n",
    "  - `On` : 1 (medication is working)\n",
    "  \n",
    "- `dyskinesia = {0,1,2}`\n",
    "  - Without dyskinesia: 0 \n",
    "  - Non-troublesome dyskinesia: 1 \n",
    "  - Severe dyskinesia: 2 \n",
    "  \n",
    "- `tremor = {0,1,2,3,4}` \n",
    "The description of the database mentions `tremor` is rated from 0 to 4 according to its severity, but from all the data, the maximum value of `tremor` recorded is 3. \n",
    "\n",
    "Data:\n",
    "- ancillary\n",
    "- clinical : UPDRS evaluation score \n",
    "- demographics : #TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"real\"\n",
    "# data_real_subtype='smartphone_accelerometer'\n",
    "data_real_subtype = \"smartwatch_accelerometer\"\n",
    "# data_real_subtype='smartwatch_gyroscope'\n",
    "\n",
    "path_train_data, df_train_label = define_data_type(data_type=data_type)\n",
    "\n",
    "# List of interesting measurement id we want to look at\n",
    "list_measurement_id = [\n",
    "    \"5b4c7c81-659d-40ea-a1fd-59622074fd10\",\n",
    "    \"ee053d95-c155-400d-ae42-fe24834ad4a9\",\n",
    "    \"ce51ee31-8553-4321-9f83-8cd3dabe2f66\",\n",
    "    \"e07708ff-7b8d-4070-af70-3aa81423ab5b\",\n",
    "    #'7d3f4b7a-167f-4a26-9062-94ce9d8794c1',\n",
    "    \"99af8d14-cd09-4107-9502-355378ba4e08\",\n",
    "    #'7d5ac31a-cb53-40f7-8188-0b13724ea55c',\n",
    "    \"9e43840b-dd89-498b-af1a-a62896a4d5d9\",\n",
    "    \"e391f546-bf8a-46c7-a16c-95bc02f40629\",\n",
    "]\n",
    "\n",
    "# Filter df_train_label according to the measurement_id we are most interested in\n",
    "df_train_label = interesting_patients(\n",
    "    df_train_label=df_train_label, list_measurement_id=list_measurement_id\n",
    ")\n",
    "\n",
    "# Display filtered df_train_label\n",
    "# display(df_train_label)\n",
    "\n",
    "###  Plot the accelerometer data\n",
    "# Path example: /home/sjoshi/codes/python/BeatPD/code/accelerometer_plots/real/smartwatch_gyroscope/\n",
    "path = path_save_accelerometer_plots + \"/\" + data_type + \"/\" + data_real_subtype + \"/\"\n",
    "plot_accelerometer(data_type=data_type, path_accelerometer_plots=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the occurences of each symptoms for each patient\n",
    "df_occurences, df_train_label_subject_id = compute_symptoms_occurences_dataframe(\n",
    "    df_train_label=df_train_label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graphs\n",
    "plot_symptoms_occurences(\n",
    "    df_occurences=df_occurences, df_train_label_subject_id=df_train_label_subject_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing both databases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}\n",
    ".tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\"></th>\n",
    "    <th class=\"tg-0pky\">CIS-PD</th>\n",
    "    <th class=\"tg-0pky\">REAL-PD</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\"># of subject_id training</td>\n",
    "    <td class=\"tg-c3ow\">16</td>\n",
    "    <td class=\"tg-c3ow\">12</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\"># of female training</td>\n",
    "    <td class=\"tg-c3ow\">5</td>\n",
    "    <td class=\"tg-c3ow\">7</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\"># of male training</td>\n",
    "    <td class=\"tg-c3ow\">11</td>\n",
    "    <td class=\"tg-c3ow\">5</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Age average (std deviation)</td>\n",
    "    <td class=\"tg-c3ow\">62.8125 (10.857)</td>\n",
    "    <td class=\"tg-c3ow\">59.833 (5.828)</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame for specified subject_id\n",
    "# display(df_train_label_subject_id.get_group('hbv013'))\n",
    "\n",
    "# print(display(df_train_label_subject_id.get_group('hbv013')['tremor'].value_counts()))\n",
    "\n",
    "# Graphs for a specific subject_id the 3 symptoms\n",
    "# df_train_label_subject_id.get_group('hbv014')['tremor'].value_counts().plot(kind='bar', title='tremor')\n",
    "# df_train_label_subject_id.get_group('hbv014')['dyskinesia'].value_counts().plot(kind='bar', title='dys')\n",
    "# df_train_label_subject_id.get_group('hbv014')['on_off'].value_counts().plot(kind='bar', title='on_off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests & Drafts, back-up space that's not important, just notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20 * 60 / 59848"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the default option to display all row with display(DF)\n",
    "# pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tried to do convolution instead of np.multiply and dot product to get the derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(\n",
    "    np.array(\n",
    "        [\n",
    "            [0.2, 1, 5, 9],\n",
    "            [0.4, 2, 6, 10],\n",
    "            [0.6, 3, 7, 11],\n",
    "            [0.8, 4, 8, 12],\n",
    "            [1, 5, 9, 13],\n",
    "            [1.2, 6, 10, 14],\n",
    "            [1.4, 7, 11, 15],\n",
    "            [1.6, 7, 11, 15],\n",
    "            [1.8, 8, 12, 16],\n",
    "            [2, 9, 13, 16],\n",
    "        ]\n",
    "    ),\n",
    "    columns=[\"Timestamp\", \"X\", \"Y\", \"Z\"],\n",
    ")\n",
    "display(df2)\n",
    "m = np.linspace(-3, 3, num=2 * 3 + 1)\n",
    "display(m)\n",
    "\n",
    "np.convolve(df2.loc[0:6, \"X\"], m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to filter a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of the data to find edge cases\n",
    "\n",
    "\n",
    "# Create variable with TRUE if nationality is USA\n",
    "dys = df_train_label[\"dyskinesia\"] > 1\n",
    "\n",
    "# Create variable with TRUE if age is greater than 50\n",
    "tre = df_train_label[\"on_off\"] > 0\n",
    "\n",
    "# Select all cases where nationality is USA and age is greater than 50\n",
    "df_train_label[dys & tre]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing around with pct_change function to try and remove_silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_pct_change = df_train_data.iloc[:,-3:].pct_change(periods=5)\n",
    "# #df_pct_change = df_train_data['X'].pct_change(periods=5)\n",
    "# df_pct_change.columns = ['X', 'Y','Z']\n",
    "# print('pct_change measurement id : ', df_train_label[\"measurement_id\"][idx])\n",
    "# display(df_pct_change)\n",
    "# df_pct_change = df_pct_change[df_pct_change > 0.01]\n",
    "\n",
    "cars = {\n",
    "    \"Brand\": [\n",
    "        \"Honda Civic\",\n",
    "        \"Toyota Corolla\",\n",
    "        \"Ford Focus\",\n",
    "        \"Audi A4\",\n",
    "        \"Toyota Corolla\",\n",
    "        \"Ford Focus\",\n",
    "        \"Audi A4\",\n",
    "    ],\n",
    "    \"Price\": [1, 1, 2, 3, 4, 5, 6],\n",
    "    \"Price2\": [2, 3, 4, 5, 6, 7, 8],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(cars, columns=[\"Brand\", \"Price\", \"Price2\"])\n",
    "display(df)\n",
    "\n",
    "display(df.iloc[:, -2:])\n",
    "\n",
    "display(df.iloc[:, -2:].pct_change(periods=1))\n",
    "\n",
    "df = df.iloc[:, -2:].pct_change(periods=1, fill_method=\"ffill\")\n",
    "\n",
    "df_pct_change = df[(df[\"Price\"] > 0.25) & (df[\"Price2\"] > 0.3)]\n",
    "print(\"------------\")\n",
    "display(df_pct_change)\n",
    "print(\"------------\")\n",
    "# display(df['Price'].pct_change(periods=1))\n",
    "\n",
    "# display(df['Price','Price2'].pct_change(periods=2))\n",
    "\n",
    "display(df[\"Price\"].pct_change(periods=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = {  #'Brand': ['Honda Civic','Toyota Corolla','Ford Focus','Audi A4','Toyota Corolla','Ford Focus','Audi A4'],\n",
    "    \"Price\": [1, 1, 2, 3, -10, 5, 6],\n",
    "    \"Price2\": [2, 3, 4, 5, 6, 7, 8],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(cars, columns=[\"Brand\", \"Price\", \"Price2\"])\n",
    "display(df)\n",
    "\n",
    "display(df.abs().max())\n",
    "\n",
    "display((df.abs().max() * 5) / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_inactivity_max(df_train_label):\n",
    "    last_filtered_value = pd.Series(np.zeros(3), index=[\"X\", \"Y\", \"Z\"])\n",
    "    filtered_value = pd.Series(np.zeros(3), index=[\"X\", \"Y\", \"Z\"])\n",
    "    display(last_filtered_value)\n",
    "    for idx in df_train_label.index:\n",
    "        df_allo = []\n",
    "        df_train_data = pd.read_csv(\n",
    "            path_train_data + df_train_label[\"measurement_id\"][idx] + \".csv\"\n",
    "        )\n",
    "\n",
    "        # Get the absolute max value for X, Y, Z\n",
    "        max_values = df_train_data.iloc[:, -3:].abs().max()\n",
    "\n",
    "        # Compute what is 5% of that max\n",
    "        thresold_energy = 5\n",
    "        df_treshold = (max_values * thresold_energy) / 100\n",
    "\n",
    "        # display(df_train_data)\n",
    "        # Candidates are the frames where X, Y, Z are below that treshold (5% of the max)\n",
    "        #         df_candidates = df_train_data[(df_train_data.X.abs() <= df_treshold.X) &\n",
    "        #                                      (df_train_data.Y.abs() <= df_treshold.Y) &\n",
    "        #                                      (df_train_data.Z.abs() <= df_treshold.Z)]\n",
    "        # display(df_candidates)\n",
    "        for idx2 in df_train_data.index:\n",
    "            # print('df_train_data[idx2]')\n",
    "            # display(df_train_data.iloc[idx2,-3:])\n",
    "            last_filtered_value = filtered_value\n",
    "            filtered_value = last_filtered_value + 0.004 * (\n",
    "                df_train_data.iloc[idx2, -3:] - last_filtered_value\n",
    "            )\n",
    "            y = pd.DataFrame(columns=[\"Timestamp\"])\n",
    "            y = pd.concat(\n",
    "                [y, pd.DataFrame([df_train_data.iloc[idx2, 0]], columns=[\"Timestamp\"])],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "            #             print('display y :')\n",
    "            #             display(y)\n",
    "            #             print('end display y')\n",
    "\n",
    "            #             print('display filtered value')\n",
    "            #             display(pd.DataFrame(filtered_value).transpose())\n",
    "            #             print('end display filtered value')\n",
    "            df_allo.append(\n",
    "                pd.concat([y, pd.DataFrame(filtered_value).transpose()], axis=1)\n",
    "            )\n",
    "        #             print('display df_allo')\n",
    "        #             display(df_allo)\n",
    "\n",
    "        # FIXME : change the name df_allo\n",
    "        df_allo = pd.DataFrame(df_allo, columns=(\"Timestamp\", \"X\", \"Y\", \"Z\"))\n",
    "\n",
    "        df_allo.plot(x=\"Timestamp\", legend=True, subplots=True, title=\"allo\")\n",
    "        stop()\n",
    "\n",
    "\n",
    "#         v_candidate_x = pd.DataFrame({'Candidate':list(0)})\n",
    "#         v_candidate_x = np.where(df_train_data.X.abs() <= df_treshold.X, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This didn't work because it's using pct_change between X coordinates where coincidences happens\n",
    "\n",
    "\n",
    "def remove_inactivity_pct_change(df_train_label, data_real_subtype=\"\"):\n",
    "    \"\"\"\n",
    "    Save .csv files with silence (inactivity) removed \n",
    "\n",
    "    Path used: \n",
    "    # cis-pd.training_data.no_silence/\n",
    "    # real-pd.training_data.no_silence/smartphone_accelerometer/\n",
    "    # real-pd.training_data.no_silence/smartwatch_accelerometer/\n",
    "    # real-pd.training_data.no_silence/smartwatch_gyroscope/\n",
    "    # data_type = {'cis', 'real'}\n",
    "\n",
    "    Arguments:\n",
    "    df_train_label: Dataframe with training labels\n",
    "\n",
    "    data_real_subtype: Optional. If data_type is real, data_real_subtype needs to be provided\n",
    "        data_real_subtype={smartphone_accelerometer , smartwatch_accelerometer , smartwatch_gyroscope}\n",
    "\n",
    "    Returns: \n",
    "    path_no_inactivity_data: Return the path where the files are saved because it is needed\n",
    "                          if we want to plot the accelerometer, for example\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for idx in df_train_label.index:\n",
    "        df_train_data = pd.read_csv(\n",
    "            path_train_data + df_train_label[\"measurement_id\"][idx] + \".csv\"\n",
    "        )\n",
    "        # print('measurement id : ', df_train_label[\"measurement_id\"][idx])\n",
    "        # display(df_train_data)\n",
    "        cols_to_norm = [\"x\", \"y\", \"z\"] if data_type == \"real\" else [\"X\", \"Y\", \"Z\"]\n",
    "        df_train_data[cols_to_norm] = df_train_data[cols_to_norm].apply(\n",
    "            lambda x: (x - x.min()) / (x.max() - x.min())\n",
    "        )\n",
    "        periods = 300\n",
    "        df_pct_change = df_train_data.iloc[:, -3:].pct_change(periods=periods)\n",
    "\n",
    "        df_pct_change.columns = [\"X\", \"Y\", \"Z\"]\n",
    "        # print('pct_change measurement id : ', df_train_label[\"measurement_id\"][idx])\n",
    "        # display(df_pct_change)\n",
    "\n",
    "        # Apply the treshold to the DataFrame with an AND condition, so all axis must have at least 1% of change\n",
    "        # between the periods\n",
    "        # pd.options.display.max_rows = 1000\n",
    "        print(\"----------before filter--------\")\n",
    "        display(df_pct_change.abs())\n",
    "\n",
    "        print(\"WHAT IS DETECTED AS INACTIVITY\")\n",
    "        display(\n",
    "            df_pct_change[\n",
    "                (df_pct_change.X.abs() < 0.0002)\n",
    "                | (df_pct_change.Y.abs() < 0.0002)\n",
    "                | (df_pct_change.Z.abs() < 0.0002)\n",
    "            ]\n",
    "        )\n",
    "        print(\"END OF WHAT IS DETECTED AS INACTIVITY\")\n",
    "\n",
    "        df_pct_change = df_pct_change[\n",
    "            (df_pct_change.X.abs() >= 0.0002)\n",
    "            & (df_pct_change.Y.abs() >= 0.0002)\n",
    "            & (df_pct_change.Z.abs() >= 0.0002)\n",
    "        ]\n",
    "        print(\"----------after filter--------\")\n",
    "        display(df_pct_change)\n",
    "\n",
    "        filter_df = df_train_data[\n",
    "            df_train_data.index.isin(df_pct_change.index.to_list())\n",
    "        ]\n",
    "\n",
    "        # Counts the number of time where we had to remove inactivity from a dataframe to know how often\n",
    "        # the inactivity zones appear.\n",
    "        print(\"len(filter_df)+periods \", str(len(filter_df) + periods))\n",
    "        print(\"len(df_train_data) \", str(len(df_train_data)))\n",
    "        if len(filter_df) + periods != len(df_train_data):\n",
    "            count = count + 1\n",
    "\n",
    "        # To provide the name of the header for the Dataframe, we get the name of the x axis as it depends\n",
    "        # on the data_type and then we insert it at the first position before the X,Y,Z axis\n",
    "        x_axis_data_type = \"t\" if data_type == \"real\" else \"Timestamp\"\n",
    "        cols_to_norm.insert(0, x_axis_data_type)\n",
    "\n",
    "        # filter_df.plot(x='Timestamp',legend=True, subplots=True,title='allo')\n",
    "\n",
    "        # Save the dataframe in a file with the measurement_id as the name of the file\n",
    "        path_no_inactivity_data = (\n",
    "            data_dir\n",
    "            + data_type\n",
    "            + \"-pd.training_data.no_silence/\"\n",
    "            + data_real_subtype\n",
    "            + \"/\"\n",
    "        )\n",
    "        filter_df.to_csv(\n",
    "            path_no_inactivity_data + df_train_label[\"measurement_id\"][idx] + \".csv\",\n",
    "            index=False,\n",
    "            header=cols_to_norm,\n",
    "        )\n",
    "    print(\n",
    "        \"Inactivity zones were detected \",\n",
    "        str(count),\n",
    "        \" times out of \",\n",
    "        str(len(df_train_label.index)),\n",
    "    )\n",
    "    return path_no_inactivity_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_zeros = pd.DataFrame([False,True,False,False,False,True,False]).astype(int)\n",
    "\n",
    "df_zeros = np.array([0, 0, 0, 1, 0, 1, 1, 1, 1], dtype=bool)\n",
    "\n",
    "display(df_zeros.astype(int))\n",
    "count = 0\n",
    "duration_threshold = 2\n",
    "indices_list = []  # List of tuples\n",
    "howmany = 0\n",
    "for i in range(0, len(df_zeros)):\n",
    "    if df_zeros[i] == 1:\n",
    "        count = count + 1\n",
    "        print(\"1 à lindex\", i)\n",
    "    else:\n",
    "        if count >= duration_threshold:\n",
    "            print(\"threshold atteint start \", start, \" end at \", end)\n",
    "            start = i - count\n",
    "            end = i - 1\n",
    "            indices_list.append((start, end))\n",
    "            howmany = howmany + 1\n",
    "            count = 0\n",
    "        # if it doesn't reach the threshold, we change the 1 for 0 because we don't want to remove those\n",
    "        elif count >= 1:\n",
    "            print(\"Effacer les 1 de \", start, \" a \", end)\n",
    "            df_zeros[i - count : i] = [0] * (end - start)\n",
    "            count = 0\n",
    "\n",
    "display(df_zeros.astype(int))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
