{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEAT-PD Challenge\n",
    "\n",
    "Challenge website : https://www.synapse.org/#!Synapse:syn20825169/wiki/596118\n",
    "\n",
    "Data information : https://www.synapse.org/#!Synapse:syn20825169/wiki/600405\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas/Doubts [Laureano]\n",
    "\n",
    "VAD like thing to remove unwanted data?\n",
    "modified MFCC?\n",
    "X,Y,Z = relative positions or acceleration?\n",
    "\n",
    "Imp: Predict per person. Maybe UBM like thing and adapt it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Imports for the high pass signal\n",
    "from scipy.signal import butter, freqz, lfilter\n",
    "\n",
    "# KFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Import required modules\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os.path\n",
    "\n",
    "# To write WAV File\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "# To make derivative work on multiple CPUs\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transform_data\n",
    "from transform_data import *\n",
    "from create_graphs import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "\n",
    "# FIXME : Move this to data?\n",
    "path_save_accelerometer_plots = \"/home/sjoshi/codes/python/BeatPD/code/accelerometer_plots/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "possible to have participant characteristics from additional db data? ex https://ieeexplore.ieee.org/abstract/document/7911257"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIS-PD Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contains 16 subject_id (patients) for the training set \n",
    "\n",
    "- Gender: 11 Male, 5 Female \n",
    "- Race: 15 White, 1 NA\n",
    "- Ethnicity: 15 Not Hispanic or Latino, 1 Unknown\n",
    "- Age average (standard deviation) : 62.8125 (10.8579)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIS-PD: Basic accelerometer visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_type = \"cis\"\n",
    "\n",
    "# TODO: explain\n",
    "path_train_data, df_train_label = define_data_type(data_type=data_type, data_dir=data_dir)\n",
    "# display(df_train_label)\n",
    "# List of interesting measurement id we want to look at\n",
    "# list_measurement_id=[#'ab5287f4-8261-47ad-8ff2-22b5fe5d246e',\n",
    "#'db2e053a-0fb8-4206-891a-6f079fb14e3a']#,\n",
    "# 'ef5b1267-c212-46c5-aab0-4f4437bc6c67',\n",
    "# '4ec74fb9-7347-435d-83dc-79ad74c3bc49',\n",
    "# '8e8539ad-8841-476b-b15c-888ce3461989',\n",
    "# '22b88456-fe8f-4138-af55-be12afca4b81',\n",
    "# 'ad84583d-e5ae-4926-b077-531a0f7d08a9',\n",
    "# 'eef56825-940a-4c3e-aebb-60838d60869e',\n",
    "# 'e0441156-c4b8-467c-8f4f-3b532d594d8f',\n",
    "# '464ac314-6c4b-4c4a-957c-28a2339150d6']\n",
    "\n",
    "list_measurement_id = [\n",
    "    \"5cf68c8e-0b7a-4b73-ad4f-015c7a20fb5a\",\n",
    "    \"cc7b822c-e310-46f0-a8ea-98c95fdb67a1\",\n",
    "    \"5163afe8-a6b0-4ea4-b2ba-9b4501dd5912\",\n",
    "    \"db2e053a-0fb8-4206-891a-6f079fb14e3a\",\n",
    "    \"2d852742-10a9-4c56-9f38-779f2cd66879\",\n",
    "    \"2e3a4c9c-ff01-4a28-bfcf-ce9b7633a39d\",  # no inactivity should be removed\n",
    "    \"3cf49c01-0499-4bad-9167-67691711204a\",  # no inactivity should be removed PAS LA??\n",
    "    \"3d0f965c-9d72-43d1-9369-1ea3acf963cc\",  # PAS LA ???\n",
    "    \"4b269cc2-8f0c-4816-adbf-10c0069b8833\",\n",
    "    \"4bc51b90-bfce-4231-85e1-5de3b4bc0745\",\n",
    "    \"4fc3c295-857f-4920-8fa5-f21bfdc7ab4f\",\n",
    "]  # bit of inactivity in the middle]\n",
    "\n",
    "list_measurement_id = [\n",
    "    \"2d852742-10a9-4c56-9f38-779f2cd66879\",\n",
    "    \"4fc3c295-857f-4920-8fa5-f21bfdc7ab4f\",\n",
    "    \"db2e053a-0fb8-4206-891a-6f079fb14e3a\",\n",
    "]\n",
    "\n",
    "# Filter df_train_label according to the measurement_id we are most interested in\n",
    "df_train_label = interesting_patients(\n",
    "    df_train_label=df_train_label, list_measurement_id=list_measurement_id\n",
    ")\n",
    "\n",
    "# Display filtered df_train_label\n",
    "display(df_train_label)\n",
    "\n",
    "# path_no_inactivity_data = remove_inactivity_pct_change(df_train_label)\n",
    "\n",
    "# Plot the accelerometer data\n",
    "plot_accelerometer(df_train_label,\n",
    "                   data_type=data_type,\n",
    "                   path_train_data=path_train_data,\n",
    "                   path_accelerometer_plots=path_save_accelerometer_plots\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIS-PD: Create High Pass Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"cis\"\n",
    "# This is only to switch between training_data or ancillary_data which is additional data provided \n",
    "data_subset='ancillary_data' #training_data ancillary_data\n",
    "\n",
    "path_train_data, df_train_label = define_data_type(data_type, data_dir, data_subset)\n",
    "\n",
    "high_pass_path='/home/sjoshi/codes/python/BeatPD/data/BeatPD/cis-pd.'+ \\\n",
    "                data_subset+'.high_pass/'\n",
    "\n",
    "high_pass_filter(df_train_label, high_pass_path, path_train_data, data_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIS-PD: Create Masks for inactivity removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"cis\"\n",
    "# This is only to switch between training_data or ancillary_data which is additional data provided \n",
    "data_subset='ancillary_data' #training_data\n",
    "\n",
    "path_train_data, df_train_label = define_data_type(data_type, data_dir, data_subset)\n",
    "\n",
    "print(path_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_type = \"cis\"\n",
    "\n",
    "remove_inactivity_highpass(\n",
    "    df_train_label,\n",
    "    path_train_data,\n",
    "    data_type,\n",
    "    energy_threshold=5,\n",
    "    duration_threshold=3000,\n",
    "    plot_frequency_response=False,\n",
    "    mask_path='/home/sjoshi/codes/python/BeatPD/data/BeatPD/cis-pd.'+\n",
    "    data_subset+'.high_pass_mask/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIS-PD: Create first derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"cis\"\n",
    "# This is only to switch between training_data or ancillary_data which is additional data provided \n",
    "data_subset='ancillary_data' #training_data\n",
    "\n",
    "path_train_data, df_train_label = define_data_type(data_type, data_dir, data_subset)\n",
    "\n",
    "\n",
    "do_work = partial(\n",
    "    get_first_derivative, \n",
    "    path_train_data=path_train_data,\n",
    "    derivative_path=data_dir+\"cis-pd.training_data.derivative_original_data/\",\n",
    "    padding=True, \n",
    "    mask_path='/home/sjoshi/codes/python/BeatPD/data/BeatPD/cis-pd.training_data.high_pass_mask/',\n",
    ")\n",
    "\n",
    "num_jobs = 1\n",
    "with ProcessPoolExecutor(num_jobs) as ex:\n",
    "    results = list(ex.map(do_work, df_train_label['measurement_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIS-PD: Create WAV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIS-PD: Write Wav Files - Training Data - High Pass Filter Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "data_subset='training_data.high_pass'\n",
    "create_cis_wav_files(data_subset, data_dir, sAxis=\"X\", data_type=\"cis\", bMask=False)\n",
    "\n",
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "data_subset='training_data.high_pass'\n",
    "create_cis_wav_files(data_subset, data_dir, sAxis=\"Y\", data_type=\"cis\", bMask=False)\n",
    "\n",
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "data_subset='training_data.high_pass'\n",
    "create_cis_wav_files(data_subset, data_dir, sAxis=\"Z\", data_type=\"cis\", bMask=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIS-PD: Write Wav Files - Training Data - Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "data_subset='training_data'\n",
    "create_cis_wav_files(data_subset, data_dir, sAxis=\"X\", data_type=\"cis\", bMask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "data_subset='training_data'\n",
    "create_cis_wav_files(data_subset, data_dir, sAxis=\"Y\", data_type=\"cis\", bMask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "data_subset='training_data'\n",
    "create_cis_wav_files(data_subset, data_dir, sAxis=\"Z\", data_type=\"cis\", bMask=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIS-PD: Write Wav Files - Training Data - Inactivity Removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "data_subset='training_data'\n",
    "create_cis_wav_files(data_subset, data_dir, sAxis=\"X\", data_type=\"cis\", bMask=True)\n",
    "\n",
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "data_subset='training_data'\n",
    "create_cis_wav_files(data_subset, data_dir, sAxis=\"Y\", data_type=\"cis\", bMask=True)\n",
    "\n",
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "data_subset='training_data'\n",
    "create_cis_wav_files(data_subset, data_dir, sAxis=\"Z\", data_type=\"cis\", bMask=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIS-PD: Write Wav Files - Training Data - Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates files in these folders:\n",
    "# cis-pd.ancillary_data.wav_X\n",
    "# cis-pd.ancillary_data.wav_Y\n",
    "# cis-pd.ancillary_data.wav_Z\n",
    "\n",
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "data_subset='ancillary_data'\n",
    "create_cis_wav_files(data_subset, data_dir, sAxis=\"X\", data_type=\"cis\", bMask=False)\n",
    "\n",
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "data_subset='ancillary_data'\n",
    "create_cis_wav_files(data_subset, data_dir, sAxis=\"Y\", data_type=\"cis\", bMask=False)\n",
    "\n",
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "data_subset='ancillary_data'\n",
    "create_cis_wav_files(data_subset, data_dir, sAxis=\"Z\", data_type=\"cis\", bMask=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIS-PD: Write Wav Files - Ancillary Data - Inactivity Removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates files in these folders:\n",
    "# cis-pd.ancillary_data.high_pass_mask.wav_X\n",
    "# cis-pd.ancillary_data.high_pass_mask.wav_Y\n",
    "# cis-pd.ancillary_data.high_pass_mask.wav_Z\n",
    "\n",
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "data_subset='ancillary_data'\n",
    "create_cis_wav_files(data_subset, data_dir, sAxis=\"X\", data_type=\"cis\", bMask=True)\n",
    "\n",
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "data_subset='ancillary_data'\n",
    "create_cis_wav_files(data_subset, data_dir, sAxis=\"Y\", data_type=\"cis\", bMask=True)\n",
    "\n",
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "data_subset='ancillary_data'\n",
    "create_cis_wav_files(data_subset, data_dir, sAxis=\"Z\", data_type=\"cis\", bMask=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIS-PD: Write Wav Files - Ancillary Data - High Pass Filter Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates files in these folders:\n",
    "# cis-pd.ancillary_data.high_pass.wav_X\n",
    "# cis-pd.ancillary_data.high_pass.wav_Y\n",
    "# cis-pd.ancillary_data.high_pass.wav_Z\n",
    "\n",
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "data_subset='ancillary_data.high_pass'\n",
    "create_cis_wav_files(data_subset, data_dir, sAxis=\"X\", data_type=\"cis\", bMask=False)\n",
    "\n",
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "data_subset='ancillary_data.high_pass'\n",
    "create_cis_wav_files(data_subset, data_dir, sAxis=\"Y\", data_type=\"cis\", bMask=False)\n",
    "\n",
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "data_subset='ancillary_data.high_pass'\n",
    "create_cis_wav_files(data_subset, data_dir, sAxis=\"Z\", data_type=\"cis\", bMask=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_fold(df_train_label,\n",
    "               n_splits=5,\n",
    "               subject_id=None,\n",
    "               data_subset='training_data',\n",
    "               data_real_subtype=\"\"):\n",
    "    \"\"\"\n",
    "    Function that returns a list of X dataframes (X is according to the number of n_splits chosen)\n",
    "\n",
    "    The dataframes are the labels needed according to the split \n",
    "\n",
    "    Keyword Arguments:\n",
    "    - df_train_label: Dataframe containing the labels\n",
    "    - n_split: Optional. The number of folds. Default: 5\n",
    "    - subject_id: Optional. Specify a subject_id to get measurement_id only for that subject_id \n",
    "    - data_real_subtype: Only for REAL-PD database\n",
    "    \"\"\"\n",
    "    kf = StratifiedKFold(n_splits) # shuffle??? \n",
    "\n",
    "    # Building the dataframe to split\n",
    "    X = []\n",
    "\n",
    "    # if we want the data split for one specific subject_id\n",
    "    if subject_id:\n",
    "        df_train_label_subject_id = df_train_label.groupby(\"subject_id\")\n",
    "        X = df_train_label_subject_id.get_group(subject_id)\n",
    "    else:\n",
    "        # if we want to have all a split for all data no matter the subject_id\n",
    "        # NOTE: I didn't make sure to have one subject_id represented in both train/test\n",
    "        for idx in df_train_label.index:\n",
    "            X.append([df_train_label[\"measurement_id\"][idx]])\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    kf.get_n_splits(X)\n",
    "\n",
    "    # Building lists of df_train_label because we have by default 5 splits,\n",
    "    # so the lists will contain 5 DataFrames with different split indices required\n",
    "    list_df_train_label = list()\n",
    "    list_df_test_label = list()\n",
    "    split_idx = 0\n",
    "    \n",
    "    # Removing NaN values only for the purpose of kf.split as stratified kfold don't like NaN values\n",
    "#     print(type(X))\n",
    "#     print('Before contains NaN ? : ', X.isnull().sum())\n",
    "    X = prepro_missing_values(X)\n",
    "#     print('After contains NaN ? : ', X_no_nan.isnull().sum())\n",
    "    \n",
    "    if subject_id == 1046:\n",
    "        # 1046 is balanced on tremor because that's the only labels we have for that patient\n",
    "        y = X.iloc[:,-1]\n",
    "    else:\n",
    "        # We do the KFOLDs on a balanced on/off only \n",
    "        y = X.iloc[:,-3] \n",
    "\n",
    "    print(\"----- \" + str(subject_id) + \" -----\")\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        print(len(X))\n",
    "        print(len(df_train_label))\n",
    "        df_train_label = X.iloc[train_index]\n",
    "        df_test_label = X.iloc[test_index]\n",
    "        \n",
    "        # Following is just to see if the splits are balanced or not \n",
    "        train_y, test_y = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        train_0, train_1, train_2, train_3, train_4 = len(train_y[train_y==0]), \\\n",
    "                                                        len(train_y[train_y==1]), \\\n",
    "                                                        len(train_y[train_y==2]), \\\n",
    "                                                        len(train_y[train_y==3]), \\\n",
    "                                                        len(train_y[train_y==4])\n",
    "        test_0, test_1, test_2, test_3, test_4  = len(test_y[test_y==0]), \\\n",
    "                                                  len(test_y[test_y==1]), \\\n",
    "                                                  len(test_y[test_y==2]), \\\n",
    "                                                  len(test_y[test_y==3]), \\\n",
    "                                                  len(test_y[test_y==4])\n",
    "        print('>Train: 0=%d, 1=%d, 2=%d, 3=%d, 4=%d Test: 0=%d, 1=%d, 2=%d, 3=%d, 4=%d' % (train_0, train_1, train_2, train_3, train_4, \\\n",
    "                                                        test_0, test_1, test_2, test_3, test_4))\n",
    "        \n",
    "        list_df_train_label.append(df_train_label)\n",
    "        list_df_test_label.append(df_test_label)\n",
    "\n",
    "        # name of the file according to its database and type\n",
    "        # NOTE: Be careful that the end of the name of the folder where to save the kfolds is hardcoded here\n",
    "        path_save_k_fold_dataframes = (\n",
    "            data_dir + data_type + \"-pd.\"+data_subset+\".k_fold_v2/\" + data_real_subtype + \"/\"\n",
    "        )\n",
    "        df_train_label.to_csv(\n",
    "            path_save_k_fold_dataframes\n",
    "            + str(subject_id)\n",
    "            + \"_train_kfold_\"\n",
    "            + str(split_idx)\n",
    "            + \".csv\",\n",
    "            index=False,\n",
    "            header=[\"measurement_id\", \"subject_id\", \"on_off\", \"dyskinesia\", \"tremor\"],\n",
    "        )\n",
    "        df_test_label.to_csv(\n",
    "            path_save_k_fold_dataframes\n",
    "            + str(subject_id)\n",
    "            + \"_test_kfold_\"\n",
    "            + str(split_idx)\n",
    "            + \".csv\",\n",
    "            index=False,\n",
    "            header=[\"measurement_id\", \"subject_id\", \"on_off\", \"dyskinesia\", \"tremor\"],\n",
    "        )\n",
    "        split_idx = split_idx + 1\n",
    "    return list_df_train_label, list_df_test_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the K-Fold files for the CIS database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the data type as we have two databases\n",
    "data_type = \"cis\"\n",
    "path_train_data, df_train_label = define_data_type(data_type=data_type)\n",
    "\n",
    "# Group data by subject_id\n",
    "df_train_label_subject_id = df_train_label.groupby(\"subject_id\")\n",
    "\n",
    "data_subset = 'training_data'\n",
    "\n",
    "# Go through the subject_id and k-fold their data\n",
    "# FIXME: get_k_fold could me renamed to just create the folds, save them, not return anything\n",
    "for subject_id, value in df_train_label_subject_id:\n",
    "    list_df_train_label, list_df_test_label = get_k_fold(\n",
    "        df_train_label=df_train_label,\n",
    "        n_splits=5,\n",
    "        training_or_ancillary=data_subset,\n",
    "        subject_id=subject_id\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the K-Fold Files for the REAL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate the files, you have to uncomment one data_real_subtype at a time and\n",
    "# execute this cell 3 times for the 3 subtypes.\n",
    "\n",
    "data_type = \"real\"\n",
    "data_subset='training_data' # Either training_data or ancillary_data \n",
    "\n",
    "for data_real_subtype in ['smartphone_accelerometer','smartwatch_accelerometer','smartwatch_gyroscope']:\n",
    "    \n",
    "    path_train_data, df_train_label = define_data_type(data_type,\n",
    "                                                   data_dir,\n",
    "                                                   data_subset,\n",
    "                                                   data_real_subtype)\n",
    "\n",
    "    # Group data by subject_id\n",
    "    df_train_label_subject_id = df_train_label.groupby(\"subject_id\")\n",
    "\n",
    "    # Go through the subject_id and k-fold their data\n",
    "    for subject_id, value in df_train_label_subject_id:\n",
    "        list_df_train_label, list_df_test_label = get_k_fold(\n",
    "            df_train_label=df_train_label,\n",
    "            n_splits=5,\n",
    "            subject_id=subject_id,\n",
    "            data_subset=data_subset,\n",
    "            data_real_subtype=data_real_subtype,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REAL-PD Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This database, originally named \"Parkinson@Home\" is renamed to \"Real-PD\" for this challenge. The study was made over 2 weeks, with at home monitoring. \n",
    "\n",
    "The devices used are an android phone, a motorolla watch. \n",
    "- `smartwatch_accelerometer` and `smartwatch_gyroscope` : Motorolla Watch\n",
    "- `smartphone_accelerometer` : Android phone \n",
    "\n",
    "-> Question: so is the smartwatch & smartphone accelerometer should measure the same movements? \n",
    "\n",
    "The REAL-PD database has many missing values. \n",
    "\n",
    "The subject_id `hbv013` is the only one without missing data. Other patients all have at least one missing symptom (`diskenisia`, ) or two (`on/off and tremor`, `on_off and dyskinesia`, `dyskinesia and tremor`) missing.\n",
    "\n",
    "Measurements id with no data (`on_off`, `dyskinesia` and `tremor` are all missing):\n",
    "- `b50d1b0c-2cd1-45f8-9097-0742e5cbbcc8`\n",
    "- `b598c177-4e38-4ea8-8543-bd8f7e580f96`\n",
    "- `cf841bf8-0082-4ea3-999f-1f43e39a8dc6`\n",
    "- `b1e15f8a-109f-459b-ba87-46899240ee66`\n",
    "- `6f0e2580-56ec-4743-9356-d3e4d9a0aee5`\n",
    "- `773536f6-9b70-43d0-b099-5d167d74924a`\n",
    "- `54a0e841-ad45-4ba7-ac83-1785c5f7748b`\n",
    "- `cd9ed2e2-7e04-44c7-b041-7788f133c193`\n",
    "- `a6954a91-338b-4523-9e4a-5e69a8fac206`\n",
    "\n",
    "The 3 symptoms are reported as follows in this dataset: \n",
    "- `on_off = {0,1}`\n",
    "  - `Off` : 0 (medication is wearing off) \n",
    "  - `On` : 1 (medication is working)\n",
    "  \n",
    "- `dyskinesia = {0,1,2}`\n",
    "  - Without dyskinesia: 0 \n",
    "  - Non-troublesome dyskinesia: 1 \n",
    "  - Severe dyskinesia: 2 \n",
    "  \n",
    "- `tremor = {0,1,2,3,4}` \n",
    "The description of the database mentions `tremor` is rated from 0 to 4 according to its severity, but from all the data, the maximum value of `tremor` recorded is 3. \n",
    "\n",
    "Data:\n",
    "- ancillary\n",
    "- clinical : UPDRS evaluation score \n",
    "- demographics : #TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REAL-PD: Create Masks for inactivity removal for all subtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"real\"\n",
    "\n",
    "for data_real_subtype in ['smartphone_accelerometer','smartwatch_accelerometer','smartwatch_gyroscope']:\n",
    "    path_train_data, df_train_label = define_data_type(data_type,\n",
    "                                                       data_dir,\n",
    "                                                       data_subset,\n",
    "                                                       data_real_subtype)\n",
    "    \n",
    "    data_subset='training_data' #training_data testing_data\n",
    "    \n",
    "    high_pass_path='/home/sjoshi/codes/python/BeatPD/data/BeatPD/real-pd.'+ \\\n",
    "                data_subset+'.high_pass/'+data_real_subtype+'/'\n",
    "    \n",
    "    high_pass_filter(df_train_label, high_pass_path, path_train_data, data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_type = \"real\"\n",
    "#data_real_subtype='smartphone_accelerometer'\n",
    "#data_real_subtype = \"smartwatch_accelerometer\"\n",
    "#data_real_subtype='smartwatch_gyroscope'\n",
    "\n",
    "# List of interesting measurement id we want to look at\n",
    "# list_measurement_id = [\n",
    "#     \"5b4c7c81-659d-40ea-a1fd-59622074fd10\",\n",
    "#     \"ee053d95-c155-400d-ae42-fe24834ad4a9\",\n",
    "#     \"ce51ee31-8553-4321-9f83-8cd3dabe2f66\",\n",
    "#     \"e07708ff-7b8d-4070-af70-3aa81423ab5b\",\n",
    "#     #'7d3f4b7a-167f-4a26-9062-94ce9d8794c1',\n",
    "#     \"99af8d14-cd09-4107-9502-355378ba4e08\",\n",
    "#     #'7d5ac31a-cb53-40f7-8188-0b13724ea55c',\n",
    "#     \"9e43840b-dd89-498b-af1a-a62896a4d5d9\",\n",
    "#     \"e391f546-bf8a-46c7-a16c-95bc02f40629\",\n",
    "# ]\n",
    "\n",
    "# list_measurement_id = ['0c310608-1a32-4b09-b164-375d93ddb2aa']\n",
    "\n",
    "# Filter df_train_label according to the measurement_id we are most interested in\n",
    "# df_train_label = interesting_patients(df_train_label=df_train_label, list_measurement_id=list_measurement_id)\n",
    "for data_real_subtype in ['smartphone_accelerometer','smartwatch_accelerometer','smartwatch_gyroscope']:\n",
    "    data_subset='ancillary_data' #training_data testing_data \n",
    "    print(data_real_subtype)\n",
    "    path_train_data, df_train_label = define_data_type(data_type,\n",
    "                                                       data_dir,\n",
    "                                                       data_subset,\n",
    "                                                       data_real_subtype)\n",
    "\n",
    "#     list_measurement_id = ['f465145e-562d-4c02-b347-bc42ae3b4998']#['33f5a031-43a8-496a-89ee-0b9d99019617', '2515fa8d-8320-4010-bcd9-5dba52b1b5ba']\n",
    "    # Filter df_train_label according to the measurement_id we are most interested in\n",
    "#     df_train_label = interesting_patients(df_train_label=df_train_label,\n",
    "#                                           list_measurement_id=list_measurement_id)\n",
    "\n",
    "    remove_inactivity_highpass(\n",
    "        df_train_label,\n",
    "        path_train_data=path_train_data,\n",
    "        data_type=data_type,\n",
    "        energy_threshold=5,\n",
    "        duration_threshold=3000,\n",
    "        plot_frequency_response=False,\n",
    "        plot_accelerometer_after_removal=False,\n",
    "        mask_path=data_dir+'/real-pd.'+\n",
    "        data_subset+\n",
    "        '.high_pass_mask/'+data_real_subtype+'/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REAL-PD: Create first derivative for all subtypes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for data_real_subtype in ['smartphone_accelerometer','smartwatch_accelerometer','smartwatch_gyroscope']:\n",
    "    path_train_data, df_train_label = define_data_type(data_type=data_type)\n",
    "    print(len(df_train_label))\n",
    "    for idx in df_train_label.index:\n",
    "        try:\n",
    "#             print('where we get the file : ', path_train_data)\n",
    "            df_train_data = pd.read_csv(path_train_data + df_train_label[\"measurement_id\"][idx] + \".csv\")\n",
    "        except FileNotFoundError:\n",
    "            print('Removing ' + df_train_label[\"measurement_id\"][idx] +\n",
    "                  ' as it doesn\\'t exist for ' +\n",
    "                  data_real_subtype)\n",
    "            df_train_label = df_train_label.drop(idx)\n",
    "        print(len(df_train_label))\n",
    "    do_work = partial(\n",
    "        get_first_derivative, \n",
    "        path_train_data=path_train_data,\n",
    "        derivative_path=\"real-pd.training_data.derivative_original_data/\"+data_real_subtype+'/',\n",
    "        padding=True, \n",
    "        mask_path=data_dir+'/real-pd.training_data.high_pass_mask/'+data_real_subtype+'/'\n",
    "    )\n",
    "\n",
    "    num_jobs = 8\n",
    "    with ProcessPoolExecutor(num_jobs) as ex:\n",
    "        results = list(ex.map(do_work, df_train_label['measurement_id']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REAL-PD: Create WAV files for all subtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X Axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "create_real_wav_files(data_subset=\"ancillary_data\", data_dir=data_dir, sAxis=\"X\", data_type=\"real\")\n",
    "\n",
    "# data_type =\"real\"\n",
    "# #FIXME add the one missing when it's done running \n",
    "# #'smartphone_accelerometer', 'smartwatch_accelerometer', smartwatch_gyroscope\n",
    "# for data_real_subtype in ['smartphone_accelerometer', 'smartwatch_accelerometer', 'smartwatch_gyroscope']:\n",
    "#     data_subset = 'ancillary_data' #training_data\n",
    "#     path_train_data, df_train_label = define_data_type(data_type, data_subset, data_real_subtype)\n",
    "# #     list_mesurement_id=['33f5a031-43a8-496a-89ee-0b9d99019617']\n",
    "#     # Filter df_train_label according to the measurement_id we are most interested in\n",
    "# #     df_train_label = interesting_patients(df_train_label=df_train_label, list_measurement_id=list_measurement_id)\n",
    "\n",
    "#     for idx in df_train_label.index:\n",
    "#         try:            \n",
    "#             df_train_data = pd.read_csv(path_train_data + df_train_label[\"measurement_id\"][idx] + \".csv\")\n",
    "#         except FileNotFoundError:\n",
    "#             print('Removing ' + df_train_label[\"measurement_id\"][idx] +\n",
    "#                   ' as it doesn\\'t exist for ' +\n",
    "#                   data_real_subtype)\n",
    "#             df_train_label = df_train_label.drop(idx)\n",
    "        \n",
    "#     do_work = partial(\n",
    "#         write_wav, \n",
    "#         path_train_data=path_train_data,\n",
    "#         wav_path=data_dir+'real-pd.'+data_subset+'.wav_X/'+data_real_subtype+'/',\n",
    "#         sAxis='X',\n",
    "#         mask_path=data_dir+'/real-pd.'+data_subset+'.high_pass_mask/'+data_real_subtype+'/'\n",
    "#     )\n",
    "\n",
    "#     num_jobs = 6\n",
    "#     with ProcessPoolExecutor(num_jobs) as ex:\n",
    "#         results = list(ex.map(do_work, df_train_label['measurement_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "create_real_wav_files(data_subset=\"ancillary_data\", data_dir=data_dir, sAxis=\"Y\", data_type=\"real\")\n",
    "\n",
    "\n",
    "# data_type = \"real\"\n",
    "\n",
    "# for data_real_subtype in ['smartphone_accelerometer', 'smartwatch_accelerometer', 'smartwatch_gyroscope']:\n",
    "#     data_subset='ancillary_data'#'training_data' #training_data\n",
    "#     path_train_data, df_train_label = define_data_type(data_type,\n",
    "#                                                    data_dir,\n",
    "#                                                    data_subset,\n",
    "#                                                    data_real_subtype)\n",
    "#     for idx in df_train_label.index:\n",
    "#         try:            \n",
    "#             df_train_data = pd.read_csv(path_train_data + df_train_label[\"measurement_id\"][idx] + \".csv\")\n",
    "#         except FileNotFoundError:\n",
    "#             print('Removing ' + df_train_label[\"measurement_id\"][idx] +\n",
    "#                   ' as it doesn\\'t exist for ' +\n",
    "#                   data_real_subtype)\n",
    "#             df_train_label = df_train_label.drop(idx)\n",
    "\n",
    "#     do_work = partial(\n",
    "#         write_wav,\n",
    "#         path_train_data=path_train_data,\n",
    "#         wav_path=data_dir+'/real-pd.'+data_subset+'.wav_Y/'+data_real_subtype+'/',\n",
    "#         mask_path=data_dir+'/real-pd.'+data_subset+'.high_pass_mask/'+data_real_subtype+'/',\n",
    "#         sAxis='Y',\n",
    "#     )\n",
    "\n",
    "#     num_jobs = 8\n",
    "#     with ProcessPoolExecutor(num_jobs) as ex:\n",
    "#         results = list(ex.map(do_work, df_train_label['measurement_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z Axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/sjoshi/codes/python/BeatPD/data/BeatPD/\"\n",
    "create_real_wav_files(data_subset=\"ancillary_data\", data_dir=data_dir, sAxis=\"Z\", data_type=\"real\")\n",
    "\n",
    "\n",
    "# data_type = \"real\"\n",
    "\n",
    "# for data_real_subtype in ['smartwatch_gyroscope']:#['smartphone_accelerometer', 'smartwatch_accelerometer', 'smartwatch_gyroscope']:\n",
    "#     data_subset='ancillary_data' #training_data\n",
    "#     path_train_data, df_train_label = define_data_type(data_type,\n",
    "#                                                    data_dir,\n",
    "#                                                    data_subset,\n",
    "#                                                    data_real_subtype)\n",
    "#     display(df_train_label)\n",
    "#     for idx in df_train_label.index:\n",
    "#         try:\n",
    "#             print(path_train_data + df_train_label[\"measurement_id\"][idx] + \".csv\")\n",
    "#             df_train_data = pd.read_csv(path_train_data + df_train_label[\"measurement_id\"][idx] + \".csv\")\n",
    "#         except FileNotFoundError:\n",
    "#             print('Removing ' + df_train_label[\"measurement_id\"][idx] +\n",
    "#                   ' as it doesn\\'t exist for ' +\n",
    "#                   data_real_subtype)\n",
    "#             df_train_label = df_train_label.drop(idx)\n",
    "#     display(df_train_label)\n",
    "#     do_work = partial(\n",
    "#         write_wav, \n",
    "#         path_train_data=path_train_data,\n",
    "#         wav_path=data_dir+'/real-pd.'+data_subset+'.wav_Z/'+data_real_subtype+'/',\n",
    "#         mask_path=data_dir+'/real-pd.'+data_subset+'.high_pass_mask/'+data_real_subtype+'/',\n",
    "#         sAxis='Z'\n",
    "#     )\n",
    "\n",
    "#     num_jobs = 8\n",
    "#     with ProcessPoolExecutor(num_jobs) as ex:\n",
    "#         results = list(ex.map(do_work, df_train_label['measurement_id']))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests & Drafts, back-up space that's not important, just notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the default option to display all row with display(DF)\n",
    "# pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tried to do convolution instead of np.multiply and dot product to get the derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(\n",
    "    np.array(\n",
    "        [\n",
    "            [0.2, 1, 5, 9],\n",
    "            [0.4, 2, 6, 10],\n",
    "            [0.6, 3, 7, 11],\n",
    "            [0.8, 4, 8, 12],\n",
    "            [1, 5, 9, 13],\n",
    "            [1.2, 6, 10, 14],\n",
    "            [1.4, 7, 11, 15],\n",
    "            [1.6, 7, 11, 15],\n",
    "            [1.8, 8, 12, 16],\n",
    "            [2, 9, 13, 16],\n",
    "        ]\n",
    "    ),\n",
    "    columns=[\"Timestamp\", \"X\", \"Y\", \"Z\"],\n",
    ")\n",
    "display(df2)\n",
    "m = np.linspace(-3, 3, num=2 * 3 + 1)\n",
    "display(m)\n",
    "\n",
    "np.convolve(df2.loc[0:6, \"X\"], m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to filter a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of the data to find edge cases\n",
    "\n",
    "\n",
    "# Create variable with TRUE if nationality is USA\n",
    "dys = df_train_label[\"dyskinesia\"] > 1\n",
    "\n",
    "# Create variable with TRUE if age is greater than 50\n",
    "tre = df_train_label[\"on_off\"] > 0\n",
    "\n",
    "# Select all cases where nationality is USA and age is greater than 50\n",
    "df_train_label[dys & tre]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_inactivity_max(df_train_label):\n",
    "    last_filtered_value = pd.Series(np.zeros(3), index=[\"X\", \"Y\", \"Z\"])\n",
    "    filtered_value = pd.Series(np.zeros(3), index=[\"X\", \"Y\", \"Z\"])\n",
    "    display(last_filtered_value)\n",
    "    for idx in df_train_label.index:\n",
    "        df_allo = []\n",
    "        df_train_data = pd.read_csv(\n",
    "            path_train_data + df_train_label[\"measurement_id\"][idx] + \".csv\"\n",
    "        )\n",
    "\n",
    "        # Get the absolute max value for X, Y, Z\n",
    "        max_values = df_train_data.iloc[:, -3:].abs().max()\n",
    "\n",
    "        # Compute what is 5% of that max\n",
    "        thresold_energy = 5\n",
    "        df_treshold = (max_values * thresold_energy) / 100\n",
    "\n",
    "        # display(df_train_data)\n",
    "        # Candidates are the frames where X, Y, Z are below that treshold (5% of the max)\n",
    "        #         df_candidates = df_train_data[(df_train_data.X.abs() <= df_treshold.X) &\n",
    "        #                                      (df_train_data.Y.abs() <= df_treshold.Y) &\n",
    "        #                                      (df_train_data.Z.abs() <= df_treshold.Z)]\n",
    "        # display(df_candidates)\n",
    "        for idx2 in df_train_data.index:\n",
    "            # print('df_train_data[idx2]')\n",
    "            # display(df_train_data.iloc[idx2,-3:])\n",
    "            last_filtered_value = filtered_value\n",
    "            filtered_value = last_filtered_value + 0.004 * (\n",
    "                df_train_data.iloc[idx2, -3:] - last_filtered_value\n",
    "            )\n",
    "            y = pd.DataFrame(columns=[\"Timestamp\"])\n",
    "            y = pd.concat(\n",
    "                [y, pd.DataFrame([df_train_data.iloc[idx2, 0]], columns=[\"Timestamp\"])],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "            #             print('display y :')\n",
    "            #             display(y)\n",
    "            #             print('end display y')\n",
    "\n",
    "            #             print('display filtered value')\n",
    "            #             display(pd.DataFrame(filtered_value).transpose())\n",
    "            #             print('end display filtered value')\n",
    "            df_allo.append(\n",
    "                pd.concat([y, pd.DataFrame(filtered_value).transpose()], axis=1)\n",
    "            )\n",
    "        #             print('display df_allo')\n",
    "        #             display(df_allo)\n",
    "\n",
    "        # FIXME : change the name df_allo\n",
    "        df_allo = pd.DataFrame(df_allo, columns=(\"Timestamp\", \"X\", \"Y\", \"Z\"))\n",
    "\n",
    "        df_allo.plot(x=\"Timestamp\", legend=True, subplots=True, title=\"allo\")\n",
    "        stop()\n",
    "\n",
    "\n",
    "#         v_candidate_x = pd.DataFrame({'Candidate':list(0)})\n",
    "#         v_candidate_x = np.where(df_train_data.X.abs() <= df_treshold.X, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_zeros = pd.DataFrame([False,True,False,False,False,True,False]).astype(int)\n",
    "\n",
    "df_zeros = np.array([0, 0, 0, 1, 0, 1, 1, 1, 1], dtype=bool)\n",
    "\n",
    "display(df_zeros.astype(int))\n",
    "count = 0\n",
    "duration_threshold = 2\n",
    "indices_list = []  # List of tuples\n",
    "howmany = 0\n",
    "for i in range(0, len(df_zeros)):\n",
    "    if df_zeros[i] == 1:\n",
    "        count = count + 1\n",
    "        print(\"1 à lindex\", i)\n",
    "    else:\n",
    "        if count >= duration_threshold:\n",
    "            print(\"threshold atteint start \", start, \" end at \", end)\n",
    "            start = i - count\n",
    "            end = i - 1\n",
    "            indices_list.append((start, end))\n",
    "            howmany = howmany + 1\n",
    "            count = 0\n",
    "        # if it doesn't reach the threshold, we change the 1 for 0 because we don't want to remove those\n",
    "        elif count >= 1:\n",
    "            print(\"Effacer les 1 de \", start, \" a \", end)\n",
    "            df_zeros[i - count : i] = [0] * (end - start)\n",
    "            count = 0\n",
    "\n",
    "display(df_zeros.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
